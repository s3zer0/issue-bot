"""
GPT Í∏∞Î∞ò ÌÇ§ÏõåÎìú Ï∂îÏ∂úÍ∏∞ - Ïõπ Í≤ÄÏÉâ Í∏∞Îä• Ï∂îÍ∞Ä Î≤ÑÏ†Ñ.
"""

import asyncio
import json
import re
import time
from typing import List, Optional
from loguru import logger
from openai import AsyncOpenAI

from src.config import config
from ..base import BaseKeywordExtractor, KeywordExtractionResult, KeywordItem, KeywordImportance


class GPTKeywordExtractor(BaseKeywordExtractor):
    """OpenAI GPTÎ•º ÏÇ¨Ïö©Ìïú ÌÇ§ÏõåÎìú Ï∂îÏ∂úÍ∏∞ - Ïõπ Í≤ÄÏÉâ Í∏∞Îä• Ìè¨Ìï®."""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """GPT Ï∂îÏ∂úÍ∏∞ Ï¥àÍ∏∞Ìôî."""
        api_key = api_key or config.get_openai_api_key()
        super().__init__("GPT", api_key)

        if not self.api_key:
            raise ValueError("OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")

        self.client = AsyncOpenAI(api_key=self.api_key)
        self.model = model or config.get_openai_model()
        self.temperature = config.get_openai_temperature()
        self.max_tokens = config.get_openai_max_tokens()
        self.max_retries = config.get_max_retry_count()

        # üîç Ïõπ Í≤ÄÏÉâ Í∏∞Îä• Ï∂îÍ∞Ä
        self.perplexity_client = None
        self._initialize_search_client()

        self.is_initialized = True
        logger.info(f"GPT ÌÇ§ÏõåÎìú Ï∂îÏ∂úÍ∏∞ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (Î™®Îç∏: {self.model}, Ïõπ Í≤ÄÏÉâ: {'ÌôúÏÑ±Ìôî' if self.perplexity_client else 'ÎπÑÌôúÏÑ±Ìôî'})")

    def _initialize_search_client(self):
        """Perplexity ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî."""
        try:
            from src.clients.perplexity_client import PerplexityClient
            self.perplexity_client = PerplexityClient()
            logger.info("Ïõπ Í≤ÄÏÉâ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        except ImportError:
            logger.warning("Perplexity ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Ïõπ Í≤ÄÏÉâ Í∏∞Îä•Ïù¥ ÎπÑÌôúÏÑ±ÌôîÎê©ÎãàÎã§.")
        except Exception as e:
            logger.warning(f"Ïõπ Í≤ÄÏÉâ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")

    async def extract_keywords(
        self,
        topic: str,
        context: Optional[str] = None,
        max_keywords: int = 20
    ) -> KeywordExtractionResult:
        """GPTÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÌÇ§ÏõåÎìúÎ•º Ï∂îÏ∂úÌï©ÎãàÎã§."""
        start_time = time.time()
        logger.info(f"GPT ÌÇ§ÏõåÎìú Ï∂îÏ∂ú ÏãúÏûë: '{topic}'")

        try:
            # ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
            prompt = self._build_prompt(topic, context, max_keywords)

            # Ïõπ Í≤ÄÏÉâ Í∏∞Îä•ÏùÑ Ìè¨Ìï®Ìïú API Ìò∏Ï∂ú
            if self.perplexity_client:
                raw_response = await self._call_gpt_with_search(prompt)
            else:
                raw_response = await self._call_gpt(prompt)

            # ÏùëÎãµ ÌååÏã±
            keywords = self._parse_response(raw_response)

            # KeywordItem Í∞ùÏ≤¥Î°ú Î≥ÄÌôò
            keyword_items = self._create_keyword_items(keywords)

            return KeywordExtractionResult(
                keywords=keyword_items,
                source_name=self.name,
                extraction_time=time.time() - start_time,
                raw_response=raw_response,
                metadata={
                    'model': self.model,
                    'web_search_available': self.perplexity_client is not None
                }
            )

        except Exception as e:
            logger.error(f"GPT ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return KeywordExtractionResult(
                keywords=[],
                source_name=self.name,
                extraction_time=time.time() - start_time,
                error=str(e)
            )

    def _build_prompt(self, topic: str, context: Optional[str], max_keywords: int) -> str:
        """GPT ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±."""
        base_prompt = f"""Ï£ºÏ†ú "{topic}"Ïóê ÎåÄÌïú Ï†ÑÎ¨∏Ï†ÅÏù¥Í≥† Ìè¨Í¥ÑÏ†ÅÏù∏ ÌÇ§ÏõåÎìúÎ•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

**Î™©Ï†Å**: Ïù¥ ÌÇ§ÏõåÎìúÎì§ÏùÄ ÏµúÏã† Îâ¥Ïä§, Í∏∞Ïà† Î¨∏ÏÑú, Ïó∞Íµ¨ ÎÖºÎ¨∏ÏóêÏÑú Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.

**ÌÇ§ÏõåÎìú ÏÉùÏÑ± ÏßÄÏπ®**:
- ÏµúÏã† Í∏∞Ïà†, Ï†úÌíà, Ìä∏Î†åÎìúÏóê ÎåÄÌï¥ÏÑúÎäî 2024-2025ÎÖÑ ÌòÑÏû¨ Ï†ïÎ≥¥Î•º Î∞òÏòÅÌïòÏÑ∏Ïöî
- Í≤ÄÏ¶ù Í∞ÄÎä•Ìïú Ïã§Ï†ú Ïö©Ïñ¥Îßå ÏÉùÏÑ±ÌïòÏÑ∏Ïöî
- Ï∂îÏ∏°Ïù¥ÎÇò Ï∞ΩÏûëÏùÄ Í∏àÏßÄÎê©ÎãàÎã§

**ÌÇ§ÏõåÎìú Ïπ¥ÌÖåÍ≥†Î¶¨**:
1. **ÌïµÏã¨ ÌÇ§ÏõåÎìú (Primary)**: Ï£ºÏ†úÏùò Î≥∏ÏßàÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Í∞ÄÏû• Ï§ëÏöîÌïú Ïö©Ïñ¥ (5-7Í∞ú)
2. **Í¥ÄÎ†® Ïö©Ïñ¥ (Related)**: Íµ¨Ï≤¥Ï†ÅÏù∏ Ï†úÌíàÎ™Ö, Í∏∞Ïà†Î™Ö, ÌöåÏÇ¨Î™Ö Îì± (5-7Í∞ú)
3. **Îß•ÎùΩ ÌÇ§ÏõåÎìú (Context)**: ÏÇ∞ÏóÖ, Ìä∏Î†åÎìú, ÏùëÏö© Î∂ÑÏïº Îì± (5-7Í∞ú)

**ÏùëÎãµ ÌòïÏãù (JSON)**:
{{
    "primary_keywords": ["ÌÇ§ÏõåÎìú1", "ÌÇ§ÏõåÎìú2", ...],
    "related_terms": ["Ïö©Ïñ¥1", "Ïö©Ïñ¥2", ...],
    "context_keywords": ["Îß•ÎùΩ1", "Îß•ÎùΩ2", ...],
    "confidence": 0.0-1.0
}}"""

        if context:
            base_prompt += f"\n\n**Ï∂îÍ∞Ä Îß•ÎùΩ**: {context}"

        return base_prompt

    async def _call_gpt(self, prompt: str) -> str:
        """Í∏∞Î≥∏ GPT API Ìò∏Ï∂ú (Ïõπ Í≤ÄÏÉâ ÏóÜÏùå)."""
        for attempt in range(self.max_retries):
            try:
                request_params = {
                    "model": self.model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "ÎãπÏã†ÏùÄ ÌäπÏ†ï Ï£ºÏ†úÏóê ÎåÄÌïú Ï†ÑÎ¨∏Ï†ÅÏù∏ ÌÇ§ÏõåÎìúÎ•º ÏÉùÏÑ±ÌïòÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Ìï≠ÏÉÅ Ïú†Ìö®Ìïú JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌïòÏÑ∏Ïöî."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens
                }

                # GPT-4 Î™®Îç∏Ïùò Í≤ΩÏö∞ JSON Î™®Îìú ÌôúÏÑ±Ìôî
                if "gpt-4" in self.model:
                    request_params["response_format"] = {"type": "json_object"}

                response = await self.client.chat.completions.create(**request_params)
                return response.choices[0].message.content.strip()

            except Exception as e:
                logger.warning(f"GPT API Ìò∏Ï∂ú Ïã§Ìå® (ÏãúÎèÑ {attempt + 1}): {e}")
                if attempt == self.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)

    async def _call_gpt_with_search(self, prompt: str) -> str:
        """Ïõπ Í≤ÄÏÉâ Í∏∞Îä•Ïù¥ Ìè¨Ìï®Îêú GPT API Ìò∏Ï∂ú."""

        # Ïõπ Í≤ÄÏÉâ Ìï®Ïàò Ï†ïÏùò
        search_function = {
            "type": "function",
            "function": {
                "name": "web_search",
                "description": "ÏµúÏã† Ï†ïÎ≥¥ÎÇò ÏÇ¨Ïã§ ÌôïÏù∏Ïù¥ ÌïÑÏöîÌï† Îïå ÏõπÏùÑ Í≤ÄÏÉâÌï©ÎãàÎã§",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Í≤ÄÏÉâÌï† ÌÇ§ÏõåÎìúÎÇò ÏßàÎ¨∏"
                        }
                    },
                    "required": ["query"]
                }
            }
        }

        for attempt in range(self.max_retries):
            try:
                # Ï≤´ Î≤àÏß∏ GPT Ìò∏Ï∂ú (Í≤ÄÏÉâ Ìï®Ïàò Ìè¨Ìï®)
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": """ÎãπÏã†ÏùÄ ÌÇ§ÏõåÎìú ÏÉùÏÑ± Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. 
ÏµúÏã† Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌïú Í≤ΩÏö∞ web_search Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÌôïÌïú Ï†ïÎ≥¥Î•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.
ÏµúÏ¢ÖÏ†ÅÏúºÎ°úÎäî Î∞òÎìúÏãú Ïú†Ìö®Ìïú JSON ÌòïÏãùÏúºÎ°ú ÌÇ§ÏõåÎìúÎ•º ÏùëÎãµÌïòÏÑ∏Ïöî."""
                        },
                        {"role": "user", "content": prompt}
                    ],
                    tools=[search_function],
                    tool_choice="auto",
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )

                message = response.choices[0].message

                # Function callÏù¥ ÏûàÎäîÏßÄ ÌôïÏù∏
                if message.tool_calls:
                    logger.debug(f"GPTÍ∞Ä Ïõπ Í≤ÄÏÉâ ÏöîÏ≤≠: {len(message.tool_calls)}Í∞ú Í≤ÄÏÉâ")

                    # Performance: Parallel web search execution
                    messages = [
                        {
                            "role": "system",
                            "content": """ÎãπÏã†ÏùÄ ÌÇ§ÏõåÎìú ÏÉùÏÑ± Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. 
Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Ï∞∏Í≥†ÌïòÏó¨ Ï†ïÌôïÌïòÍ≥† ÏµúÏã†Ïùò ÌÇ§ÏõåÎìúÎ•º JSON ÌòïÏãùÏúºÎ°ú ÏÉùÏÑ±ÌïòÏÑ∏Ïöî."""
                        },
                        {"role": "user", "content": prompt},
                        message
                    ]

                    # Performance: Collect all search tasks for parallel execution
                    search_tasks = []
                    tool_call_mapping = {}
                    
                    for tool_call in message.tool_calls:
                        if tool_call.function.name == "web_search":
                            args = json.loads(tool_call.function.arguments)
                            query = args.get("query", "")
                            
                            logger.debug(f"Ïõπ Í≤ÄÏÉâ Ï§ÄÎπÑ: {query}")
                            search_task = self._perform_web_search(query)
                            search_tasks.append(search_task)
                            tool_call_mapping[len(search_tasks) - 1] = tool_call

                    # Performance: Execute all searches in parallel
                    if search_tasks:
                        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
                        
                        # Add all search results to messages
                        for i, (search_result, tool_call) in enumerate(zip(search_results, tool_call_mapping.values())):
                            if not isinstance(search_result, Exception):
                                messages.append({
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "content": search_result
                                })
                            else:
                                logger.warning(f"Web search failed: {search_result}")
                                messages.append({
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "content": f"Í≤ÄÏÉâ Ïã§Ìå®: {str(search_result)}"
                                })

                    # Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Ìè¨Ìï®Ìïú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±
                    final_response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        temperature=self.temperature,
                        max_tokens=self.max_tokens
                    )

                    return final_response.choices[0].message.content.strip()
                else:
                    # Í≤ÄÏÉâ ÏóÜÏù¥ Î∞îÎ°ú ÏùëÎãµ
                    return message.content.strip()

            except Exception as e:
                logger.warning(f"GPT Function Call API Ìò∏Ï∂ú Ïã§Ìå® (ÏãúÎèÑ {attempt + 1}): {e}")
                if attempt == self.max_retries - 1:
                    # Ïõπ Í≤ÄÏÉâ Ïã§Ìå® Ïãú Í∏∞Î≥∏ Î∞©Î≤ïÏúºÎ°ú Ìè¥Î∞±
                    logger.warning("Ïõπ Í≤ÄÏÉâ Ïã§Ìå®, Í∏∞Î≥∏ GPT Ìò∏Ï∂úÎ°ú Ìè¥Î∞±")
                    return await self._call_gpt(prompt)
                await asyncio.sleep(2 ** attempt)

    async def _perform_web_search(self, query: str) -> str:
        """Ïã§Ï†ú Ïõπ Í≤ÄÏÉâ ÏàòÌñâ."""
        try:
            if self.perplexity_client:
                # Perplexity APIÎ°ú Í≤ÄÏÉâ
                result = await self.perplexity_client._make_api_call(
                    f"{query}Ïóê ÎåÄÌïú ÏµúÏã† Ï†ïÎ≥¥ÏôÄ Í≥µÏãù Î∞úÌëú ÎÇ¥Ïö©ÏùÑ Í≤ÄÏÉâÌï¥Ï£ºÏÑ∏Ïöî. "
                    f"ÌäπÌûà Ï†ïÌôïÌïú Ï†úÌíàÎ™Ö, Î≤ÑÏ†Ñ, Ï∂úÏãúÏùº, Í∏∞Ïà† ÏÇ¨Ïñë Îì±ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî."
                )
                return f"Í≤ÄÏÉâ Í≤∞Í≥º: {result}"
            else:
                return f"Í≤ÄÏÉâ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏóÜÏùå: {query}"

        except Exception as e:
            logger.warning(f"Ïõπ Í≤ÄÏÉâ Ïã§Ìå®: {e}")
            return f"Í≤ÄÏÉâ Ïã§Ìå®: {query} (Ïò§Î•ò: {str(e)})"

    def _parse_response(self, raw_response: str) -> dict:
        """GPT ÏùëÎãµ ÌååÏã±."""
        try:
            # JSON Ï∂îÏ∂ú
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', raw_response, re.DOTALL)
            if not json_match:
                raise ValueError("ÏùëÎãµÏóêÏÑú JSONÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")

            data = json.loads(json_match.group())

            # ÌïÑÏàò ÌïÑÎìú ÌôïÏù∏ Î∞è Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
            data.setdefault("primary_keywords", [])
            data.setdefault("related_terms", [])
            data.setdefault("context_keywords", [])
            data.setdefault("confidence", 0.8)

            return data

        except Exception as e:
            logger.error(f"GPT ÏùëÎãµ ÌååÏã± Ïã§Ìå®: {e}")
            logger.debug(f"ÏõêÎ≥∏ ÏùëÎãµ: {raw_response}")

            # Ìè¥Î∞±: Í∏∞Î≥∏ Íµ¨Ï°∞ Î∞òÌôò
            return {
                "primary_keywords": [self.preprocess_topic(raw_response.split()[0]) if raw_response else "ÌÇ§ÏõåÎìú"],
                "related_terms": [],
                "context_keywords": [],
                "confidence": 0.3
            }

    def _create_keyword_items(self, parsed_data: dict) -> List[KeywordItem]:
        """ÌååÏã±Îêú Îç∞Ïù¥ÌÑ∞Î•º KeywordItem Í∞ùÏ≤¥Î°ú Î≥ÄÌôò."""
        keyword_items = []
        confidence = float(parsed_data.get('confidence', 0.8))

        # Primary keywords - HIGH importance
        for kw in parsed_data.get('primary_keywords', []):
            keyword_items.append(KeywordItem(
                keyword=kw,
                sources=[self.name],
                importance=KeywordImportance.HIGH,
                confidence=confidence,
                category='primary'
            ))

        # Related terms - NORMAL importance
        for kw in parsed_data.get('related_terms', []):
            keyword_items.append(KeywordItem(
                keyword=kw,
                sources=[self.name],
                importance=KeywordImportance.NORMAL,
                confidence=confidence * 0.9,
                category='related'
            ))

        # Context keywords - NORMAL to LOW importance
        for kw in parsed_data.get('context_keywords', []):
            keyword_items.append(KeywordItem(
                keyword=kw,
                sources=[self.name],
                importance=KeywordImportance.NORMAL,
                confidence=confidence * 0.8,
                category='context'
            ))

        return keyword_items