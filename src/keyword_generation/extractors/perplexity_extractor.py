"""
Perplexity ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°.
"""

import asyncio
import time
import re
from typing import List, Optional, Dict
from loguru import logger

from src.clients.perplexity_client import PerplexityClient
from ..base import BaseKeywordExtractor, KeywordExtractionResult, KeywordItem, KeywordImportance


class PerplexityKeywordExtractor(BaseKeywordExtractor):
    """Perplexity APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°."""

    def __init__(self, api_key: Optional[str] = None):
        """Perplexity ì¶”ì¶œê¸° ì´ˆê¸°í™”."""
        super().__init__("Perplexity", api_key)

        # ê¸°ì¡´ PerplexityClient ìž¬ì‚¬ìš©
        self.client = PerplexityClient(api_key)
        self.is_initialized = True
        logger.info("Perplexity í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    async def extract_keywords(
        self,
        topic: str,
        context: Optional[str] = None,
        max_keywords: int = 20
    ) -> KeywordExtractionResult:
        """Perplexityë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        start_time = time.time()
        logger.info(f"Perplexity í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œìž‘: '{topic}'")

        try:
            # ì›¹ ê²€ìƒ‰ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
            prompt = self._build_prompt(topic, context, max_keywords)

            # Perplexity API í˜¸ì¶œ
            response = await self.client._make_api_call(prompt)
            content = response['choices'][0]['message']['content']

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords_from_content(content, topic)

            return KeywordExtractionResult(
                keywords=keywords,
                source_name=self.name,
                extraction_time=time.time() - start_time,
                raw_response=content,
                metadata={'model': self.client.model}
            )

        except Exception as e:
            logger.error(f"Perplexity í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return KeywordExtractionResult(
                keywords=[],
                source_name=self.name,
                extraction_time=time.time() - start_time,
                error=str(e)
            )

    def _build_prompt(self, topic: str, context: Optional[str], max_keywords: int) -> str:
        """ì—­í•  íŠ¹í™”ëœ Perplexityìš© ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ìƒì„±."""
        
        prompt = f"""ë‹¹ì‹ ì€ ì›¹ ì •ë³´ ìˆ˜ì§‘ê³¼ ë¶„ì„ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì—°êµ¬ ì‚¬ì„œ(Research Librarian)ìž…ë‹ˆë‹¤.

**ì „ë¬¸ ì˜ì—­**: 
- ì‹¤ì‹œê°„ ì›¹ ì •ë³´ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„
- ë‹¤ì–‘í•œ ì˜¨ë¼ì¸ ì†ŒìŠ¤ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” ì •ë³´ ì‹ë³„
- í•™ìˆ  ìžë£Œ, ê³µì‹ ë¬¸ì„œ, ê¸°ìˆ  ë¸”ë¡œê·¸, ë‰´ìŠ¤ ë“± ë‹¤ê°ì  ì •ë³´ ìˆ˜ì§‘

**í˜„ìž¬ ìž„ë¬´**: "{topic}"ì— ëŒ€í•œ ì›¹ ì „ë°˜ì˜ í‚¤ì›Œë“œ ë™í–¥ ë¶„ì„

**ë‹¨ê³„ë³„ ë¶„ì„ ìˆ˜í–‰**:

1. **ì •ë³´ ì†ŒìŠ¤ ë¶„ì„**: ë¨¼ì € ì´ ì£¼ì œì™€ ê´€ë ¨ëœ ì£¼ìš” ì •ë³´ ì†ŒìŠ¤ë“¤ì„ íŒŒì•…í•´ë³´ì„¸ìš”.
   - ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë‚˜ ë¬¸ì„œ
   - ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬
   - ê¸°ìˆ  ë¸”ë¡œê·¸ë‚˜ í¬ëŸ¼ ê²Œì‹œë¬¼
   - í•™ìˆ  ë…¼ë¬¸ì´ë‚˜ ì—°êµ¬ ìžë£Œ

2. **í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘**:

**ðŸ“° ì›¹ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (Primary)**:
- ìµœì‹  ë‰´ìŠ¤ì™€ ê¸°ì‚¬ì—ì„œ í˜„ìž¬ ê°€ìž¥ ìžì£¼ ì–¸ê¸‰ë˜ëŠ” ìš©ì–´
- ê³µì‹ ë°œí‘œë‚˜ ë³´ë„ìžë£Œì— ë“±ìž¥í•˜ëŠ” í•µì‹¬ ìš©ì–´
- ì›¹ ê²€ìƒ‰ëŸ‰ì´ ê¸‰ì¦í•˜ê³  ìžˆëŠ” ìš©ì–´
- (5-7ê°œ)

**ðŸ’¬ ì»¤ë®¤ë‹ˆí‹° í™œì„± í‚¤ì›Œë“œ (Related)**:
- ê°œë°œìž ì»¤ë®¤ë‹ˆí‹°, í¬ëŸ¼ì—ì„œ í™œë°œížˆ ë…¼ì˜ë˜ëŠ” ìš©ì–´
- GitHub, Stack Overflow ë“±ì—ì„œ ì–¸ê¸‰ë˜ëŠ” ê¸°ìˆ  ìš©ì–´
- ì†Œì…œë¯¸ë””ì–´ì—ì„œ í™”ì œê°€ ë˜ëŠ” ê´€ë ¨ ìš©ì–´
- (5-7ê°œ)

**ðŸ“š ì „ë¬¸ ë¬¸ì„œ í‚¤ì›Œë“œ (Context)**:
- ê³µì‹ ë¬¸ì„œ, API ë ˆí¼ëŸ°ìŠ¤ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì •í™•í•œ ìš©ì–´
- í•™ìˆ  ë…¼ë¬¸ì´ë‚˜ ê¸°ìˆ  ë°±ì„œì˜ ì „ë¬¸ ìš©ì–´
- ì—…ê³„ í‘œì¤€ì´ë‚˜ ì‚¬ì–‘ì„œì— ëª…ì‹œëœ ìš©ì–´
- (5-7ê°œ)

**í’ˆì§ˆ ê¸°ì¤€ (Negative Prompting)**:
âŒ **ì œì™¸í•´ì•¼ í•  í‚¤ì›Œë“œ**:
- ê³¼ìž¥ëœ ë§ˆì¼€íŒ… í‘œí˜„ ("í˜ì‹ ì ", "ì°¨ì„¸ëŒ€", "ìµœê³ ")
- ë„ˆë¬´ ì¼ë°˜ì ì¸ í˜•ìš©ì‚¬ ("ì¢‹ì€", "ë‚˜ìœ", "í°", "ìž‘ì€")
- ì´ˆë³´ìž ëŒ€ìƒ ìš©ì–´ ("ìž…ë¬¸", "ê¸°ì´ˆ", "íŠœí† ë¦¬ì–¼")
- í™•ì¸ë˜ì§€ ì•Šì€ ë£¨ë¨¸ë‚˜ ì¶”ì¸¡ì„± ì •ë³´

âœ… **í¬í•¨í•´ì•¼ í•  í‚¤ì›Œë“œ**:
- ì‹¤ì œ ì›¹ì—ì„œ ê²€ì¦ ê°€ëŠ¥í•œ ì •í™•í•œ ìš©ì–´
- ê³µì‹ì ìœ¼ë¡œ ë°œí‘œë˜ê±°ë‚˜ ë¬¸ì„œí™”ëœ ìš©ì–´
- ì „ë¬¸ê°€ë“¤ì´ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ìˆ  ìš©ì–´
- í˜„ìž¬ í™œë°œížˆ ë…¼ì˜ë˜ê³  ìžˆëŠ” ì‹¤ìš©ì  ìš©ì–´

**ì¶œë ¥ í˜•ì‹**:
### ì›¹ íŠ¸ë Œë“œ í‚¤ì›Œë“œ
- [ì‹¤ì œ ë‰´ìŠ¤/ê³µì‹ ë°œí‘œì—ì„œ í™•ì¸ëœ ìš©ì–´]
- [ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ ìƒìœ„ ìš©ì–´]
...

### ì»¤ë®¤ë‹ˆí‹° í™œì„± í‚¤ì›Œë“œ  
- [ê°œë°œìž ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë…¼ì˜ë˜ëŠ” ìš©ì–´]
- [GitHub/Stack Overflow ì¸ê¸° ìš©ì–´]
...

### ì „ë¬¸ ë¬¸ì„œ í‚¤ì›Œë“œ
- [ê³µì‹ ë¬¸ì„œ/APIì—ì„œ ì‚¬ìš©ë˜ëŠ” ì •í™•í•œ ìš©ì–´]
- [í•™ìˆ  ë…¼ë¬¸/ê¸°ìˆ  ë°±ì„œì˜ ì „ë¬¸ ìš©ì–´]
..."""

        # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Tier 2 ëª¨ë“œ)
        if context and "Tier 1 keywords for refinement:" in context:
            tier1_context = context.split("Tier 1 keywords for refinement:")[-1].strip()
            prompt += f"""

**Tier 1 ì •ì œ ëª¨ë“œ**: ì´ì „ì— ìƒì„±ëœ í‚¤ì›Œë“œë“¤ì„ ì›¹ ì •ë³´ë¡œ ë³´ì™„í•˜ê³  ê²€ì¦í•˜ì„¸ìš”.

**ì°¸ê³ í•  Tier 1 í‚¤ì›Œë“œ**: {tier1_context}

**ì›¹ ì •ë³´ ê¸°ë°˜ ê°œì„  ì§€ì¹¨**:
- Tier 1 í‚¤ì›Œë“œì˜ ì •í™•ì„±ì„ ì›¹ ì •ë³´ë¡œ ê²€ì¦
- ëˆ„ë½ëœ ì›¹ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€ ë°œêµ´
- ìµœì‹  ì›¹ ë™í–¥ì„ ë°˜ì˜í•œ í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸
- ì›¹ì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” í‘œí˜„ìœ¼ë¡œ ì •ì œ"""

        elif context:
            prompt += f"\n\n**ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸**: {context}"

        prompt += f"""

**ì¤‘ìš”**: ëª¨ë“  í‚¤ì›Œë“œëŠ” ì‹¤ì œ ì›¹ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ê²ƒë§Œ í¬í•¨í•˜ê³ , ì´ {max_keywords}ê°œ ì´ë‚´ë¡œ ì œí•œí•˜ì„¸ìš”."""

        return prompt

    def _extract_keywords_from_content(self, content: str, topic: str) -> List[KeywordItem]:
        """Perplexity ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ."""
        keyword_items = []

        # ì„¹ì…˜ë³„ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§žì¶¤)
        sections = {
            'ì›¹ íŠ¸ë Œë“œ í‚¤ì›Œë“œ': ('web_trending', KeywordImportance.HIGH, 0.9),
            'ì»¤ë®¤ë‹ˆí‹° í™œì„± í‚¤ì›Œë“œ': ('community_active', KeywordImportance.HIGH, 0.85),
            'ì „ë¬¸ ë¬¸ì„œ í‚¤ì›Œë“œ': ('professional_docs', KeywordImportance.NORMAL, 0.8),
            # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜ì„± ìœ ì§€
            'í•µì‹¬ í‚¤ì›Œë“œ': ('primary', KeywordImportance.HIGH, 0.9),
            'íŠ¸ë Œë”© í‚¤ì›Œë“œ': ('trending', KeywordImportance.HIGH, 0.85),
            'ì „ë¬¸ ìš©ì–´': ('technical', KeywordImportance.NORMAL, 0.8)
        }

        for section_name, (category, importance, confidence) in sections.items():
            # ì„¹ì…˜ ì°¾ê¸°
            section_pattern = rf'#*\s*{section_name}.*?(?=#{2,}|\Z)'
            section_match = re.search(section_pattern, content, re.DOTALL | re.IGNORECASE)

            if section_match:
                section_text = section_match.group()

                # í‚¤ì›Œë“œ ì¶”ì¶œ (- ë˜ëŠ” * ë¡œ ì‹œìž‘í•˜ëŠ” í•­ëª©)
                keyword_pattern = r'[-*]\s*(.+?)(?:\n|$)'
                keywords = re.findall(keyword_pattern, section_text)

                for kw in keywords:
                    kw = kw.strip()
                    if kw and len(kw) > 1:
                        keyword_items.append(KeywordItem(
                            keyword=kw,
                            sources=[self.name],
                            importance=importance,
                            confidence=confidence,
                            category=category,
                            metadata={'source_type': 'web_search'}
                        ))

        # í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í´ë°±
        if not keyword_items:
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            words = re.findall(r'\b\w+\b', content)
            word_freq = {}
            for word in words:
                if len(word) > 3 and word.lower() != topic.lower():
                    word_freq[word.lower()] = word_freq.get(word.lower(), 0) + 1

            # ë¹ˆë„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
            for word, freq in top_words:
                keyword_items.append(KeywordItem(
                    keyword=word,
                    sources=[self.name],
                    importance=KeywordImportance.NORMAL if freq > 2 else KeywordImportance.LOW,
                    confidence=0.6,
                    category='extracted',
                    metadata={'frequency': freq}
                ))

        return keyword_items