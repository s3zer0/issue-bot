"""
LLM-as-a-Judge ê¸°ë°˜ í™˜ê° íƒì§€ê¸° - ì™„ì „ ìˆ˜ì • ë²„ì „.

GPT-4oë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì‚¬ì‹¤ì„±, ë…¼ë¦¬ì„±, ì¼ê´€ì„±ì„ í‰ê°€í•˜ê³ 
í™˜ê° ê°€ëŠ¥ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.
"""

import json
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from loguru import logger
from openai import AsyncOpenAI

from src.config import config
from .base import BaseHallucinationDetector
from .models import HallucinationScore


@dataclass
class LLMJudgeScore(HallucinationScore):
    """LLM Judge ë¶„ì„ ê²°ê³¼."""

    category_scores: Dict[str, float] = field(default_factory=dict)
    problematic_areas: List[Dict[str, str]] = field(default_factory=list)
    judge_reasoning: str = ""

    def __post_init__(self):
        super().__post_init__()
        self.method_name = "LLM-Judge"

    def get_summary(self) -> str:
        """LLM Judge ë¶„ì„ ê²°ê³¼ ìš”ì•½."""
        if self.category_scores:
            categories = ", ".join([
                f"{cat}: {score:.2f}"
                for cat, score in self.category_scores.items()
            ])
            return (
                f"LLM Judge - ì‹ ë¢°ë„: {self.confidence:.2f} "
                f"({categories})"
            )
        else:
            return f"LLM Judge - ì‹ ë¢°ë„: {self.confidence:.2f}"


class LLMJudgeDetector(BaseHallucinationDetector):
    """LLMì„ ì‹¬íŒìœ¼ë¡œ í™œìš©í•œ í™˜ê° íƒì§€ê¸°."""

    def __init__(self, model_name: str = "gpt-4o"):
        """
        LLM Judge íƒì§€ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_name (str): í‰ê°€ì— ì‚¬ìš©í•  OpenAI ëª¨ë¸
        """
        super().__init__("LLM-Judge")

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.api_key = config.get_openai_api_key()
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = AsyncOpenAI(api_key=self.api_key)
        self.model = str(model_name)  # ëª…ì‹œì  str ë³€í™˜

        # í‰ê°€ ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
        self.category_weights = {
            "factual_accuracy": 0.35,
            "logical_consistency": 0.25,
            "contextual_relevance": 0.20,
            "source_reliability": 0.20
        }

        self.is_initialized = True
        logger.info(f"LLM Judge íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")

    async def analyze_text(self, text: str, context: Optional[str] = None) -> LLMJudgeScore:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ í™˜ê° ê°€ëŠ¥ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            context (Optional[str]): í…ìŠ¤íŠ¸ì˜ ë§¥ë½ (ì£¼ì œ, ì§ˆë¬¸ ë“±)

        Returns:
            LLMJudgeScore: ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"LLM Judge ë¶„ì„ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)})")

        try:
            # í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            evaluation_prompt = self._create_evaluation_prompt(text, context)

            # LLMì— í‰ê°€ ìš”ì²­
            evaluation_result = await self._get_llm_evaluation(evaluation_prompt)

            # ê²°ê³¼ íŒŒì‹± ë° ì ìˆ˜ ê³„ì‚°
            scores = self._parse_evaluation_result(evaluation_result)
            final_confidence = self._calculate_weighted_score(scores)

            # í™˜ê° ê°€ëŠ¥ì„±ì´ ë†’ì€ ë¶€ë¶„ ì‹ë³„
            problematic_areas = self._identify_problematic_areas(evaluation_result)

            logger.info(f"LLM Judge ë¶„ì„ ì™„ë£Œ - ì‹ ë¢°ë„: {final_confidence:.2f}")

            return LLMJudgeScore(
                confidence=final_confidence,
                category_scores=scores,
                problematic_areas=problematic_areas,
                judge_reasoning=evaluation_result.get("overall_reasoning", ""),
                analysis_details={
                    "model_used": str(self.model),  # ğŸ”§ ëª…ì‹œì  str ë³€í™˜
                    "context_provided": bool(context is not None),  # ğŸ”§ ëª…ì‹œì  bool ë³€í™˜
                    "text_length": int(len(text)),  # ğŸ”§ ëª…ì‹œì  int ë³€í™˜
                    "evaluation_success": True
                }
            )

        except Exception as e:
            logger.error(f"LLM Judge ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œ ì¤‘ë¦½ì ì¸ ì ìˆ˜ ë°˜í™˜
            return LLMJudgeScore(
                confidence=0.5,
                category_scores={
                    "factual_accuracy": 0.5,
                    "logical_consistency": 0.5,
                    "contextual_relevance": 0.5,
                    "source_reliability": 0.5
                },
                problematic_areas=[],
                judge_reasoning=f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                analysis_details={
                    "error": str(e),
                    "model_used": str(self.model),
                    "evaluation_success": False
                }
            )

    def _create_evaluation_prompt(self, text: str, context: Optional[str]) -> str:
        """
        LLM í‰ê°€ë¥¼ ìœ„í•œ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            text (str): í‰ê°€í•  í…ìŠ¤íŠ¸
            context (Optional[str]): ë§¥ë½ ì •ë³´

        Returns:
            str: í‰ê°€ í”„ë¡¬í”„íŠ¸
        """
        context_section = f"ì›ë˜ ì§ˆë¬¸/ì£¼ì œ: {context}\n\n" if context else ""

        prompt = f"""ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ì˜ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í™˜ê°(hallucination) ê°€ëŠ¥ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

{context_section}í‰ê°€í•  í…ìŠ¤íŠ¸:
{text}

ë‹¤ìŒ 4ê°€ì§€ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ê°ê° 0-100ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , êµ¬ì²´ì ì¸ ì´ìœ ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”:

1. **ì‚¬ì‹¤ì  ì •í™•ì„± (Factual Accuracy)**
   - ì–¸ê¸‰ëœ ì‚¬ì‹¤, ìˆ«ì, ë‚ ì§œ, ì´ë¦„ ë“±ì´ ì •í™•í•œê°€?
   - ê²€ì¦ ê°€ëŠ¥í•œ ì •ë³´ì¸ê°€?
   - ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ ì‚¬ì‹¤ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?

2. **ë…¼ë¦¬ì  ì¼ê´€ì„± (Logical Consistency)**
   - í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ëª¨ìˆœë˜ëŠ” ì£¼ì¥ì´ ìˆëŠ”ê°€?
   - ì¸ê³¼ê´€ê³„ê°€ ë…¼ë¦¬ì ì¸ê°€?
   - ì£¼ì¥ê³¼ ê·¼ê±°ê°€ ì¼ì¹˜í•˜ëŠ”ê°€?

3. **ë§¥ë½ì  ê´€ë ¨ì„± (Contextual Relevance)**
   - ì£¼ì–´ì§„ ë§¥ë½ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
   - ì£¼ì œì—ì„œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ê°€?
   - ì ì ˆí•œ ìˆ˜ì¤€ì˜ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ëŠ”ê°€?

4. **ì¶œì²˜ ì‹ ë¢°ì„± (Source Reliability)**
   - ì–¸ê¸‰ëœ ì¶œì²˜ë‚˜ ì •ë³´ê°€ ì‹ ë¢°í•  ë§Œí•œê°€?
   - ê¶Œìœ„ìˆëŠ” ê¸°ê´€ì´ë‚˜ ì „ë¬¸ê°€ì˜ ì •ë³´ì¸ê°€?
   - ê²€ì¦ ê°€ëŠ¥í•œ ì¶œì²˜ê°€ ì œì‹œë˜ì—ˆëŠ”ê°€?

**ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ìœ íš¨í•œ JSONìœ¼ë¡œ ì‘ë‹µ):**
{{
  "factual_accuracy": {{
    "score": ì ìˆ˜(0-100),
    "reasoning": "í‰ê°€ ì´ìœ "
  }},
  "logical_consistency": {{
    "score": ì ìˆ˜(0-100),
    "reasoning": "í‰ê°€ ì´ìœ "
  }},
  "contextual_relevance": {{
    "score": ì ìˆ˜(0-100),
    "reasoning": "í‰ê°€ ì´ìœ "
  }},
  "source_reliability": {{
    "score": ì ìˆ˜(0-100),
    "reasoning": "í‰ê°€ ì´ìœ "
  }},
  "problematic_areas": [
    {{
      "text": "ë¬¸ì œê°€ ìˆëŠ” í…ìŠ¤íŠ¸ ë¶€ë¶„",
      "issue": "êµ¬ì²´ì ì¸ ë¬¸ì œì "
    }}
  ],
  "overall_reasoning": "ì¢…í•©ì ì¸ í‰ê°€ ì´ìœ "
}}

ì¤‘ìš”: ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""

        return prompt

    async def _get_llm_evaluation(self, prompt: str) -> Dict:
        """
        OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ LLM í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            prompt (str): í‰ê°€ í”„ë¡¬í”„íŠ¸

        Returns:
            Dict: í‰ê°€ ê²°ê³¼

        Raises:
            Exception: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        max_retries = 3

        for attempt in range(max_retries):
            try:
                # ğŸ”§ JSON ì§ë ¬í™” ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ íŒŒë¼ë¯¸í„° êµ¬ì„±
                request_params = {
                    "model": str(self.model),  # ëª…ì‹œì  str ë³€í™˜
                    "messages": [
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ì˜ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
                        },
                        {
                            "role": "user",
                            "content": str(prompt)  # ëª…ì‹œì  str ë³€í™˜
                        }
                    ],
                    "temperature": 0.1,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
                    "max_tokens": 2000,
                    "response_format": {"type": "json_object"}  # JSON ì¶œë ¥ ê°•ì œ
                }

                logger.debug(f"LLM Judge API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries}")

                response = await self.client.chat.completions.create(**request_params)
                content = response.choices[0].message.content

                if not content:
                    raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

                # JSON íŒŒì‹± ì‹œë„
                try:
                    result = json.loads(content)
                    logger.debug("JSON íŒŒì‹± ì„±ê³µ")
                    return result
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}), ë°±ì—… íŒŒì‹± ì‹œë„: {e}")
                    return self._fallback_parse(content)

            except Exception as e:
                logger.error(f"LLM í‰ê°€ ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")

                if attempt == max_retries - 1:
                    # ìµœì¢… ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                    logger.error("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
                    return self._get_default_evaluation()

                # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
                import asyncio
                await asyncio.sleep(1)

        return self._get_default_evaluation()

    def _get_default_evaluation(self) -> Dict:
        """API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ í‰ê°€ ê²°ê³¼."""
        return {
            "factual_accuracy": {"score": 50, "reasoning": "API í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "logical_consistency": {"score": 50, "reasoning": "API í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "contextual_relevance": {"score": 50, "reasoning": "API í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "source_reliability": {"score": 50, "reasoning": "API í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "problematic_areas": [],
            "overall_reasoning": "API í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ì¸í•´ ì¤‘ë¦½ì  ì ìˆ˜ ì ìš©"
        }

    def _fallback_parse(self, content: str) -> Dict:
        """
        JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë°±ì—… íŒŒì‹± ë°©ë²•.

        Args:
            content (str): LLM ì‘ë‹µ

        Returns:
            Dict: íŒŒì‹±ëœ ê²°ê³¼ (ê¸°ë³¸ê°’ í¬í•¨)
        """
        logger.warning("ë°±ì—… íŒŒì‹± ë©”ì„œë“œ ì‚¬ìš©")

        # ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡°
        result = {
            "factual_accuracy": {"score": 50, "reasoning": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "logical_consistency": {"score": 50, "reasoning": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "contextual_relevance": {"score": 50, "reasoning": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "source_reliability": {"score": 50, "reasoning": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©"},
            "problematic_areas": [],
            "overall_reasoning": "ì‘ë‹µ íŒŒì‹±ì— ì‹¤íŒ¨í•˜ì—¬ ì¤‘ë¦½ì  ì ìˆ˜ ì ìš©"
        }

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
        content_lower = content.lower()

        # ê¸ì •ì  í‚¤ì›Œë“œ
        positive_keywords = ['ì •í™•', 'correct', 'accurate', 'ì‹ ë¢°', 'reliable', 'ë…¼ë¦¬ì ', 'logical']
        negative_keywords = ['ë¶€ì •í™•', 'incorrect', 'wrong', 'ëª¨ìˆœ', 'contradiction', 'ì˜ì‹¬', 'doubt']

        positive_count = sum(1 for word in positive_keywords if word in content_lower)
        negative_count = sum(1 for word in negative_keywords if word in content_lower)

        # ì ìˆ˜ ì¡°ì •
        if positive_count > negative_count:
            adjustment = 20  # 70ì ìœ¼ë¡œ ì¡°ì •
        elif negative_count > positive_count:
            adjustment = -20  # 30ì ìœ¼ë¡œ ì¡°ì •
        else:
            adjustment = 0  # 50ì  ìœ ì§€

        for category in result:
            if isinstance(result[category], dict) and "score" in result[category]:
                result[category]["score"] = max(0, min(100, 50 + adjustment))

        return result

    def _parse_evaluation_result(self, result: Dict) -> Dict[str, float]:
        """
        í‰ê°€ ê²°ê³¼ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            result (Dict): LLM í‰ê°€ ê²°ê³¼

        Returns:
            Dict[str, float]: ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        scores = {}

        for category in self.category_weights.keys():
            if category in result and isinstance(result[category], dict):
                # 100ì  ë§Œì ì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                raw_score = result[category].get("score", 50)
                # ì•ˆì „í•œ ë²”ìœ„ ë³´ì¥
                normalized_score = max(0.0, min(1.0, float(raw_score) / 100.0))
                scores[category] = normalized_score
            else:
                # ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½ ì ìˆ˜
                scores[category] = 0.5
                logger.warning(f"í‰ê°€ ê²°ê³¼ì— {category} ì¹´í…Œê³ ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        return scores

    def _calculate_weighted_score(self, scores: Dict[str, float]) -> float:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            scores (Dict[str, float]): ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜

        Returns:
            float: ìµœì¢… ì‹ ë¢°ë„ (0.0 ~ 1.0)
        """
        weighted_sum = 0.0
        total_weight = 0.0

        for category, weight in self.category_weights.items():
            if category in scores:
                weighted_sum += float(scores[category]) * float(weight)
                total_weight += float(weight)

        if total_weight > 0:
            result = weighted_sum / total_weight
            # ì•ˆì „í•œ ë²”ìœ„ ë³´ì¥ ë° ë°˜ì˜¬ë¦¼
            return round(max(0.0, min(1.0, result)), 3)
        else:
            return 0.5  # ê¸°ë³¸ê°’

    def _identify_problematic_areas(self, result: Dict) -> List[Dict[str, str]]:
        """
        í‰ê°€ ê²°ê³¼ì—ì„œ ë¬¸ì œê°€ ìˆëŠ” ë¶€ë¶„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            result (Dict): LLM í‰ê°€ ê²°ê³¼

        Returns:
            List[Dict[str, str]]: ë¬¸ì œ ì˜ì—­ ë¦¬ìŠ¤íŠ¸
        """
        problematic_areas = result.get("problematic_areas", [])

        # ìœ íš¨ì„± ê²€ì‚¬ ë° ì•ˆì „í•œ ë³€í™˜
        valid_areas = []
        for area in problematic_areas:
            try:
                if isinstance(area, dict) and "text" in area and "issue" in area:
                    valid_areas.append({
                        "text": str(area["text"])[:200],  # ìµœëŒ€ 200ì, ëª…ì‹œì  str ë³€í™˜
                        "issue": str(area["issue"])[:500]  # ìµœëŒ€ 500ì, ëª…ì‹œì  str ë³€í™˜
                    })
            except Exception as e:
                logger.warning(f"ë¬¸ì œ ì˜ì—­ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue

        return valid_areas[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ

    async def analyze_claims(self, claims: List[str], context: Optional[str] = None) -> Dict[str, float]:
        """
        ê°œë³„ ì£¼ì¥ë“¤ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            claims (List[str]): í‰ê°€í•  ì£¼ì¥ ë¦¬ìŠ¤íŠ¸
            context (Optional[str]): ë§¥ë½ ì •ë³´

        Returns:
            Dict[str, float]: ê° ì£¼ì¥ì˜ ì‹ ë¢°ë„ ì ìˆ˜
        """
        claim_scores = {}

        for claim in claims:
            try:
                score = await self.analyze_text(claim, context)
                claim_scores[claim] = score.confidence
            except Exception as e:
                logger.warning(f"ì£¼ì¥ ë¶„ì„ ì‹¤íŒ¨: {claim[:50]}... - {e}")
                claim_scores[claim] = 0.5  # ê¸°ë³¸ê°’

        return claim_scores

    def get_detector_info(self) -> Dict[str, str]:
        """íƒì§€ê¸° ì •ë³´ ë°˜í™˜."""
        return {
            "name": self.name,
            "model": str(self.model),
            "is_initialized": str(self.is_initialized),
            "api_key_set": str(bool(self.api_key))
        }