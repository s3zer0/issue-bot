"""
í™˜ê° íƒì§€ê°€ í†µí•©ëœ í–¥ìƒëœ ì´ìŠˆ ê²€ìƒ‰ê¸°
"""

import asyncio
import time
import hashlib
from typing import Optional, List, Dict, Any, Union
from dataclasses import dataclass, field
from loguru import logger

from src.models import KeywordResult, IssueItem, SearchResult
from src.search.issue_searcher import IssueSearcher
from src.detection.keyword_generator import generate_keywords_for_topic
from src.hallucination_detection.threshold_manager import ThresholdManager
from src.hallucination_detection.reppl_detector import RePPLDetector
from src.hallucination_detection.consistency_checker import SelfConsistencyChecker
from src.hallucination_detection.models import CombinedHallucinationScore, ConsistencyScore
from sentence_transformers import SentenceTransformer


# ğŸš€ Global Sentence Transformer cache for performance optimization
class GlobalModelCache:
    """ì „ì—­ Sentence Transformer ëª¨ë¸ ìºì‹œ"""
    _instance = None
    _models = {}
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def get_model(self, model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') -> SentenceTransformer:
        """ìºì‹œëœ ëª¨ë¸ ë°˜í™˜ ë˜ëŠ” ìƒˆë¡œ ë¡œë“œ"""
        if model_name not in self._models:
            logger.info(f"Sentence Transformer ëª¨ë¸ ë¡œë”©: {model_name}")
            self._models[model_name] = SentenceTransformer(model_name)
        else:
            logger.debug(f"ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©: {model_name}")
        return self._models[model_name]
    
    def clear_cache(self):
        """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”"""
        self._models.clear()
        logger.info("Sentence Transformer ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”")


@dataclass
class OptimizationMetrics:
    """ìµœì í™” ì„±ëŠ¥ ì§€í‘œ"""
    total_issues_processed: int = 0
    total_processing_time: float = 0.0
    avg_issue_time: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0
    self_consistency_runs: int = 0
    self_consistency_skips: int = 0
    timeout_utilizations: List[float] = field(default_factory=list)
    # Progressive Deepening ì§€í‘œ
    progressive_deepening_triggered: int = 0
    progressive_deepening_successful: int = 0
    progressive_deepening_failed: int = 0
    initial_low_confidence_issues: int = 0
    
    def calculate_cache_hit_ratio(self) -> float:
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0
    
    def calculate_self_consistency_skip_ratio(self) -> float:
        total = self.self_consistency_runs + self.self_consistency_skips
        return self.self_consistency_skips / total if total > 0 else 0.0


class SmartCache:
    """
    ğŸš€ ì¶”ê°€ ìµœì í™”: ìŠ¤ë§ˆíŠ¸ ìºì‹± ì‹œìŠ¤í…œ
    
    ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ í™˜ê° íƒì§€ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬
    ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ë°©ì§€í•˜ê³  ì„±ëŠ¥ì„ ëŒ€í­ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, text: str, context: str = "") -> str:
        """í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„± (ì •ê·œí™”ë¡œ íˆíŠ¸ìœ¨ ê°œì„ )"""
        # í…ìŠ¤íŠ¸ ì •ê·œí™”ë¡œ ìºì‹œ íˆíŠ¸ìœ¨ ê°œì„ 
        normalized_text = self._normalize_text_for_cache(text)
        normalized_context = self._normalize_text_for_cache(context)
        combined = f"{normalized_text}:{normalized_context}"
        return hashlib.md5(combined.encode()).hexdigest()
    
    def _normalize_text_for_cache(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”ë¡œ ìºì‹œ íš¨ìœ¨ì„± ê°œì„ """
        if not text:
            return ""
        
        # ê¸°ë³¸ ì •ê·œí™”
        normalized = text.lower().strip()
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        normalized = ' '.join(normalized.split())
        
        # ë‚ ì§œì™€ ì‹œê°„ ì •ê·œí™” (2024-01-15 -> DATE, 2025ë…„ -> YEAR ë“±)
        import re
        normalized = re.sub(r'\d{4}-\d{2}-\d{2}', 'DATE', normalized)
        normalized = re.sub(r'\d{4}ë…„', 'YEARë…„', normalized)
        normalized = re.sub(r'\d+\.\d+', 'VERSION', normalized)  # ë²„ì „ ë²ˆí˜¸ ì •ê·œí™”
        
        # ìˆ«ì ì •ê·œí™” (êµ¬ì²´ì ì¸ ìˆ«ìë¥¼ ì¼ë°˜í™”)
        normalized = re.sub(r'\b\d+\b', 'NUM', normalized)
        
        return normalized
    
    def get(self, text: str, context: str = "") -> Optional[Any]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        key = self._generate_key(text, context)
        current_time = time.time()
        
        if key in self.cache:
            stored_time, result = self.cache[key]
            
            # TTL ì²´í¬
            if current_time - stored_time < self.ttl_seconds:
                self.access_times[key] = current_time
                self.hits += 1
                logger.debug(f"ìºì‹œ íˆíŠ¸: {key[:8]}")
                return result
            else:
                # ë§Œë£Œëœ í•­ëª© ì œê±°
                del self.cache[key]
                del self.access_times[key]
        
        self.misses += 1
        return None
    
    def put(self, text: str, result: Any, context: str = ""):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        key = self._generate_key(text, context)
        current_time = time.time()
        
        # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        self.cache[key] = (current_time, result)
        self.access_times[key] = current_time
        logger.debug(f"ìºì‹œ ì €ì¥: {key[:8]}")
    
    def _evict_oldest(self):
        """ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)"""
        if not self.access_times:
            return
        
        oldest_key = min(self.access_times, key=self.access_times.get)
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
        logger.debug(f"ìºì‹œ ì œê±°: {oldest_key[:8]}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total = self.hits + self.misses
        hit_ratio = self.hits / total if total > 0 else 0
        
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_ratio': hit_ratio,
            'cache_size': len(self.cache),
            'max_size': self.max_size
        }


class EnhancedIssueSearcher:
    """ì—¬ëŸ¬ í™˜ê° íƒì§€ ë°©ë²•ì´ í†µí•©ëœ ì´ìŠˆ ê²€ìƒ‰ê¸° (ì™„ì „ ìµœì í™” ë²„ì „)."""

    def __init__(
            self,
            api_key: Optional[str] = None,
            enable_reppl: bool = True,
            enable_consistency: bool = True,
            enable_llm_judge: bool = True,
            threshold_manager: Optional[ThresholdManager] = None
    ):
        """
        í™˜ê° íƒì§€ ê¸°ëŠ¥ì´ ê°•í™”ëœ ì´ìŠˆ ê²€ìƒ‰ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        # ê¸°ë³¸ ì´ìŠˆ ê²€ìƒ‰ê¸°
        self.base_searcher = IssueSearcher(api_key)

        # ì„ê³„ê°’ ê´€ë¦¬ì
        self.threshold_manager = threshold_manager or ThresholdManager()

        # ğŸš€ ìµœì í™” ì„¤ì •
        self.max_concurrent_issues = 8  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¦ê°€ (5ê°œ â†’ 8ê°œ)
        self.self_consistency_threshold = 0.6  # SC ì‹¤í–‰ ì„ê³„ê°’
        
        # ğŸš€ ì¶”ê°€ ìµœì í™”: ìŠ¤ë§ˆíŠ¸ ìºì‹±
        self.cache = SmartCache(max_size=1000, ttl_seconds=3600)
        
        # ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
        self.metrics = OptimizationMetrics()
        
        # ì ì‘í˜• íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.timeout_config = {
            'base_timeout': 60.0,
            'max_timeout': 300.0,
            'chars_per_second_ratio': 500,
            'additional_per_500chars': 5.0
        }
        
        # ğŸ†• Progressive Deepening ì„¤ì •
        self.progressive_deepening_config = {
            'enable': True,                    # Progressive Deepening í™œì„±í™”
            'initial_confidence_threshold': 0.5,  # 1ì°¨ ë¶„ì„ ì‹ ë¢°ë„ ì„ê³„ê°’
            'deepening_confidence_threshold': 0.7, # 2ì°¨ ë¶„ì„ ëª©í‘œ ì‹ ë¢°ë„
            'timeout_multiplier': 1.5,         # 2ì°¨ ë¶„ì„ íƒ€ì„ì•„ì›ƒ ë°°ìˆ˜
            'max_deepening_attempts': 1,       # ìµœëŒ€ ì‹¬í™” ë¶„ì„ ì‹œë„ íšŸìˆ˜
            'enhanced_query_count': 5,         # ì‹¬í™” ë¶„ì„ì‹œ Self-Consistency ì¿¼ë¦¬ ìˆ˜
            'enhanced_similarity_threshold': 0.85,  # ì‹¬í™” ë¶„ì„ì‹œ ìœ ì‚¬ë„ ì„ê³„ê°’
        }

        # í™˜ê° íƒì§€ê¸°ë“¤ ì´ˆê¸°í™”
        self.detectors = {}
        if enable_reppl:
            self.detectors['RePPL'] = RePPLDetector()
        if enable_consistency:
            self.detectors['Self-Consistency'] = SelfConsistencyChecker()
        if enable_llm_judge:
            try:
                from src.hallucination_detection.llm_judge import LLMJudgeDetector
                self.detectors['LLM-Judge'] = LLMJudgeDetector()
            except ImportError:
                logger.warning("LLM Judge íƒì§€ê¸°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        logger.info(
            f"ì™„ì „ ìµœì í™”ëœ ì´ìŠˆ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ "
            f"(í™œì„± íƒì§€ê¸°: {list(self.detectors.keys())}, "
            f"ë™ì‹œ ì‹¤í–‰: {self.max_concurrent_issues}ê°œ, "
            f"ìµœì†Œ ì‹ ë¢°ë„: {self.threshold_manager.thresholds.min_confidence_threshold})"
        )

    def _calculate_adaptive_timeout(self, text: str) -> float:
        text_length = len(text)
        base = 60.0  # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ
        max_timeout = 300.0  # ìµœëŒ€ íƒ€ì„ì•„ì›ƒ

        # í…ìŠ¤íŠ¸ ë³µì¡ë„ì— ë”°ë¥¸ ì¶”ê°€ ì‹œê°„ ê³„ì‚°
        additional = min(
            max_timeout - base,
            (text_length / 500) * 5.0  # 500ìë‹¹ 5ì´ˆ
        )

        return base + additional


    def _should_run_self_consistency(self, priority_confidence: float) -> bool:
        """
        ğŸš€ ê°œì„ ì‚¬í•­ 2: ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ê²°ê³¼ì— ë”°ë¥¸ Self-Consistency ì‹¤í–‰ ê²°ì •
        """
        should_run = priority_confidence >= self.self_consistency_threshold
        
        if should_run:
            self.metrics.self_consistency_runs += 1
        else:
            self.metrics.self_consistency_skips += 1
        
        logger.debug(
            f"Self-Consistency ê²°ì •: ìš°ì„ ìˆœìœ„ {priority_confidence:.2f} "
            f"{'â‰¥' if should_run else '<'} {self.self_consistency_threshold:.2f} â†’ "
            f"{'ì‹¤í–‰' if should_run else 'ìŠ¤í‚µ'}"
        )
        
        return should_run

    async def search_with_validation(
            self,
            keyword_result: KeywordResult,
            time_period: str,
            max_retries: int = 3
    ) -> SearchResult:
        """
        í‚¤ì›Œë“œë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  í™˜ê° íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ìµœì í™” ë²„ì „)
        """
        logger.info(f"ìµœì í™”ëœ ê²€ìƒ‰ ì‹œì‘: ì£¼ì œ '{keyword_result.topic}', ê¸°ê°„ '{time_period}'")
        overall_start = time.time()
        
        current_keywords = keyword_result
        all_attempts_issues = []
        
        # ğŸš€ ë¬´í•œ ë£¨í”„ ë°©ì§€: ì´ì „ì— ì‹œë„ëœ í‚¤ì›Œë“œ ì¡°í•©ì„ ì¶”ì 
        keyword_history = set()
        regeneration_count = 0
        max_regenerations = 3  # ìµœëŒ€ í‚¤ì›Œë“œ ì¬ìƒì„± íšŸìˆ˜ ì œí•œ
        
        # ì´ˆê¸° í‚¤ì›Œë“œ ì„œëª…ì„ ê¸°ë¡
        initial_signature = self._get_keyword_signature(current_keywords)
        keyword_history.add(initial_signature)

        for attempt in range(max_retries):
            logger.info(f"ì´ìŠˆ ê²€ìƒ‰ ì‹œë„ {attempt + 1}/{max_retries}")

            # 1. ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
            search_result = await self.base_searcher.search_issues_from_keywords(
                current_keywords, time_period, collect_details=True
            )

            if not search_result.issues:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                if attempt < max_retries - 1:
                    # ğŸš€ ì•ˆì „í•œ í‚¤ì›Œë“œ ì¬ìƒì„± (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                    new_keywords, regeneration_count, should_continue = await self._regenerate_keywords_safely(
                        current_keywords, 
                        search_result, 
                        "no_results",
                        keyword_history,
                        regeneration_count,
                        max_regenerations
                    )
                    
                    if not should_continue:
                        logger.warning("í‚¤ì›Œë“œ ì¬ìƒì„± ì¤‘ë‹¨. í˜„ì¬ ê²°ê³¼ë¡œ ì§„í–‰.")
                        break
                    
                    current_keywords = new_keywords
                continue

            all_attempts_issues.extend(search_result.issues)

            # 2. ğŸš€ ìµœì í™”ëœ í™˜ê° íƒì§€ ìˆ˜í–‰
            validated_issues = await self._validate_issues_optimized(
                search_result.issues, current_keywords
            )

            if not validated_issues:
                logger.warning("í™˜ê° íƒì§€ë¥¼ í†µê³¼í•œ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
                if attempt < max_retries - 1:
                    # ğŸš€ ì•ˆì „í•œ í‚¤ì›Œë“œ ì¬ìƒì„± (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                    new_keywords, regeneration_count, should_continue = await self._regenerate_keywords_safely(
                        current_keywords, 
                        search_result, 
                        "low_quality_results",
                        keyword_history,
                        regeneration_count,
                        max_regenerations
                    )
                    
                    if not should_continue:
                        logger.warning("í‚¤ì›Œë“œ ì¬ìƒì„± ì¤‘ë‹¨. í˜„ì¬ ê²°ê³¼ë¡œ ì§„í–‰.")
                        break
                    
                    current_keywords = new_keywords
                continue

            # 3. ì‹ ë¢°ë„ë³„ ë¶„ë¥˜ ë° ê²°ê³¼ í‰ê°€
            high_confidence_issues = [
                issue for issue in validated_issues
                if getattr(issue, 'hallucination_confidence', 0) >= 0.7
            ]

            # ì„±ê³µ ì¡°ê±´ í™•ì¸
            if len(high_confidence_issues) >= min(3, len(validated_issues) // 2):
                logger.info(
                    f"ìµœì í™”ëœ ê²€ìƒ‰ ì„±ê³µ: {len(validated_issues)}ê°œ ì´ìŠˆ ì¤‘ "
                    f"{len(high_confidence_issues)}ê°œ ë†’ì€ ì‹ ë¢°ë„"
                )
                
                # ìµœì¢… ê²°ê³¼ ì—…ë°ì´íŠ¸
                search_result.issues = validated_issues
                search_result.search_time = time.time() - overall_start
                
                # ğŸš€ ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
                self._update_final_metrics(search_result)
                break
            else:
                # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ í‚¤ì›Œë“œ ì¬ìƒì„±
                if attempt < max_retries - 1:
                    logger.info(
                        f"ë†’ì€ ì‹ ë¢°ë„ ì´ìŠˆ ë¶€ì¡± ({len(high_confidence_issues)}ê°œ), "
                        f"í‚¤ì›Œë“œ ì¬ìƒì„± ì¤‘..."
                    )
                    # ğŸš€ ì•ˆì „í•œ í‚¤ì›Œë“œ ì¬ìƒì„± (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                    new_keywords, regeneration_count, should_continue = await self._regenerate_keywords_safely(
                        current_keywords, 
                        search_result, 
                        "insufficient_confidence",
                        keyword_history,
                        regeneration_count,
                        max_regenerations,
                        high_confidence_issues,
                        validated_issues
                    )
                    
                    if not should_continue:
                        logger.warning("í‚¤ì›Œë“œ ì¬ìƒì„± ì¤‘ë‹¨. í˜„ì¬ ê²°ê³¼ë¡œ ì§„í–‰.")
                        # í˜„ì¬ ê²°ê³¼ë¡œ ìµœì¢… ì²˜ë¦¬
                        search_result.search_time = time.time() - overall_start
                        self._update_final_metrics(search_result)
                        break
                    
                    current_keywords = new_keywords
                else:
                    # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œëŠ” í˜„ì¬ ê²°ê³¼ë¡œ ì§„í–‰
                    logger.info("ë§ˆì§€ë§‰ ì‹œë„: í˜„ì¬ ê²°ê³¼ë¡œ ì§„í–‰")
                    search_result.search_time = time.time() - overall_start
                    self._update_final_metrics(search_result)
                    break

        # ğŸš€ ìµœì¢… ê²€ì¦: ê²°ê³¼ê°€ ë„ˆë¬´ ë¶€ì¡±í•˜ë©´ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì‹¤í–‰
        final_result = await self._apply_fallback_if_needed(search_result, keyword_result.topic)
        
        # ğŸš€ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥
        self._log_performance_report()
        
        return final_result

    async def _validate_issues_optimized(
        self,
        issues: List[IssueItem],
        keyword_result: KeywordResult
    ) -> List[IssueItem]:
        """
        ğŸš€ ëª¨ë“  ìµœì í™”ê°€ ì ìš©ëœ ì´ìŠˆ ê²€ì¦
        
        í¬í•¨ëœ ìµœì í™”:
        - ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¦ê°€ (8ê°œ)
        - ìŠ¤ë§ˆíŠ¸ ìºì‹±
        - ì ì‘í˜• íƒ€ì„ì•„ì›ƒ
        - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²˜ë¦¬
        - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬
        """
        if not issues:
            return []

        logger.info(f"ì™„ì „ ìµœì í™”ëœ í™˜ê° íƒì§€ ì‹œì‘: {len(issues)}ê°œ ì´ìŠˆ")
        start_time = time.time()
        
        # ğŸš€ ê°œì„ ì‚¬í•­ 4: ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¦ê°€
        semaphore = asyncio.Semaphore(self.max_concurrent_issues)

        async def validate_with_optimizations(issue):
            async with semaphore:
                return await self._validate_single_issue_with_cache(issue, keyword_result)

        # ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ê²€ì¦: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ ì²˜ë¦¬
        all_validated = []
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì´ìŠˆë¥¼ ì²˜ë¦¬ (í° ë°°ì¹˜ ëŒ€ì‹  ì—°ì†ì  ì²˜ë¦¬)
        async def stream_process_issues():
            """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì´ìŠˆë¥¼ ì—°ì†ì ìœ¼ë¡œ ì²˜ë¦¬"""
            results = []
            active_tasks = []
            
            for i, issue in enumerate(issues):
                # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
                if len(active_tasks) >= self.max_concurrent_issues:
                    # ì™„ë£Œëœ íƒœìŠ¤í¬ ìˆ˜ì§‘
                    done, active_tasks = await asyncio.wait(
                        active_tasks, return_when=asyncio.FIRST_COMPLETED
                    )
                    
                    for task in done:
                        try:
                            result = await task
                            if result is not None:
                                results.append(result)
                        except Exception as e:
                            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ê²€ì¦ ì‹¤íŒ¨: {e}")
                
                # ìƒˆë¡œìš´ íƒœìŠ¤í¬ ì¶”ê°€
                task = asyncio.create_task(validate_with_optimizations(issue))
                active_tasks.append(task)
                
                # ì§„í–‰ë¥  ë¡œê¹…
                if (i + 1) % 10 == 0:
                    logger.debug(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì§„í–‰: {i+1}/{len(issues)}")
            
            # ë‚¨ì€ íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            if active_tasks:
                remaining_results = await asyncio.gather(*active_tasks, return_exceptions=True)
                for result in remaining_results:
                    if isinstance(result, Exception):
                        logger.error(f"ìµœì¢… ìŠ¤íŠ¸ë¦¬ë° ê²€ì¦ ì‹¤íŒ¨: {result}")
                    elif result is not None:
                        results.append(result)
            
            return results
        
        all_validated = await stream_process_issues()

        # ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
        processing_time = time.time() - start_time
        self.metrics.total_issues_processed += len(issues)
        self.metrics.total_processing_time += processing_time
        self.metrics.avg_issue_time = (
            self.metrics.total_processing_time / self.metrics.total_issues_processed
        )

        logger.info(
            f"ì™„ì „ ìµœì í™” ê²€ì¦ ì™„ë£Œ ({processing_time:.2f}ì´ˆ): "
            f"{len(issues)}ê°œ â†’ {len(all_validated)}ê°œ í†µê³¼"
        )

        return all_validated

    async def _validate_single_issue_with_cache(
        self,
        issue: IssueItem,
        keyword_result: KeywordResult
    ) -> Optional[IssueItem]:
        """
        ğŸš€ ìºì‹±ì´ ì ìš©ëœ ë‹¨ì¼ ì´ìŠˆ ê²€ì¦
        """
        # ìºì‹œ í™•ì¸
        cache_key_text = f"{issue.title}:{issue.summary}"
        cached_result = self.cache.get(cache_key_text, keyword_result.topic)
        
        if cached_result is not None:
            logger.debug(f"ìºì‹œ íˆíŠ¸: {issue.title[:30]}")
            # ìºì‹œëœ ê²°ê³¼ë¥¼ ì´ìŠˆì— ì ìš©
            setattr(issue, 'hallucination_analysis', cached_result['analysis'])
            setattr(issue, 'hallucination_confidence', cached_result['confidence'])
            return issue if cached_result['is_valid'] else None
        
        # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ê²€ì¦ ìˆ˜í–‰
        logger.debug(f"ìºì‹œ ë¯¸ìŠ¤: {issue.title[:30]}")
        result = await self._validate_single_issue_optimized(issue, keyword_result)
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        if result is not None:
            cache_data = {
                'analysis': getattr(result, 'hallucination_analysis', None),
                'confidence': getattr(result, 'hallucination_confidence', 0.0),
                'is_valid': True
            }
        else:
            cache_data = {
                'analysis': None,
                'confidence': 0.0,
                'is_valid': False
            }
        
        self.cache.put(cache_key_text, cache_data, keyword_result.topic)
        return result

    async def _validate_single_issue_optimized(
        self,
        issue: IssueItem,
        keyword_result: KeywordResult
    ) -> Optional[IssueItem]:
        """
        ğŸš€ ë‹¨ì¼ ì´ìŠˆ ì™„ì „ ìµœì í™” ê²€ì¦ (Progressive Deepening í¬í•¨)
        """
        topic = keyword_result.topic
        
        try:
            # ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„
            analysis_text = self._prepare_analysis_text(issue)
            
            # ğŸš€ ê°œì„ ì‚¬í•­ 1: ì ì‘í˜• íƒ€ì„ì•„ì›ƒ ê³„ì‚°
            adaptive_timeout = self._calculate_adaptive_timeout(analysis_text)
            
            # === 1ì°¨ ë¶„ì„: ë¹ ë¥¸ ì´ˆê¸° ë¶„ì„ ===
            initial_result = await self._run_initial_analysis(
                issue, topic, analysis_text, adaptive_timeout
            )
            
            if initial_result is None:
                return None
            
            # ì´ˆê¸° ë¶„ì„ ì‹ ë¢°ë„ í™•ì¸
            initial_confidence = getattr(initial_result, 'hallucination_confidence', 0.0)
            
            # === ğŸš€ ìŠ¤ë§ˆíŠ¸ Progressive Deepening ê²°ì • ===
            should_deepen = self._should_apply_progressive_deepening(
                issue, initial_confidence, analysis_text
            )
            
            if should_deepen:
                logger.info(
                    f"Progressive Deepening íŠ¸ë¦¬ê±° - ì´ˆê¸° ì‹ ë¢°ë„: {initial_confidence:.2f} < "
                    f"{self.progressive_deepening_config['initial_confidence_threshold']:.2f} "
                    f"(ì´ìŠˆ: {issue.title[:30]})"
                )
                
                self.metrics.progressive_deepening_triggered += 1
                self.metrics.initial_low_confidence_issues += 1
                
                # === 2ì°¨ ë¶„ì„: ì‹¬í™” ì •ë°€ ë¶„ì„ ===
                deepened_result = await self._run_progressive_deepening_analysis(
                    issue, topic, analysis_text, adaptive_timeout, initial_result
                )
                
                if deepened_result is not None:
                    deepened_confidence = getattr(deepened_result, 'hallucination_confidence', 0.0)
                    
                    if deepened_confidence >= self.progressive_deepening_config['deepening_confidence_threshold']:
                        self.metrics.progressive_deepening_successful += 1
                        logger.info(
                            f"Progressive Deepening ì„±ê³µ - í–¥ìƒëœ ì‹ ë¢°ë„: "
                            f"{initial_confidence:.2f} â†’ {deepened_confidence:.2f}"
                        )
                        return deepened_result
                    else:
                        self.metrics.progressive_deepening_failed += 1
                        logger.debug(
                            f"Progressive Deepening ì‹¤íŒ¨ - ì—¬ì „íˆ ë‚®ì€ ì‹ ë¢°ë„: {deepened_confidence:.2f}"
                        )
                else:
                    self.metrics.progressive_deepening_failed += 1
                    logger.debug("Progressive Deepening ì‹¤íŒ¨ - ë¶„ì„ ì˜¤ë¥˜")
            
            # ì´ˆê¸° ê²°ê³¼ ë°˜í™˜ (Progressive Deepeningì´ ë¹„í™œì„±í™”ë˜ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°)
            return initial_result
            
        except Exception as e:
            logger.error(f"ì´ìŠˆ '{issue.title}' ê²€ì¦ ì˜¤ë¥˜: {e}")
            return None

    async def _run_initial_analysis(
        self,
        issue: IssueItem,
        topic: str,
        analysis_text: str,
        adaptive_timeout: float
    ) -> Optional[IssueItem]:
        """
        ğŸš€ 1ì°¨ ë¶„ì„: ë¹ ë¥¸ ì´ˆê¸° ë¶„ì„ ìˆ˜í–‰
        """
        # === 1ë‹¨ê³„: ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ì‹¤í–‰ ===
        priority_scores = await self._run_priority_detectors_optimized(
            issue, topic, analysis_text, adaptive_timeout * 0.7
        )
        
        # === 2ë‹¨ê³„: Self-Consistency ì¡°ê±´ë¶€ ì‹¤í–‰ ===
        all_scores = priority_scores.copy()
        
        if priority_scores:
            avg_priority_confidence = sum(
                score.confidence for score in priority_scores.values()
            ) / len(priority_scores)
            
            # ğŸš€ ê°œì„ ì‚¬í•­ 2 & 3: ì¡°ê±´ë¶€ Self-Consistency
            if self._should_run_self_consistency(avg_priority_confidence):
                consistency_score = await self._run_optimized_self_consistency(
                    analysis_text, issue.title, adaptive_timeout * 0.3
                )
                
                # Handle return format (ConsistencyScore object or dict)
                if consistency_score:
                    if isinstance(consistency_score, dict):
                        # Timeout or error case
                        status = consistency_score.get('status', 'unknown')
                        logger.debug(f"Self-Consistency ê²°ê³¼: {status}")
                    else:
                        # Success case - ConsistencyScore object
                        all_scores['Self-Consistency'] = consistency_score
        
        # === 3ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° ìµœì¢… ê²€ì¦ ===
        return self._finalize_issue_validation(issue, all_scores)

    async def _run_progressive_deepening_analysis(
        self,
        issue: IssueItem,
        topic: str,
        analysis_text: str,
        adaptive_timeout: float,
        initial_result: IssueItem
    ) -> Optional[IssueItem]:
        """
        ğŸ†• 2ì°¨ ë¶„ì„: Progressive Deepening ì‹¬í™” ì •ë°€ ë¶„ì„
        
        ë” ê¸´ íƒ€ì„ì•„ì›ƒê³¼ í–¥ìƒëœ ì„¤ì •ìœ¼ë¡œ ì¬ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        logger.debug(f"Progressive Deepening ì‹œì‘: {issue.title[:30]}")
        
        # ì‹¬í™” ë¶„ì„ìš© ì—°ì¥ëœ íƒ€ì„ì•„ì›ƒ
        deepening_timeout = adaptive_timeout * self.progressive_deepening_config['timeout_multiplier']
        
        try:
            # === ì‹¬í™” ë‹¨ê³„ 1: í–¥ìƒëœ ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ì‹¤í–‰ ===
            enhanced_priority_scores = await self._run_enhanced_priority_detectors(
                issue, topic, analysis_text, deepening_timeout * 0.6
            )
            
            # === ì‹¬í™” ë‹¨ê³„ 2: ê°•í™”ëœ Self-Consistency ì‹¤í–‰ ===
            all_enhanced_scores = enhanced_priority_scores.copy()
            
            if enhanced_priority_scores:
                # ê°•í™”ëœ Self-Consistency ë¬´ì¡°ê±´ ì‹¤í–‰
                enhanced_consistency_score = await self._run_enhanced_self_consistency(
                    analysis_text, issue.title, deepening_timeout * 0.4
                )
                
                if enhanced_consistency_score:
                    if isinstance(enhanced_consistency_score, dict):
                        # Timeout or error case
                        status = enhanced_consistency_score.get('status', 'unknown')
                        logger.debug(f"Enhanced Self-Consistency ê²°ê³¼: {status}")
                    else:
                        # Success case - ConsistencyScore object
                        all_enhanced_scores['Enhanced-Self-Consistency'] = enhanced_consistency_score
            
            # === ì‹¬í™” ë‹¨ê³„ 3: ê²°ê³¼ í†µí•© ë° ê²€ì¦ ===
            if all_enhanced_scores:
                # ì´ˆê¸° ê²°ê³¼ì™€ ì‹¬í™” ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ì‹ ë¢°ë„ ê³„ì‚°
                initial_scores = getattr(initial_result, 'hallucination_analysis', None)
                if initial_scores and hasattr(initial_scores, 'individual_scores'):
                    # ê¸°ì¡´ ì ìˆ˜ì™€ ìƒˆë¡œìš´ ì ìˆ˜ë¥¼ ê²°í•©
                    combined_scores = initial_scores.individual_scores.copy()
                    combined_scores.update(all_enhanced_scores)
                    all_enhanced_scores = combined_scores
                
                return self._finalize_issue_validation(issue, all_enhanced_scores)
            else:
                logger.warning(f"Progressive Deepening ì‹¤íŒ¨: ëª¨ë“  ì‹¬í™” íƒì§€ê¸° ì‹¤íŒ¨ - {issue.title[:30]}")
                return None
                
        except Exception as e:
            logger.error(f"Progressive Deepening ì˜¤ë¥˜: {e} - {issue.title[:30]}")
            return None

    async def _run_enhanced_priority_detectors(
        self,
        issue: IssueItem,
        topic: str,
        analysis_text: str,
        timeout: float
    ) -> Dict[str, Any]:
        """
        ğŸ†• í–¥ìƒëœ ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ì‹¤í–‰ (Progressive Deepeningìš©)
        
        ë” ê¸´ íƒ€ì„ì•„ì›ƒê³¼ ë” ì •ë°€í•œ ì„¤ì •ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ íƒì§€ê¸°ë“¤ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        enhanced_tasks = {}
        
        # Enhanced RePPL ë¶„ì„ (ë” ì •ë°€í•œ ì„¤ì •)
        if 'RePPL' in self.detectors:
            enhanced_tasks['Enhanced-RePPL'] = asyncio.create_task(
                self.detectors['RePPL'].analyze_issue(issue, topic),
                name=f"EnhancedRePPL-{issue.title[:20]}"
            )
        
        # Enhanced LLM Judge ê²€ì‚¬ (ë” ìƒì„¸í•œ ì»¨í…ìŠ¤íŠ¸)
        if 'LLM-Judge' in self.detectors:
            enhanced_context = (
                f"ì£¼ì œ: {topic}\n"
                f"ì œëª©: {issue.title}\n"
                f"ì‹¬í™” ë¶„ì„ ëª¨ë“œ: Progressive Deepening\n"
                f"ë¶„ì„ ê¸¸ì´: {len(analysis_text)} ë¬¸ì"
            )
            enhanced_tasks['Enhanced-LLM-Judge'] = asyncio.create_task(
                self.detectors['LLM-Judge'].analyze_text(
                    analysis_text,
                    context=enhanced_context
                ),
                name=f"EnhancedLLMJudge-{issue.title[:20]}"
            )
        
        # í–¥ìƒëœ íƒì§€ê¸° ë³‘ë ¬ ì‹¤í–‰ (ë” ê¸´ íƒ€ì„ì•„ì›ƒ)
        enhanced_scores = {}
        
        if enhanced_tasks:
            done, pending = await asyncio.wait(
                enhanced_tasks.values(),
                return_when=asyncio.ALL_COMPLETED,
                timeout=timeout
            )
            
            # ì™„ë£Œëœ íƒœìŠ¤í¬ ê²°ê³¼ ìˆ˜ì§‘
            for task in done:
                task_name = task.get_name()
                try:
                    result = await task
                    detector_type = task_name.split('-')[1] if '-' in task_name else task_name
                    if detector_type == 'LLMJudge':
                        detector_type = 'LLM-Judge'
                    
                    enhanced_scores[f"Enhanced-{detector_type}"] = result
                    logger.debug(f"Enhanced {detector_type} ì™„ë£Œ: {result.confidence:.2f}")
                    
                except Exception as e:
                    detector_type = task_name.split('-')[1] if '-' in task_name else task_name
                    logger.warning(f"Enhanced {detector_type} ì‹¤íŒ¨: {e}")
            
            # ë¯¸ì™„ë£Œ íƒœìŠ¤í¬ ì •ë¦¬
            for task in pending:
                task.cancel()
                detector_type = task.get_name().split('-')[1] if '-' in task.get_name() else task.get_name()
                logger.warning(f"Enhanced {detector_type} íƒ€ì„ì•„ì›ƒ")
                
                try:
                    await task
                except (asyncio.CancelledError, Exception):
                    pass
        
        return enhanced_scores

    async def _run_enhanced_self_consistency(
        self,
        text: str,
        context: str,
        timeout: float
    ) -> Optional[Union[ConsistencyScore, Dict[str, Any]]]:
        """
        ğŸ†• ê°•í™”ëœ Self-Consistency ì‹¤í–‰ (Progressive Deepeningìš©)
        
        ë” ë§ì€ ì¿¼ë¦¬ì™€ ë” ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ Self-Consistencyë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        Returns ConsistencyScore on success, Dict on error/timeout, None on failure
        """
        if 'Self-Consistency' not in self.detectors:
            return {'status': 'detector_unavailable', 'confidence': 0.0}
        
        detector = self.detectors['Self-Consistency']
        
        # ğŸ†• ì‹¬í™” ë¶„ì„ìš© ê°•í™”ëœ ì„¤ì • ì ìš©
        original_settings = self._apply_enhanced_consistency_settings(detector)
        
        try:
            # Enhanced Self-Consistency ì‹¤í–‰
            enhanced_task = asyncio.create_task(
                detector.analyze_text(text, context=f"Enhanced Analysis: {context}")
            )
            
            result = await asyncio.wait_for(enhanced_task, timeout=timeout)
            logger.debug(f"Enhanced Self-Consistency ì„±ê³µ: {result.confidence:.2f}")
            return result
            
        except asyncio.TimeoutError:
            enhanced_task.cancel()
            
            text_preview = text[:50] + "..." if len(text) > 50 else text
            logger.warning(
                f"Enhanced Self-Consistency íƒ€ì„ì•„ì›ƒ - ì„¤ì • íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ, "
                f"ë¶„ì„ í…ìŠ¤íŠ¸: '{text_preview}'"
            )
            
            try:
                await enhanced_task
            except (asyncio.CancelledError, Exception):
                pass
            
            return {'status': 'timeout', 'confidence': 0.0}
        except Exception as e:
            logger.warning(f"Enhanced Self-Consistency ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'confidence': 0.0, 'error': str(e)}
        finally:
            # ì›ë˜ ì„¤ì • ë³µì›
            self._restore_consistency_settings(detector, original_settings)

    def _should_apply_progressive_deepening(
        self, 
        issue: IssueItem, 
        initial_confidence: float, 
        analysis_text: str
    ) -> bool:
        """
        ğŸš€ ìŠ¤ë§ˆíŠ¸ Progressive Deepening ê²°ì • ë¡œì§
        
        ë‹¨ìˆœíˆ ì‹ ë¢°ë„ë§Œ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ ìš”ì†Œë¥¼ ê³ ë ¤í•˜ì—¬
        Progressive Deepeningì´ ì‹¤ì œë¡œ ë„ì›€ì´ ë ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        """
        
        # ê¸°ë³¸ ì¡°ê±´: Progressive Deepeningì´ í™œì„±í™”ë˜ì–´ì•¼ í•¨
        if not self.progressive_deepening_config['enable']:
            return False
        
        # ğŸš€ ì¡°ê±´ 1: ì´ˆê¸° ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ì•„ì•¼ í•¨
        confidence_threshold = self.progressive_deepening_config['initial_confidence_threshold']
        if initial_confidence >= confidence_threshold:
            return False
        
        # ğŸš€ ì¡°ê±´ 2: ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ìŠ¤í‚µ (ê°œì„  ì—¬ì§€ê°€ ì ìŒ)
        if len(analysis_text) < 150:
            logger.debug(f"Progressive Deepening ìŠ¤í‚µ - í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ: {len(analysis_text)} < 150")
            return False
        
        # ğŸš€ ì¡°ê±´ 3: ëª…ë°±íˆ ë¬¸ì œê°€ ìˆëŠ” ì´ìŠˆëŠ” ìŠ¤í‚µ (ì‹œê°„ ì ˆì•½)
        if initial_confidence < 0.2:
            logger.debug(f"Progressive Deepening ìŠ¤í‚µ - ëª…ë°±íˆ ì‹ ë¢°ë„ ë‚®ìŒ: {initial_confidence:.2f} < 0.2")
            return False
        
        # ğŸš€ ì¡°ê±´ 4: ì¤‘ê°„ ì‹ ë¢°ë„ ì˜ì—­ì—ì„œë§Œ Progressive Deepening ì ìš©
        # (0.2 ~ 0.5 ë²”ìœ„ì—ì„œ ê°€ì¥ íš¨ê³¼ì )
        if 0.2 <= initial_confidence < confidence_threshold:
            logger.debug(f"Progressive Deepening ì ìš© - ê°œì„  ê°€ëŠ¥ ë²”ìœ„: {initial_confidence:.2f}")
            return True
        
        return False

    def _apply_enhanced_consistency_settings(self, detector) -> Dict[str, Any]:
        """
        ğŸ†• Progressive Deepeningìš© ê°•í™”ëœ Self-Consistency ì„¤ì • ì ìš©
        """
        original_settings = {}
        
        # ğŸ†• ì¿¼ë¦¬ ìˆ˜ ì¦ê°€ (ê¸°ë³¸ 3ê°œ â†’ 5ê°œ)
        if hasattr(detector, 'set_query_count'):
            original_settings['query_count'] = getattr(detector, 'query_count', 3)
            detector.set_query_count(
                self.progressive_deepening_config['enhanced_query_count']
            )
        
        # ğŸ†• ë” ì—„ê²©í•œ ìœ ì‚¬ë„ ê¸°ì¤€ (ê¸°ë³¸ 0.8 â†’ 0.85)
        if hasattr(detector, 'set_similarity_threshold'):
            original_settings['similarity_threshold'] = getattr(detector, 'similarity_threshold', 0.8)
            detector.set_similarity_threshold(
                self.progressive_deepening_config['enhanced_similarity_threshold']
            )
        
        return original_settings

    def _prepare_analysis_text(self, issue: IssueItem) -> str:
        """ë¶„ì„ìš© í…ìŠ¤íŠ¸ ìµœì í™”ëœ ì¤€ë¹„"""
        analysis_text = issue.summary
        
        # ìƒì„¸ ë‚´ìš©ì´ ìˆìœ¼ë©´ í¬í•¨ (ê¸¸ì´ ì œí•œìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´)
        if hasattr(issue, 'detailed_content') and issue.detailed_content:
            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
            max_detail_length = 1500
            detail = issue.detailed_content[:max_detail_length]
            if len(issue.detailed_content) > max_detail_length:
                detail += "..."
            
            analysis_text = f"{issue.summary}\n\n{detail}"
        
        return analysis_text

    async def _run_priority_detectors_optimized(
        self, 
        issue: IssueItem, 
        topic: str, 
        analysis_text: str, 
        timeout: float
    ) -> Dict[str, Any]:
        """ìµœì í™”ëœ ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ì‹¤í–‰"""
        
        priority_tasks = {}
        
        # RePPL ë¶„ì„ (ê°€ì¥ ë¹ ë¦„)
        if 'RePPL' in self.detectors:
            priority_tasks['RePPL'] = asyncio.create_task(
                self.detectors['RePPL'].analyze_issue(issue, topic),
                name=f"RePPL-{issue.title[:20]}"
            )
        
        # LLM Judge ê²€ì‚¬ (ì¤‘ê°„ ì†ë„)
        if 'LLM-Judge' in self.detectors:
            priority_tasks['LLM-Judge'] = asyncio.create_task(
                self.detectors['LLM-Judge'].analyze_text(
                    analysis_text,
                    context=f"ì£¼ì œ: {topic}, ì œëª©: {issue.title}"
                ),
                name=f"LLMJudge-{issue.title[:20]}"
            )
        
        # ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ë³‘ë ¬ ì‹¤í–‰
        priority_scores = {}
        
        if priority_tasks:
            done, pending = await asyncio.wait(
                priority_tasks.values(),
                return_when=asyncio.ALL_COMPLETED,
                timeout=timeout
            )
            
            # ì™„ë£Œëœ íƒœìŠ¤í¬ ê²°ê³¼ ìˆ˜ì§‘
            for task in done:
                task_name = task.get_name()
                try:
                    result = await task
                    detector_type = task_name.split('-')[0]
                    if detector_type == 'LLMJudge':
                        detector_type = 'LLM-Judge'
                    
                    priority_scores[detector_type] = result
                    logger.debug(f"{detector_type} ì™„ë£Œ: {result.confidence:.2f}")
                    
                except Exception as e:
                    detector_type = task_name.split('-')[0]
                    logger.warning(f"{detector_type} ì‹¤íŒ¨: {e}")
            
            # ë¯¸ì™„ë£Œ íƒœìŠ¤í¬ ì •ë¦¬
            for task in pending:
                task.cancel()
                detector_type = task.get_name().split('-')[0]
                logger.warning(f"{detector_type} ìš°ì„ ìˆœìœ„ ë‹¨ê³„ íƒ€ì„ì•„ì›ƒ")
                
                # íƒ€ì„ì•„ì›ƒëœ íƒœìŠ¤í¬ ì •ë¦¬ ëŒ€ê¸°
                try:
                    await task
                except (asyncio.CancelledError, Exception):
                    pass
        
        return priority_scores

    async def _run_optimized_self_consistency(
        self, 
        text: str, 
        context: str, 
        timeout: float
    ) -> Optional[Union[ConsistencyScore, Dict[str, Any]]]:
        """
        ğŸš€ ê°œì„ ì‚¬í•­ 3: ìµœì í™”ëœ Self-Consistency ì‹¤í–‰
        Returns ConsistencyScore on success, Dict on error/timeout, None on failure
        """
        if 'Self-Consistency' not in self.detectors:
            return {'status': 'detector_unavailable', 'confidence': 0.0}
        
        detector = self.detectors['Self-Consistency']
        
        # ğŸš€ ì§§ì€ í…ìŠ¤íŠ¸ ë¹ ë¥¸ ì²˜ë¦¬
        if len(text) < 100:
            logger.debug("ì§§ì€ í…ìŠ¤íŠ¸ - Self-Consistency ê°„ì†Œí™”")
            return self._create_optimized_consistency_score()
        
        # ğŸš€ ë™ì  ìµœì í™” ì„¤ì • ì ìš©
        original_settings = self._apply_consistency_optimizations(detector)
        
        try:
            # Self-Consistency ì‹¤í–‰
            consistency_task = asyncio.create_task(
                detector.analyze_text(text, context=context)
            )
            
            result = await asyncio.wait_for(consistency_task, timeout=timeout)
            return result
            
        except asyncio.TimeoutError:
            consistency_task.cancel()
            
            # Enhanced logging with timeout and text preview
            text_preview = text[:50] + "..." if len(text) > 50 else text
            logger.warning(
                f"Self-Consistency íƒ€ì„ì•„ì›ƒ - ì„¤ì • íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ, "
                f"ë¶„ì„ í…ìŠ¤íŠ¸: '{text_preview}'"
            )
            
            # ì •ë¦¬ ëŒ€ê¸°
            try:
                await consistency_task
            except (asyncio.CancelledError, Exception):
                pass
            
            return {'status': 'timeout', 'confidence': 0.0}
        except Exception as e:
            logger.warning(f"Self-Consistency ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'confidence': 0.0, 'error': str(e)}
        finally:
            # ì›ë˜ ì„¤ì • ë³µì›
            self._restore_consistency_settings(detector, original_settings)

    def _apply_consistency_optimizations(self, detector) -> Dict[str, Any]:
        """Self-Consistency ìµœì í™” ì„¤ì • ì ìš©"""
        original_settings = {}
        
        # ğŸš€ ì¿¼ë¦¬ ìˆ˜ ê°ì†Œ (3ê°œ â†’ 2ê°œ)
        if hasattr(detector, 'set_query_count'):
            original_settings['query_count'] = getattr(detector, 'query_count', 3)
            detector.set_query_count(2)
        
        # ğŸš€ ê´€ëŒ€í•œ ì¼ì¹˜ ê¸°ì¤€ (0.8 â†’ 0.7)
        if hasattr(detector, 'set_similarity_threshold'):
            original_settings['similarity_threshold'] = getattr(detector, 'similarity_threshold', 0.8)
            detector.set_similarity_threshold(0.7)
        
        return original_settings

    def _restore_consistency_settings(self, detector, original_settings: Dict[str, Any]):
        """ì›ë˜ Self-Consistency ì„¤ì • ë³µì›"""
        for setting, value in original_settings.items():
            if hasattr(detector, f'set_{setting}'):
                getattr(detector, f'set_{setting}')(value)

    def _create_optimized_consistency_score(self):
        """ìµœì í™”ëœ ì¼ê´€ì„± ì ìˆ˜ ìƒì„± (ì§§ì€ í…ìŠ¤íŠ¸ìš©)"""
        return ConsistencyScore(
            confidence=0.6,  # ë” í˜„ì‹¤ì ì¸ ì‹ ë¢°ë„ (85% -> 60%)
            consistency_rate=0.6,
            num_queries=1,
            num_consistent=1,
            variations=["ìµœì í™”ëœ ì‘ë‹µ"],
            common_elements=["ì§§ì€ í…ìŠ¤íŠ¸"],
            divergent_elements=[],
            analysis_details={"optimized": True, "reason": "ì§§ì€ í…ìŠ¤íŠ¸ ìµœì í™” ì²˜ë¦¬"}
        )

    def _finalize_issue_validation(
        self, 
        issue: IssueItem, 
        all_scores: Dict[str, Any]
    ) -> Optional[IssueItem]:
        """ì´ìŠˆ ê²€ì¦ ìµœì¢… ì²˜ë¦¬"""
        
        if not all_scores:
            logger.warning(f"ì´ìŠˆ '{issue.title}': ëª¨ë“  íƒì§€ê¸° ì‹¤íŒ¨")
            return None
        
        # ìµœì†Œ ì„ê³„ê°’ ë¯¸ë¦¬ ì„¤ì •
        min_threshold = self.threshold_manager.thresholds.min_confidence_threshold
        
        # ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
        max_confidence = max(score.confidence for score in all_scores.values())
        weights = self.threshold_manager.get_weights_for_confidence(max_confidence)
        
        # ì ìˆ˜ í†µí•©
        combined_score = CombinedHallucinationScore(
            individual_scores=all_scores,
            weights=weights,
            final_confidence=0
        )
        
        # ê²°ê³¼ë¥¼ ì´ìŠˆì— ì¶”ê°€
        setattr(issue, 'hallucination_analysis', combined_score)
        setattr(issue, 'hallucination_confidence', combined_score.final_confidence)
        
        # ë””ë²„ê¹…ìš© ìƒì„¸ ë¡œê·¸
        logger.debug(f"ì´ìŠˆ '{issue.title[:30]}...' í™˜ê° íƒì§€ ì™„ë£Œ:")
        logger.debug(f"  - ê°œë³„ ì ìˆ˜: {[(k, f'{v.confidence:.3f}') for k, v in all_scores.items()]}")
        logger.debug(f"  - ìµœì¢… ì‹ ë¢°ë„: {combined_score.final_confidence:.3f}")
        logger.debug(f"  - ì„ê³„ê°’ í†µê³¼: {combined_score.final_confidence >= min_threshold}")
        if combined_score.final_confidence < min_threshold:
            logger.debug(
                f"ì´ìŠˆ '{issue.title}' ì‹ ë¢°ë„ ë¶€ì¡±: "
                f"{combined_score.final_confidence:.2f} < {min_threshold:.2f}"
            )
            return None
        
        # ì„±ê³µ ë¡œê¹…
        detection_summary = ", ".join([
            f"{k}: {v.confidence:.2f}"
            for k, v in all_scores.items()
        ])
        
        logger.debug(
            f"ì´ìŠˆ '{issue.title[:30]}' ì™„ì „ ìµœì í™” ê²€ì¦ ì™„ë£Œ - "
            f"íƒì§€ê¸°: [{detection_summary}], ìµœì¢…: {combined_score.final_confidence:.2f}"
        )
        
        return issue

    def _update_final_metrics(self, search_result: SearchResult):
        """ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸"""
        # ìºì‹œ í†µê³„ í¬í•¨
        cache_stats = self.cache.get_stats()
        self.metrics.cache_hits = cache_stats['hits']
        self.metrics.cache_misses = cache_stats['misses']
        
        # ê²€ìƒ‰ ê²°ê³¼ì— ì„±ëŠ¥ ì§€í‘œ ì¶”ê°€ (Progressive Deepening í¬í•¨)
        search_result.performance_metrics = {
            'total_processing_time': self.metrics.total_processing_time,
            'avg_issue_time': self.metrics.avg_issue_time,
            'cache_hit_ratio': self.metrics.calculate_cache_hit_ratio(),
            'self_consistency_skip_ratio': self.metrics.calculate_self_consistency_skip_ratio(),
            'avg_timeout_used': sum(self.metrics.timeout_utilizations) / len(self.metrics.timeout_utilizations) if self.metrics.timeout_utilizations else 0,
            # Progressive Deepening í†µê³„
            'progressive_deepening': {
                'triggered': self.metrics.progressive_deepening_triggered,
                'successful': self.metrics.progressive_deepening_successful,
                'failed': self.metrics.progressive_deepening_failed,
                'success_rate': (self.metrics.progressive_deepening_successful / max(1, self.metrics.progressive_deepening_triggered)) * 100,
                'initial_low_confidence_issues': self.metrics.initial_low_confidence_issues
            }
        }

    def _create_enhanced_regeneration_prompt(
        self, 
        failed_keywords: KeywordResult, 
        search_result: SearchResult,
        failure_type: str,
        high_confidence_issues: Optional[List[IssueItem]] = None,
        all_validated_issues: Optional[List[IssueItem]] = None
    ) -> str:
        """
        ì‹¤íŒ¨í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ í–¥ìƒëœ í‚¤ì›Œë“œ ì¬ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            failed_keywords: ì‹¤íŒ¨í•œ í‚¤ì›Œë“œ ê²°ê³¼
            search_result: ê²€ìƒ‰ ê²°ê³¼
            failure_type: ì‹¤íŒ¨ ìœ í˜• ('no_results', 'low_quality_results', 'insufficient_confidence')
            high_confidence_issues: ë†’ì€ ì‹ ë¢°ë„ ì´ìŠˆë“¤ (ìˆëŠ” ê²½ìš°)
            all_validated_issues: ëª¨ë“  ê²€ì¦ëœ ì´ìŠˆë“¤ (ìˆëŠ” ê²½ìš°)
        
        Returns:
            str: í–¥ìƒëœ í‚¤ì›Œë“œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        """
        original_topic = failed_keywords.topic
        previous_keywords = failed_keywords.primary_keywords + failed_keywords.related_terms + failed_keywords.context_keywords
        
        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        base_context = f"ì£¼ì œ '{original_topic}'ì— ëŒ€í•œ í‚¤ì›Œë“œ ì¬ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        if failure_type == "no_results":
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            enhanced_prompt = (
                f"{base_context} "
                f"ì´ì „ í‚¤ì›Œë“œ [{', '.join(previous_keywords[:10])}]ë¡œ ê²€ìƒ‰í–ˆì§€ë§Œ ê²°ê³¼ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤. "
                f"ë‹¤ìŒì„ ê³ ë ¤í•˜ì—¬ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n"
                f"1. ë” êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ì¸ ê¸°ìˆ  ìš©ì–´ ì‚¬ìš©\n"
                f"2. ë™ì˜ì–´ë‚˜ ëŒ€ì²´ í‘œí˜„ í™œìš©\n"
                f"3. ìµœì‹  íŠ¸ë Œë“œë‚˜ ì—…ë°ì´íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨\n"
                f"4. 'introduction', 'basics', 'tutorial' ê°™ì€ ì´ˆë³´ì ëŒ€ìƒ í‚¤ì›Œë“œ ì œì™¸\n"
                f"ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ '{original_topic}'ì˜ í•µì‹¬ ê¸°ìˆ ì  ì¸¡ë©´ì„ ë‹¤ë£¨ëŠ” í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
            )
            
        elif failure_type == "low_quality_results":
            # ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆì§€ë§Œ í’ˆì§ˆì´ ë‚®ì€ ê²½ìš°
            issue_titles = [issue.title for issue in search_result.issues[:5]] if search_result.issues else []
            enhanced_prompt = (
                f"{base_context} "
                f"ì´ì „ í‚¤ì›Œë“œ [{', '.join(previous_keywords[:10])}]ë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ "
                f"[{', '.join(issue_titles)}] ë“±ì´ ë‚˜ì™”ì§€ë§Œ í™˜ê° íƒì§€ë¥¼ í†µê³¼í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                f"ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
                f"1. ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ë§ˆì¼€íŒ…ì„± ì½˜í…ì¸ \n"
                f"2. ë¶€ì •í™•í•˜ê±°ë‚˜ ì˜¤ë˜ëœ ì •ë³´\n"
                f"3. ì£¼ì œì™€ ê´€ë ¨ì„±ì´ ë‚®ì€ ë‚´ìš©\n"
                f"ë” ì‹ ë¢°í•  ìˆ˜ ìˆê³  ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ë‹¤ìŒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:\n"
                f"- ê³µì‹ ë¬¸ì„œë‚˜ ê¸°ìˆ  ë¸”ë¡œê·¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì „ë¬¸ ìš©ì–´\n"
                f"- íŠ¹ì • ë²„ì „ì´ë‚˜ ë¦´ë¦¬ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ\n"
                f"- ê°œë°œì ì»¤ë®¤ë‹ˆí‹°ì—ì„œ í™œë°œíˆ ë…¼ì˜ë˜ëŠ” ì£¼ì œ\n"
                f"'{original_topic}'ì— ëŒ€í•´ ë” ì „ë¬¸ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
            )
            
        elif failure_type == "insufficient_confidence":
            # ê²€ì¦ëœ ì´ìŠˆëŠ” ìˆì§€ë§Œ ë†’ì€ ì‹ ë¢°ë„ ì´ìŠˆê°€ ë¶€ì¡±í•œ ê²½ìš°
            low_confidence_titles = []
            if all_validated_issues and high_confidence_issues is not None:
                low_confidence_issues = [
                    issue for issue in all_validated_issues 
                    if issue not in high_confidence_issues
                ]
                low_confidence_titles = [issue.title for issue in low_confidence_issues[:3]]
            
            enhanced_prompt = (
                f"{base_context} "
                f"ì´ì „ í‚¤ì›Œë“œ [{', '.join(previous_keywords[:10])}]ë¡œ ê²€ìƒ‰í•˜ì—¬ "
                f"ì¼ë¶€ ê²°ê³¼ë¥¼ ì–»ì—ˆì§€ë§Œ ë†’ì€ ì‹ ë¢°ë„ ì´ìŠˆê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. "
            )
            
            if low_confidence_titles:
                enhanced_prompt += (
                    f"ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ ì˜ˆì‹œ: [{', '.join(low_confidence_titles)}]. "
                )
            
            enhanced_prompt += (
                f"ë” ë†’ì€ í’ˆì§ˆì˜ ê²°ê³¼ë¥¼ ìœ„í•´ ë‹¤ìŒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:\n"
                f"1. ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ê¸°ìˆ  í‚¤ì›Œë“œ\n"
                f"2. ìµœì‹  ê°œë°œ ë™í–¥ì´ë‚˜ ì—…ë°ì´íŠ¸ ê´€ë ¨ ìš©ì–´\n"
                f"3. ì‹¤ì œ êµ¬í˜„ì´ë‚˜ ì‚¬ìš© ì‚¬ë¡€ ê´€ë ¨ í‚¤ì›Œë“œ\n"
                f"4. ë¬¸ì œ í•´ê²°ì´ë‚˜ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê´€ë ¨ ìš©ì–´\n"
                f"5. 'how to', 'guide', 'tips' ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ëŒ€ì‹  êµ¬ì²´ì ì¸ ì•¡ì…˜ í‚¤ì›Œë“œ\n"
                f"'{original_topic}'ì— ëŒ€í•´ ë” ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
            )
        
        else:
            # ê¸°ë³¸ í´ë°±
            enhanced_prompt = (
                f"{base_context} "
                f"ì´ì „ í‚¤ì›Œë“œ [{', '.join(previous_keywords[:8])}]ê°€ íš¨ê³¼ì ì´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                f"'{original_topic}'ì˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ì¸¡ë©´ì´ë‚˜ ë” ì „ë¬¸ì ì¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ "
                f"ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
            )
        
        return enhanced_prompt
    
    def _get_keyword_signature(self, keyword_result: KeywordResult) -> str:
        """
        í‚¤ì›Œë“œ ê²°ê³¼ì˜ ê³ ìœ  ì„œëª…ì„ ìƒì„±í•©ë‹ˆë‹¤. (ë¬´í•œ ë£¨í”„ ë°©ì§€ìš©)
        """
        all_keywords = (
            keyword_result.primary_keywords + 
            keyword_result.related_terms + 
            keyword_result.context_keywords
        )
        # ì •ë ¬ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ê³ ìœ  ì„œëª… ìƒì„±
        return '|'.join(sorted(set(kw.lower().strip() for kw in all_keywords if kw)))
    
    async def _regenerate_keywords_safely(
        self, 
        current_keywords: KeywordResult,
        search_result: SearchResult,
        failure_type: str,
        keyword_history: set,
        regeneration_count: int,
        max_regenerations: int,
        high_confidence_issues=None,
        all_validated_issues=None
    ) -> tuple:
        """
        ì•ˆì „í•œ í‚¤ì›Œë“œ ì¬ìƒì„± (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        
        Returns:
            tuple: (new_keywords, updated_regeneration_count, should_continue)
        """
        if regeneration_count >= max_regenerations:
            logger.warning(f"ìµœëŒ€ í‚¤ì›Œë“œ ì¬ìƒì„± íšŸìˆ˜({max_regenerations})ì— ë„ë‹¬. ì¬ìƒì„± ì¤‘ë‹¨.")
            return current_keywords, regeneration_count, False
        
        enhanced_prompt = self._create_enhanced_regeneration_prompt(
            current_keywords, 
            search_result, 
            failure_type,
            high_confidence_issues,
            all_validated_issues
        )
        
        # ì£¼ì œ ì—°ê´€ì„± ë³´ì¡´: ì›ë³¸ ì£¼ì œë¥¼ enhanced_promptì— ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
        original_topic = current_keywords.topic
        topic_preserved_prompt = f"ì›ë³¸ ì£¼ì œ '{original_topic}'ì— ì§‘ì¤‘í•˜ì—¬: {enhanced_prompt}"
        
        new_keywords = await generate_keywords_for_topic(topic_preserved_prompt)
        new_signature = self._get_keyword_signature(new_keywords)
        
        # ì¤‘ë³µ í‚¤ì›Œë“œ ê²€ì‚¬
        if new_signature in keyword_history:
            logger.warning(
                f"ë™ì¼í•œ í‚¤ì›Œë“œ ì¡°í•©ì´ ì¬ìƒì„±ë¨ (ì‹œë„ {regeneration_count + 1}). "
                f"ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•´ ì¬ìƒì„± ì¤‘ë‹¨."
            )
            return current_keywords, regeneration_count, False
        
        keyword_history.add(new_signature)
        regeneration_count += 1
        
        logger.info(f"ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ (ì¬ìƒì„± íšŸìˆ˜: {regeneration_count}/{max_regenerations})")
        
        return new_keywords, regeneration_count, True

    async def _apply_fallback_if_needed(self, search_result: SearchResult, original_topic: str) -> SearchResult:
        """
        ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•©ë‹ˆë‹¤.
        
        í´ë°± ì „ëµ:
        1. ì„ê³„ê°’ì„ ë” ë‚®ì¶°ì„œ ê¸°ì¡´ ê²°ê³¼ ì¬í‰ê°€
        2. ì›ë³¸ ì£¼ì œì˜ í•µì‹¬ í‚¤ì›Œë“œë¡œ ê°„ë‹¨í•œ ê²€ìƒ‰ ì¬ì‹œë„
        3. ìµœì†Œí•œì˜ ìœ íš¨í•œ ê²°ê³¼ë¼ë„ ë°˜í™˜í•˜ë„ë¡ ë³´ì¥
        """
        if not search_result.issues:
            logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì™„ì „íˆ ë¹„ì–´ìˆìŒ. í´ë°± ê²€ìƒ‰ ì‹¤í–‰.")
            return await self._execute_fallback_search(original_topic)
        
        # ë†’ì€ ì‹ ë¢°ë„ ì´ìŠˆ ê°œìˆ˜ í™•ì¸
        high_confidence_count = sum(
            1 for issue in search_result.issues 
            if getattr(issue, 'hallucination_confidence', 0) >= 0.7
        )
        
        # í´ë°± ì¡°ê±´: ë†’ì€ ì‹ ë¢°ë„ ì´ìŠˆê°€ 1ê°œ ë¯¸ë§Œì´ê³  ì „ì²´ ì´ìŠˆë„ 3ê°œ ë¯¸ë§Œ
        if high_confidence_count < 1 and len(search_result.issues) < 3:
            logger.warning(
                f"ê²°ê³¼ ë¶€ì¡± (ë†’ì€ ì‹ ë¢°ë„: {high_confidence_count}ê°œ, ì „ì²´: {len(search_result.issues)}ê°œ). "
                f"í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì‹¤í–‰."
            )
            
            # ì„ê³„ê°’ì„ ëŒ€í­ ë‚®ì¶°ì„œ ê¸°ì¡´ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í‰ê°€
            original_threshold = self.threshold_manager.thresholds.min_confidence_threshold
            try:
                # ì„ì‹œë¡œ ì„ê³„ê°’ì„ 0.2ë¡œ ë‚®ì¶¤
                self.threshold_manager.thresholds.min_confidence_threshold = 0.2
                logger.info("ì„ê³„ê°’ì„ 0.2ë¡œ ë‚®ì¶°ì„œ ê¸°ì¡´ ì´ìŠˆ ì¬í‰ê°€ ì¤‘...")
                
                # ì›ë³¸ ì´ìŠˆë“¤ì„ ë” ê´€ëŒ€í•œ ê¸°ì¤€ìœ¼ë¡œ ì¬ê²€ì¦
                revalidated = []
                for issue in search_result.issues:
                    # ê¸°ì¡´ í™˜ê° ë¶„ì„ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ì¬í‰ê°€
                    if hasattr(issue, 'hallucination_confidence'):
                        if issue.hallucination_confidence >= 0.2:
                            revalidated.append(issue)
                
                if revalidated:
                    logger.info(f"ì¬í‰ê°€ë¡œ {len(revalidated)}ê°œ ì´ìŠˆ ë³µêµ¬ë¨")
                    search_result.issues = revalidated
                    return search_result
                    
            finally:
                # ì›ë˜ ì„ê³„ê°’ ë³µì›
                self.threshold_manager.thresholds.min_confidence_threshold = original_threshold
            
            # ì¬í‰ê°€ë¡œë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ í´ë°± ê²€ìƒ‰
            return await self._execute_fallback_search(original_topic)
        
        return search_result
    
    async def _execute_fallback_search(self, original_topic: str) -> SearchResult:
        """
        ì›ë³¸ ì£¼ì œì˜ í•µì‹¬ í‚¤ì›Œë“œë¡œ ê°„ë‹¨í•œ í´ë°± ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        logger.info(f"í´ë°± ê²€ìƒ‰ ì‹¤í–‰: '{original_topic}'")
        
        # ì›ë³¸ ì£¼ì œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        fallback_keywords = self._extract_core_keywords(original_topic)
        
        try:
            # ê¸°ë³¸ ì´ìŠˆ ê²€ìƒ‰ê¸°ë¡œ ì§ì ‘ ê²€ìƒ‰ (í™˜ê° íƒì§€ ì—†ì´)
            fallback_result = await self.issue_searcher.search_issues(fallback_keywords)
            
            if fallback_result.issues:
                # ìµœì†Œí•œì˜ ê²€ì¦ë§Œ ì ìš© (ë§¤ìš° ê´€ëŒ€í•œ ê¸°ì¤€)
                validated_fallback = []
                for issue in fallback_result.issues[:5]:  # ìµœëŒ€ 5ê°œë§Œ
                    # ê¸°ë³¸ì ì¸ ì‹ ë¢°ë„ë§Œ í• ë‹¹ (í™˜ê° íƒì§€ ê±´ë„ˆë›°ê¸°)
                    setattr(issue, 'hallucination_confidence', 0.3)
                    setattr(issue, 'fallback_result', True)
                    validated_fallback.append(issue)
                
                fallback_result.issues = validated_fallback
                logger.info(f"í´ë°± ê²€ìƒ‰ ì„±ê³µ: {len(validated_fallback)}ê°œ ì´ìŠˆ ë°˜í™˜")
                return fallback_result
            
        except Exception as e:
            logger.error(f"í´ë°± ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ê²°ê³¼ë¼ë„ ìœ íš¨í•œ êµ¬ì¡°ë¡œ ë°˜í™˜
        logger.warning("ëª¨ë“  í´ë°± ì‹œë„ ì‹¤íŒ¨. ë¹ˆ ê²°ê³¼ ë°˜í™˜.")
        return SearchResult(
            query_keywords=fallback_keywords,
            issues=[],
            total_found=0,
            search_time=0.1,
            time_period="fallback",
            api_calls_used=0,
            confidence_score=0.0,
            raw_responses=["fallback search executed"]
        )
    
    def _extract_core_keywords(self, topic: str) -> List[str]:
        """ì£¼ì œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # iOS ê´€ë ¨ ì£¼ì œ ì²˜ë¦¬
        if 'ios' in topic.lower():
            return ['iOS', 'Apple', 'iPhone', 'iPad', 'App Store']
        elif 'wwdc' in topic.lower():
            return ['WWDC', 'Apple', 'iOS', 'macOS', 'developer']
        elif 'apple' in topic.lower():
            return ['Apple', 'iOS', 'macOS', 'iPhone', 'iPad']
        
        # ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ì–´ ë¶„ë¦¬)
        words = topic.lower().split()
        important_words = [w for w in words if len(w) > 3 and w not in ['the', 'and', 'for', 'with']]
        
        # ìµœì†Œ 1ê°œì˜ í‚¤ì›Œë“œëŠ” ë³´ì¥
        return important_words[:3] if important_words else [topic]

    def _log_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥ (Progressive Deepening í¬í•¨)"""
        cache_stats = self.cache.get_stats()
        
        # Progressive Deepening í†µê³„ ê³„ì‚°
        pd_success_rate = 0.0
        if self.metrics.progressive_deepening_triggered > 0:
            pd_success_rate = (
                self.metrics.progressive_deepening_successful / 
                self.metrics.progressive_deepening_triggered
            ) * 100
        
        logger.info(
            f"ğŸ¯ ì™„ì „ ìµœì í™” ì„±ëŠ¥ ë¦¬í¬íŠ¸ (Progressive Deepening í¬í•¨):\n"
            f"  â€¢ ì´ ì²˜ë¦¬ ì´ìŠˆ: {self.metrics.total_issues_processed}ê°œ\n"
            f"  â€¢ í‰ê·  ì´ìŠˆ ì²˜ë¦¬ ì‹œê°„: {self.metrics.avg_issue_time:.3f}ì´ˆ\n"
            f"  â€¢ ìºì‹œ íˆíŠ¸ìœ¨: {cache_stats['hit_ratio']:.1%} ({cache_stats['hits']}/{cache_stats['hits'] + cache_stats['misses']})\n"
            f"  â€¢ Self-Consistency ìŠ¤í‚µìœ¨: {self.metrics.calculate_self_consistency_skip_ratio():.1%}\n"
            f"  â€¢ í‰ê·  íƒ€ì„ì•„ì›ƒ: {sum(self.metrics.timeout_utilizations) / len(self.metrics.timeout_utilizations) if self.metrics.timeout_utilizations else 0:.1f}ì´ˆ\n"
            f"  â€¢ ë™ì‹œ ì‹¤í–‰ ìˆ˜: {self.max_concurrent_issues}ê°œ\n"
            f"  ğŸ†• Progressive Deepening í†µê³„:\n"
            f"    - ë‚®ì€ ì‹ ë¢°ë„ ì´ìŠˆ: {self.metrics.initial_low_confidence_issues}ê°œ\n"
            f"    - Deepening íŠ¸ë¦¬ê±°: {self.metrics.progressive_deepening_triggered}ê°œ\n"
            f"    - Deepening ì„±ê³µ: {self.metrics.progressive_deepening_successful}ê°œ\n"
            f"    - Deepening ì‹¤íŒ¨: {self.metrics.progressive_deepening_failed}ê°œ\n"
            f"    - ì„±ê³µë¥ : {pd_success_rate:.1f}%"
        )

    # ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ìœ ì§€
    def get_detector_status(self) -> Dict[str, bool]:
        """ê° íƒì§€ê¸°ì˜ í™œì„±í™” ìƒíƒœ ë°˜í™˜"""
        return {
            name: detector.is_initialized if hasattr(detector, 'is_initialized') else True
            for name, detector in self.detectors.items()
        }
    
    def update_threshold_config(self, new_config: ThresholdManager):
        """ì„ê³„ê°’ ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.threshold_manager = new_config
        logger.info("ì„ê³„ê°’ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache = SmartCache(max_size=1000, ttl_seconds=3600)
        logger.info("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ ë°˜í™˜"""
        return {
            'processing_metrics': {
                'total_issues': self.metrics.total_issues_processed,
                'avg_time_per_issue': self.metrics.avg_issue_time,
                'total_time': self.metrics.total_processing_time
            },
            'cache_metrics': self.cache.get_stats(),
            'optimization_metrics': {
                'self_consistency_skip_ratio': self.metrics.calculate_self_consistency_skip_ratio(),
                'avg_timeout': sum(self.metrics.timeout_utilizations) / len(self.metrics.timeout_utilizations) if self.metrics.timeout_utilizations else 0,
                'max_concurrent': self.max_concurrent_issues
            }
        }