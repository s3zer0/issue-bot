"""
í™˜ê° íƒì§€ê°€ í†µí•©ëœ í–¥ìƒëœ ì´ìŠˆ ê²€ìƒ‰ê¸°
"""

import asyncio
import time
import hashlib
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field
from loguru import logger

from src.models import KeywordResult, IssueItem, SearchResult
from src.issue_searcher import create_issue_searcher
from src.keyword_generator import generate_keywords_for_topic
from src.hallucination_detection.threshold_manager import ThresholdManager
from src.hallucination_detection.reppl_detector import RePPLDetector
from src.hallucination_detection.consistency_checker import SelfConsistencyChecker
from src.hallucination_detection.models import CombinedHallucinationScore


@dataclass
class OptimizationMetrics:
    """ìµœì í™” ì„±ëŠ¥ ì§€í‘œ"""
    total_issues_processed: int = 0
    total_processing_time: float = 0.0
    avg_issue_time: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0
    self_consistency_runs: int = 0
    self_consistency_skips: int = 0
    timeout_utilizations: List[float] = field(default_factory=list)
    
    def calculate_cache_hit_ratio(self) -> float:
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0
    
    def calculate_self_consistency_skip_ratio(self) -> float:
        total = self.self_consistency_runs + self.self_consistency_skips
        return self.self_consistency_skips / total if total > 0 else 0.0


class SmartCache:
    """
    ğŸš€ ì¶”ê°€ ìµœì í™”: ìŠ¤ë§ˆíŠ¸ ìºì‹± ì‹œìŠ¤í…œ
    
    ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ í™˜ê° íƒì§€ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬
    ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ë°©ì§€í•˜ê³  ì„±ëŠ¥ì„ ëŒ€í­ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, text: str, context: str = "") -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        combined = f"{text}:{context}"
        return hashlib.md5(combined.encode()).hexdigest()
    
    def get(self, text: str, context: str = "") -> Optional[Any]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        key = self._generate_key(text, context)
        current_time = time.time()
        
        if key in self.cache:
            stored_time, result = self.cache[key]
            
            # TTL ì²´í¬
            if current_time - stored_time < self.ttl_seconds:
                self.access_times[key] = current_time
                self.hits += 1
                logger.debug(f"ìºì‹œ íˆíŠ¸: {key[:8]}")
                return result
            else:
                # ë§Œë£Œëœ í•­ëª© ì œê±°
                del self.cache[key]
                del self.access_times[key]
        
        self.misses += 1
        return None
    
    def put(self, text: str, result: Any, context: str = ""):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        key = self._generate_key(text, context)
        current_time = time.time()
        
        # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        self.cache[key] = (current_time, result)
        self.access_times[key] = current_time
        logger.debug(f"ìºì‹œ ì €ì¥: {key[:8]}")
    
    def _evict_oldest(self):
        """ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)"""
        if not self.access_times:
            return
        
        oldest_key = min(self.access_times, key=self.access_times.get)
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
        logger.debug(f"ìºì‹œ ì œê±°: {oldest_key[:8]}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total = self.hits + self.misses
        hit_ratio = self.hits / total if total > 0 else 0
        
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_ratio': hit_ratio,
            'cache_size': len(self.cache),
            'max_size': self.max_size
        }


class EnhancedIssueSearcher:
    """ì—¬ëŸ¬ í™˜ê° íƒì§€ ë°©ë²•ì´ í†µí•©ëœ ì´ìŠˆ ê²€ìƒ‰ê¸° (ì™„ì „ ìµœì í™” ë²„ì „)."""

    def __init__(
            self,
            api_key: Optional[str] = None,
            enable_reppl: bool = True,
            enable_consistency: bool = True,
            enable_llm_judge: bool = True,
            threshold_manager: Optional[ThresholdManager] = None
    ):
        """
        í™˜ê° íƒì§€ ê¸°ëŠ¥ì´ ê°•í™”ëœ ì´ìŠˆ ê²€ìƒ‰ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        # ê¸°ë³¸ ì´ìŠˆ ê²€ìƒ‰ê¸°
        self.base_searcher = create_issue_searcher(api_key)

        # ì„ê³„ê°’ ê´€ë¦¬ì
        self.threshold_manager = threshold_manager or ThresholdManager()

        # ğŸš€ ìµœì í™” ì„¤ì •
        self.max_concurrent_issues = 8  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¦ê°€ (5ê°œ â†’ 8ê°œ)
        self.self_consistency_threshold = 0.6  # SC ì‹¤í–‰ ì„ê³„ê°’
        
        # ğŸš€ ì¶”ê°€ ìµœì í™”: ìŠ¤ë§ˆíŠ¸ ìºì‹±
        self.cache = SmartCache(max_size=1000, ttl_seconds=3600)
        
        # ì„±ëŠ¥ ì§€í‘œ ì¶”ì 
        self.metrics = OptimizationMetrics()
        
        # ì ì‘í˜• íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.timeout_config = {
            'base_timeout': 20.0,
            'max_timeout': 45.0,
            'chars_per_second_ratio': 500,
            'additional_per_500chars': 5.0
        }

        # í™˜ê° íƒì§€ê¸°ë“¤ ì´ˆê¸°í™”
        self.detectors = {}
        if enable_reppl:
            self.detectors['RePPL'] = RePPLDetector()
        if enable_consistency:
            self.detectors['Self-Consistency'] = SelfConsistencyChecker()
        if enable_llm_judge:
            try:
                from src.hallucination_detection.llm_judge import LLMJudgeDetector
                self.detectors['LLM-Judge'] = LLMJudgeDetector()
            except ImportError:
                logger.warning("LLM Judge íƒì§€ê¸°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        logger.info(
            f"ì™„ì „ ìµœì í™”ëœ ì´ìŠˆ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ "
            f"(í™œì„± íƒì§€ê¸°: {list(self.detectors.keys())}, "
            f"ë™ì‹œ ì‹¤í–‰: {self.max_concurrent_issues}ê°œ, "
            f"ìµœì†Œ ì‹ ë¢°ë„: {self.threshold_manager.thresholds.min_confidence_threshold})"
        )

    def _calculate_adaptive_timeout(self, text: str) -> float:
        """
        ğŸš€ ê°œì„ ì‚¬í•­ 1: í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ì ì‘í˜• íƒ€ì„ì•„ì›ƒ ê³„ì‚°
        """
        text_length = len(text)
        base = self.timeout_config['base_timeout']
        max_timeout = self.timeout_config['max_timeout']
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ë³„ ì¶”ê°€ ì‹œê°„ ê³„ì‚°
        additional = min(
            max_timeout - base,
            (text_length / self.timeout_config['chars_per_second_ratio']) * 
            self.timeout_config['additional_per_500chars']
        )
        
        calculated_timeout = base + additional
        self.metrics.timeout_utilizations.append(calculated_timeout)
        
        logger.debug(f"ì ì‘í˜• íƒ€ì„ì•„ì›ƒ: {text_length}ì â†’ {calculated_timeout:.1f}ì´ˆ")
        return calculated_timeout

    def _should_run_self_consistency(self, priority_confidence: float) -> bool:
        """
        ğŸš€ ê°œì„ ì‚¬í•­ 2: ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ê²°ê³¼ì— ë”°ë¥¸ Self-Consistency ì‹¤í–‰ ê²°ì •
        """
        should_run = priority_confidence >= self.self_consistency_threshold
        
        if should_run:
            self.metrics.self_consistency_runs += 1
        else:
            self.metrics.self_consistency_skips += 1
        
        logger.debug(
            f"Self-Consistency ê²°ì •: ìš°ì„ ìˆœìœ„ {priority_confidence:.2f} "
            f"{'â‰¥' if should_run else '<'} {self.self_consistency_threshold:.2f} â†’ "
            f"{'ì‹¤í–‰' if should_run else 'ìŠ¤í‚µ'}"
        )
        
        return should_run

    async def search_with_validation(
            self,
            keyword_result: KeywordResult,
            time_period: str,
            max_retries: int = 3
    ) -> SearchResult:
        """
        í‚¤ì›Œë“œë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  í™˜ê° íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ìµœì í™” ë²„ì „)
        """
        logger.info(f"ìµœì í™”ëœ ê²€ìƒ‰ ì‹œì‘: ì£¼ì œ '{keyword_result.topic}', ê¸°ê°„ '{time_period}'")
        overall_start = time.time()
        
        current_keywords = keyword_result
        all_attempts_issues = []

        for attempt in range(max_retries):
            logger.info(f"ì´ìŠˆ ê²€ìƒ‰ ì‹œë„ {attempt + 1}/{max_retries}")

            # 1. ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
            search_result = await self.base_searcher.search_issues_from_keywords(
                current_keywords, time_period, collect_details=True
            )

            if not search_result.issues:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                if attempt < max_retries - 1:
                    current_keywords = await generate_keywords_for_topic(
                        f"{current_keywords.topic}ì˜ ë‹¤ë¥¸ ì¸¡ë©´"
                    )
                continue

            all_attempts_issues.extend(search_result.issues)

            # 2. ğŸš€ ìµœì í™”ëœ í™˜ê° íƒì§€ ìˆ˜í–‰
            validated_issues = await self._validate_issues_optimized(
                search_result.issues, current_keywords
            )

            if not validated_issues:
                logger.warning("í™˜ê° íƒì§€ë¥¼ í†µê³¼í•œ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
                if attempt < max_retries - 1:
                    current_keywords = await generate_keywords_for_topic(
                        f"{current_keywords.topic}ì˜ ë‹¤ë¥¸ ì¸¡ë©´"
                    )
                continue

            # 3. ì‹ ë¢°ë„ë³„ ë¶„ë¥˜ ë° ê²°ê³¼ í‰ê°€
            high_confidence_issues = [
                issue for issue in validated_issues
                if getattr(issue, 'hallucination_confidence', 0) >= 0.7
            ]

            # ì„±ê³µ ì¡°ê±´ í™•ì¸
            if len(high_confidence_issues) >= min(3, len(validated_issues) // 2):
                logger.info(
                    f"ìµœì í™”ëœ ê²€ìƒ‰ ì„±ê³µ: {len(validated_issues)}ê°œ ì´ìŠˆ ì¤‘ "
                    f"{len(high_confidence_issues)}ê°œ ë†’ì€ ì‹ ë¢°ë„"
                )
                
                # ìµœì¢… ê²°ê³¼ ì—…ë°ì´íŠ¸
                search_result.issues = validated_issues
                search_result.search_time = time.time() - overall_start
                
                # ğŸš€ ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
                self._update_final_metrics(search_result)
                break
            else:
                # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ í‚¤ì›Œë“œ ì¬ìƒì„±
                logger.info(
                    f"ë†’ì€ ì‹ ë¢°ë„ ì´ìŠˆ ë¶€ì¡± ({len(high_confidence_issues)}ê°œ), "
                    f"í‚¤ì›Œë“œ ì¬ìƒì„± ì¤‘..."
                )
                current_keywords = await generate_keywords_for_topic(
                    f"{current_keywords.topic}ì˜ ë‹¤ë¥¸ ì¸¡ë©´"
                )

        # ğŸš€ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥
        self._log_performance_report()
        
        return search_result

    async def _validate_issues_optimized(
        self,
        issues: List[IssueItem],
        keyword_result: KeywordResult
    ) -> List[IssueItem]:
        """
        ğŸš€ ëª¨ë“  ìµœì í™”ê°€ ì ìš©ëœ ì´ìŠˆ ê²€ì¦
        
        í¬í•¨ëœ ìµœì í™”:
        - ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¦ê°€ (8ê°œ)
        - ìŠ¤ë§ˆíŠ¸ ìºì‹±
        - ì ì‘í˜• íƒ€ì„ì•„ì›ƒ
        - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²˜ë¦¬
        - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬
        """
        if not issues:
            return []

        logger.info(f"ì™„ì „ ìµœì í™”ëœ í™˜ê° íƒì§€ ì‹œì‘: {len(issues)}ê°œ ì´ìŠˆ")
        start_time = time.time()
        
        # ğŸš€ ê°œì„ ì‚¬í•­ 4: ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¦ê°€
        semaphore = asyncio.Semaphore(self.max_concurrent_issues)

        async def validate_with_optimizations(issue):
            async with semaphore:
                return await self._validate_single_issue_with_cache(issue, keyword_result)

        # ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬
        batch_size = self.max_concurrent_issues * 2
        all_validated = []
        
        for i in range(0, len(issues), batch_size):
            batch = issues[i:i + batch_size]
            logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬: {i+1}-{min(i+batch_size, len(issues))}/{len(issues)}")
            
            # ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬
            batch_tasks = [validate_with_optimizations(issue) for issue in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìˆ˜ì§‘
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    logger.error(f"ì´ìŠˆ {i+j+1} ê²€ì¦ ì‹¤íŒ¨: {result}")
                elif result is not None:
                    all_validated.append(result)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ íœ´ì‹ (ë©”ëª¨ë¦¬ ì •ë¦¬)
            if i + batch_size < len(issues):
                await asyncio.sleep(0.1)

        # ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
        processing_time = time.time() - start_time
        self.metrics.total_issues_processed += len(issues)
        self.metrics.total_processing_time += processing_time
        self.metrics.avg_issue_time = (
            self.metrics.total_processing_time / self.metrics.total_issues_processed
        )

        logger.info(
            f"ì™„ì „ ìµœì í™” ê²€ì¦ ì™„ë£Œ ({processing_time:.2f}ì´ˆ): "
            f"{len(issues)}ê°œ â†’ {len(all_validated)}ê°œ í†µê³¼"
        )

        return all_validated

    async def _validate_single_issue_with_cache(
        self,
        issue: IssueItem,
        keyword_result: KeywordResult
    ) -> Optional[IssueItem]:
        """
        ğŸš€ ìºì‹±ì´ ì ìš©ëœ ë‹¨ì¼ ì´ìŠˆ ê²€ì¦
        """
        # ìºì‹œ í™•ì¸
        cache_key_text = f"{issue.title}:{issue.summary}"
        cached_result = self.cache.get(cache_key_text, keyword_result.topic)
        
        if cached_result is not None:
            logger.debug(f"ìºì‹œ íˆíŠ¸: {issue.title[:30]}")
            # ìºì‹œëœ ê²°ê³¼ë¥¼ ì´ìŠˆì— ì ìš©
            setattr(issue, 'hallucination_analysis', cached_result['analysis'])
            setattr(issue, 'hallucination_confidence', cached_result['confidence'])
            return issue if cached_result['is_valid'] else None
        
        # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ê²€ì¦ ìˆ˜í–‰
        logger.debug(f"ìºì‹œ ë¯¸ìŠ¤: {issue.title[:30]}")
        result = await self._validate_single_issue_optimized(issue, keyword_result)
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        if result is not None:
            cache_data = {
                'analysis': getattr(result, 'hallucination_analysis', None),
                'confidence': getattr(result, 'hallucination_confidence', 0.0),
                'is_valid': True
            }
        else:
            cache_data = {
                'analysis': None,
                'confidence': 0.0,
                'is_valid': False
            }
        
        self.cache.put(cache_key_text, cache_data, keyword_result.topic)
        return result

    async def _validate_single_issue_optimized(
        self,
        issue: IssueItem,
        keyword_result: KeywordResult
    ) -> Optional[IssueItem]:
        """
        ğŸš€ ë‹¨ì¼ ì´ìŠˆ ì™„ì „ ìµœì í™” ê²€ì¦
        """
        topic = keyword_result.topic
        
        try:
            # ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„
            analysis_text = self._prepare_analysis_text(issue)
            
            # ğŸš€ ê°œì„ ì‚¬í•­ 1: ì ì‘í˜• íƒ€ì„ì•„ì›ƒ ê³„ì‚°
            adaptive_timeout = self._calculate_adaptive_timeout(analysis_text)
            
            # === 1ë‹¨ê³„: ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ì‹¤í–‰ ===
            priority_scores = await self._run_priority_detectors_optimized(
                issue, topic, analysis_text, adaptive_timeout * 0.7
            )
            
            # === 2ë‹¨ê³„: Self-Consistency ì¡°ê±´ë¶€ ì‹¤í–‰ ===
            all_scores = priority_scores.copy()
            
            if priority_scores:
                avg_priority_confidence = sum(
                    score.confidence for score in priority_scores.values()
                ) / len(priority_scores)
                
                # ğŸš€ ê°œì„ ì‚¬í•­ 2 & 3: ì¡°ê±´ë¶€ Self-Consistency
                if self._should_run_self_consistency(avg_priority_confidence):
                    consistency_score = await self._run_optimized_self_consistency(
                        analysis_text, issue.title, adaptive_timeout * 0.3
                    )
                    
                    if consistency_score:
                        all_scores['Self-Consistency'] = consistency_score
            
            # === 3ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° ìµœì¢… ê²€ì¦ ===
            return self._finalize_issue_validation(issue, all_scores)
            
        except Exception as e:
            logger.error(f"ì´ìŠˆ '{issue.title}' ê²€ì¦ ì˜¤ë¥˜: {e}")
            return None

    def _prepare_analysis_text(self, issue: IssueItem) -> str:
        """ë¶„ì„ìš© í…ìŠ¤íŠ¸ ìµœì í™”ëœ ì¤€ë¹„"""
        analysis_text = issue.summary
        
        # ìƒì„¸ ë‚´ìš©ì´ ìˆìœ¼ë©´ í¬í•¨ (ê¸¸ì´ ì œí•œìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´)
        if hasattr(issue, 'detailed_content') and issue.detailed_content:
            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
            max_detail_length = 1500
            detail = issue.detailed_content[:max_detail_length]
            if len(issue.detailed_content) > max_detail_length:
                detail += "..."
            
            analysis_text = f"{issue.summary}\n\n{detail}"
        
        return analysis_text

    async def _run_priority_detectors_optimized(
        self, 
        issue: IssueItem, 
        topic: str, 
        analysis_text: str, 
        timeout: float
    ) -> Dict[str, Any]:
        """ìµœì í™”ëœ ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ì‹¤í–‰"""
        
        priority_tasks = {}
        
        # RePPL ë¶„ì„ (ê°€ì¥ ë¹ ë¦„)
        if 'RePPL' in self.detectors:
            priority_tasks['RePPL'] = asyncio.create_task(
                self.detectors['RePPL'].analyze_issue(issue, topic),
                name=f"RePPL-{issue.title[:20]}"
            )
        
        # LLM Judge ê²€ì‚¬ (ì¤‘ê°„ ì†ë„)
        if 'LLM-Judge' in self.detectors:
            priority_tasks['LLM-Judge'] = asyncio.create_task(
                self.detectors['LLM-Judge'].analyze_text(
                    analysis_text,
                    context=f"ì£¼ì œ: {topic}, ì œëª©: {issue.title}"
                ),
                name=f"LLMJudge-{issue.title[:20]}"
            )
        
        # ìš°ì„ ìˆœìœ„ íƒì§€ê¸° ë³‘ë ¬ ì‹¤í–‰
        priority_scores = {}
        
        if priority_tasks:
            done, pending = await asyncio.wait(
                priority_tasks.values(),
                return_when=asyncio.ALL_COMPLETED,
                timeout=timeout
            )
            
            # ì™„ë£Œëœ íƒœìŠ¤í¬ ê²°ê³¼ ìˆ˜ì§‘
            for task in done:
                task_name = task.get_name()
                try:
                    result = await task
                    detector_type = task_name.split('-')[0]
                    if detector_type == 'LLMJudge':
                        detector_type = 'LLM-Judge'
                    
                    priority_scores[detector_type] = result
                    logger.debug(f"{detector_type} ì™„ë£Œ: {result.confidence:.2f}")
                    
                except Exception as e:
                    detector_type = task_name.split('-')[0]
                    logger.warning(f"{detector_type} ì‹¤íŒ¨: {e}")
            
            # ë¯¸ì™„ë£Œ íƒœìŠ¤í¬ ì •ë¦¬
            for task in pending:
                task.cancel()
                detector_type = task.get_name().split('-')[0]
                logger.warning(f"{detector_type} ìš°ì„ ìˆœìœ„ ë‹¨ê³„ íƒ€ì„ì•„ì›ƒ")
                
                # íƒ€ì„ì•„ì›ƒëœ íƒœìŠ¤í¬ ì •ë¦¬ ëŒ€ê¸°
                try:
                    await task
                except (asyncio.CancelledError, Exception):
                    pass
        
        return priority_scores

    async def _run_optimized_self_consistency(
        self, 
        text: str, 
        context: str, 
        timeout: float
    ) -> Optional[Any]:
        """
        ğŸš€ ê°œì„ ì‚¬í•­ 3: ìµœì í™”ëœ Self-Consistency ì‹¤í–‰
        """
        if 'Self-Consistency' not in self.detectors:
            return None
        
        detector = self.detectors['Self-Consistency']
        
        # ğŸš€ ì§§ì€ í…ìŠ¤íŠ¸ ë¹ ë¥¸ ì²˜ë¦¬
        if len(text) < 100:
            logger.debug("ì§§ì€ í…ìŠ¤íŠ¸ - Self-Consistency ê°„ì†Œí™”")
            return self._create_optimized_consistency_score()
        
        # ğŸš€ ë™ì  ìµœì í™” ì„¤ì • ì ìš©
        original_settings = self._apply_consistency_optimizations(detector)
        
        try:
            # Self-Consistency ì‹¤í–‰
            consistency_task = asyncio.create_task(
                detector.analyze_text(text, context=context)
            )
            
            result = await asyncio.wait_for(consistency_task, timeout=timeout)
            return result
            
        except asyncio.TimeoutError:
            consistency_task.cancel()
            logger.warning("Self-Consistency íƒ€ì„ì•„ì›ƒ")
            
            # ì •ë¦¬ ëŒ€ê¸°
            try:
                await consistency_task
            except (asyncio.CancelledError, Exception):
                pass
            
            return None
        except Exception as e:
            logger.warning(f"Self-Consistency ì‹¤íŒ¨: {e}")
            return None
        finally:
            # ì›ë˜ ì„¤ì • ë³µì›
            self._restore_consistency_settings(detector, original_settings)

    def _apply_consistency_optimizations(self, detector) -> Dict[str, Any]:
        """Self-Consistency ìµœì í™” ì„¤ì • ì ìš©"""
        original_settings = {}
        
        # ğŸš€ ì¿¼ë¦¬ ìˆ˜ ê°ì†Œ (3ê°œ â†’ 2ê°œ)
        if hasattr(detector, 'set_query_count'):
            original_settings['query_count'] = getattr(detector, 'query_count', 3)
            detector.set_query_count(2)
        
        # ğŸš€ ê´€ëŒ€í•œ ì¼ì¹˜ ê¸°ì¤€ (0.8 â†’ 0.7)
        if hasattr(detector, 'set_similarity_threshold'):
            original_settings['similarity_threshold'] = getattr(detector, 'similarity_threshold', 0.8)
            detector.set_similarity_threshold(0.7)
        
        return original_settings

    def _restore_consistency_settings(self, detector, original_settings: Dict[str, Any]):
        """ì›ë˜ Self-Consistency ì„¤ì • ë³µì›"""
        for setting, value in original_settings.items():
            if hasattr(detector, f'set_{setting}'):
                getattr(detector, f'set_{setting}')(value)

    def _create_optimized_consistency_score(self):
        """ìµœì í™”ëœ ì¼ê´€ì„± ì ìˆ˜ ìƒì„± (ì§§ì€ í…ìŠ¤íŠ¸ìš©)"""
        class OptimizedConsistencyScore:
            def __init__(self):
                self.confidence = 0.85
                self.consistency_details = "ì§§ì€ í…ìŠ¤íŠ¸ ìµœì í™” ì²˜ë¦¬"
                self.query_responses = ["ìµœì í™”ëœ ì‘ë‹µ"]
                self.similarity_scores = [0.9]
        
        return OptimizedConsistencyScore()

    def _finalize_issue_validation(
        self, 
        issue: IssueItem, 
        all_scores: Dict[str, Any]
    ) -> Optional[IssueItem]:
        """ì´ìŠˆ ê²€ì¦ ìµœì¢… ì²˜ë¦¬"""
        
        if not all_scores:
            logger.warning(f"ì´ìŠˆ '{issue.title}': ëª¨ë“  íƒì§€ê¸° ì‹¤íŒ¨")
            return None
        
        # ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
        max_confidence = max(score.confidence for score in all_scores.values())
        weights = self.threshold_manager.get_weights_for_confidence(max_confidence)
        
        # ì ìˆ˜ í†µí•©
        combined_score = CombinedHallucinationScore(
            individual_scores=all_scores,
            weights=weights,
            final_confidence=0
        )
        
        # ê²°ê³¼ë¥¼ ì´ìŠˆì— ì¶”ê°€
        setattr(issue, 'hallucination_analysis', combined_score)
        setattr(issue, 'hallucination_confidence', combined_score.final_confidence)
        
        # ìµœì†Œ ì„ê³„ê°’ ê²€ì‚¬
        min_threshold = self.threshold_manager.thresholds.min_confidence_threshold
        if combined_score.final_confidence < min_threshold:
            logger.debug(
                f"ì´ìŠˆ '{issue.title}' ì‹ ë¢°ë„ ë¶€ì¡±: "
                f"{combined_score.final_confidence:.2f} < {min_threshold:.2f}"
            )
            return None
        
        # ì„±ê³µ ë¡œê¹…
        detection_summary = ", ".join([
            f"{k}: {v.confidence:.2f}"
            for k, v in all_scores.items()
        ])
        
        logger.debug(
            f"ì´ìŠˆ '{issue.title[:30]}' ì™„ì „ ìµœì í™” ê²€ì¦ ì™„ë£Œ - "
            f"íƒì§€ê¸°: [{detection_summary}], ìµœì¢…: {combined_score.final_confidence:.2f}"
        )
        
        return issue

    def _update_final_metrics(self, search_result: SearchResult):
        """ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸"""
        # ìºì‹œ í†µê³„ í¬í•¨
        cache_stats = self.cache.get_stats()
        self.metrics.cache_hits = cache_stats['hits']
        self.metrics.cache_misses = cache_stats['misses']
        
        # ê²€ìƒ‰ ê²°ê³¼ì— ì„±ëŠ¥ ì§€í‘œ ì¶”ê°€
        search_result.performance_metrics = {
            'total_processing_time': self.metrics.total_processing_time,
            'avg_issue_time': self.metrics.avg_issue_time,
            'cache_hit_ratio': self.metrics.calculate_cache_hit_ratio(),
            'self_consistency_skip_ratio': self.metrics.calculate_self_consistency_skip_ratio(),
            'avg_timeout_used': sum(self.metrics.timeout_utilizations) / len(self.metrics.timeout_utilizations) if self.metrics.timeout_utilizations else 0
        }

    def _log_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        cache_stats = self.cache.get_stats()
        
        logger.info(
            f"ğŸ¯ ì™„ì „ ìµœì í™” ì„±ëŠ¥ ë¦¬í¬íŠ¸:\n"
            f"  â€¢ ì´ ì²˜ë¦¬ ì´ìŠˆ: {self.metrics.total_issues_processed}ê°œ\n"
            f"  â€¢ í‰ê·  ì´ìŠˆ ì²˜ë¦¬ ì‹œê°„: {self.metrics.avg_issue_time:.3f}ì´ˆ\n"
            f"  â€¢ ìºì‹œ íˆíŠ¸ìœ¨: {cache_stats['hit_ratio']:.1%} ({cache_stats['hits']}/{cache_stats['hits'] + cache_stats['misses']})\n"
            f"  â€¢ Self-Consistency ìŠ¤í‚µìœ¨: {self.metrics.calculate_self_consistency_skip_ratio():.1%}\n"
            f"  â€¢ í‰ê·  íƒ€ì„ì•„ì›ƒ: {sum(self.metrics.timeout_utilizations) / len(self.metrics.timeout_utilizations) if self.metrics.timeout_utilizations else 0:.1f}ì´ˆ\n"
            f"  â€¢ ë™ì‹œ ì‹¤í–‰ ìˆ˜: {self.max_concurrent_issues}ê°œ"
        )

    # ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ ìœ ì§€
    def get_detector_status(self) -> Dict[str, bool]:
        """ê° íƒì§€ê¸°ì˜ í™œì„±í™” ìƒíƒœ ë°˜í™˜"""
        return {
            name: detector.is_initialized if hasattr(detector, 'is_initialized') else True
            for name, detector in self.detectors.items()
        }
    
    def update_threshold_config(self, new_config: ThresholdManager):
        """ì„ê³„ê°’ ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.threshold_manager = new_config
        logger.info("ì„ê³„ê°’ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache = SmartCache(max_size=1000, ttl_seconds=3600)
        logger.info("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ ë°˜í™˜"""
        return {
            'processing_metrics': {
                'total_issues': self.metrics.total_issues_processed,
                'avg_time_per_issue': self.metrics.avg_issue_time,
                'total_time': self.metrics.total_processing_time
            },
            'cache_metrics': self.cache.get_stats(),
            'optimization_metrics': {
                'self_consistency_skip_ratio': self.metrics.calculate_self_consistency_skip_ratio(),
                'avg_timeout': sum(self.metrics.timeout_utilizations) / len(self.metrics.timeout_utilizations) if self.metrics.timeout_utilizations else 0,
                'max_concurrent': self.max_concurrent_issues
            }
        }