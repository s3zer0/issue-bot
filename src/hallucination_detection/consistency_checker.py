"""
ìê¸° ì¼ê´€ì„± ê²€ì‚¬ê¸° (Self-Consistency Checker).

ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ë²ˆì˜ LLM ì‘ë‹µì„ ìƒì„±í•˜ê³ ,
ì‘ë‹µ ê°„ì˜ ì¼ê´€ì„±ì„ ë¶„ì„í•˜ì—¬ í™˜ê° í˜„ìƒì„ íƒì§€í•©ë‹ˆë‹¤.
"""

import asyncio
import re
from typing import List, Dict, Set, Tuple, Optional
from collections import Counter
from difflib import SequenceMatcher
from loguru import logger
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from src.config import config
from src.clients.perplexity_client import PerplexityClient
from .base import BaseHallucinationDetector
from .models import ConsistencyScore


class SelfConsistencyChecker(BaseHallucinationDetector):
    """ìê¸° ì¼ê´€ì„± ê²€ì‚¬ë¥¼ í†µí•œ í™˜ê° íƒì§€ê¸°."""

    def __init__(self, num_queries: int = 3, similarity_threshold: float = 0.7):
        """
        ìê¸° ì¼ê´€ì„± ê²€ì‚¬ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            num_queries (int): ì¼ê´€ì„± ê²€ì‚¬ë¥¼ ìœ„í•œ ì¿¼ë¦¬ íšŸìˆ˜
            similarity_threshold (float): ì¼ê´€ì„± íŒë‹¨ ì„ê³„ê°’
        """
        super().__init__("Self-Consistency")

        self.num_queries = num_queries
        self.similarity_threshold = similarity_threshold

        # Perplexity í´ë¼ì´ì–¸íŠ¸
        self.perplexity_client = PerplexityClient()

        # ğŸš€ ì „ì—­ ìºì‹œë¥¼ í†µí•œ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ìµœì í™”
        from src.hallucination_detection.enhanced_searcher import GlobalModelCache
        model_cache = GlobalModelCache()
        self.sentence_model = model_cache.get_model('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

        self.is_initialized = True
        logger.info(f"ìê¸° ì¼ê´€ì„± ê²€ì‚¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ì¿¼ë¦¬ ìˆ˜: {self.num_queries})")

    async def analyze_text(self, text: str, context: Optional[str] = None) -> ConsistencyScore:
        """
        í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìê¸° ì¼ê´€ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.

        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸ (ì›ë³¸ ì‘ë‹µ)
            context (Optional[str]): ì›ë˜ ì§ˆë¬¸ì´ë‚˜ ì£¼ì œ

        Returns:
            ConsistencyScore: ì¼ê´€ì„± ë¶„ì„ ê²°ê³¼
        """
        if not context:
            # í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì œ ì¶”ì¶œ ì‹œë„
            context = self._extract_topic_from_text(text)

        logger.info(f"ìê¸° ì¼ê´€ì„± ê²€ì‚¬ ì‹œì‘ (ì£¼ì œ: {context[:50]}...)")

        # ì—¬ëŸ¬ ë²ˆì˜ ì‘ë‹µ ìƒì„±
        responses = await self._generate_multiple_responses(context)

        # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ì‘ë‹µ ëª©ë¡ì— ì¶”ê°€
        all_responses = [text] + responses

        # ì¼ê´€ì„± ë¶„ì„
        consistency_rate, variations = self._analyze_consistency(all_responses)

        # ê³µí†µ ìš”ì†Œ ë° ì°¨ì´ì  ì¶”ì¶œ
        common_elements = self._extract_common_elements(all_responses)
        divergent_elements = self._extract_divergent_elements(all_responses)

        # ì¼ê´€ëœ ì‘ë‹µ ìˆ˜ ê³„ì‚°
        num_consistent = self._count_consistent_responses(all_responses)

        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence_from_consistency(
            consistency_rate,
            num_consistent,
            len(all_responses)
        )

        logger.info(
            f"ìê¸° ì¼ê´€ì„± ê²€ì‚¬ ì™„ë£Œ - ì¼ì¹˜ìœ¨: {consistency_rate:.2f}, "
            f"ì‹ ë¢°ë„: {confidence:.2f}"
        )

        return ConsistencyScore(
            confidence=confidence,  # ë¶€ëª¨ í´ë˜ìŠ¤ í•„ë“œë¥¼ ë¨¼ì €
            consistency_rate=consistency_rate,
            num_queries=len(all_responses),
            num_consistent=num_consistent,
            variations=variations[:5],  # ìƒìœ„ 5ê°œ ë³€í˜•ë§Œ ì €ì¥
            common_elements=common_elements[:10],  # ìƒìœ„ 10ê°œ ê³µí†µ ìš”ì†Œ
            divergent_elements=divergent_elements[:10],  # ìƒìœ„ 10ê°œ ì°¨ì´ì 
            analysis_details={
                "original_text_included": True,
                "similarity_threshold": self.similarity_threshold,
                "average_similarity": self._calculate_average_similarity(all_responses)
            }
        )

    async def _generate_multiple_responses(self, query: str) -> List[str]:
        """
        ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query (str): ì§ˆë¬¸ ë˜ëŠ” ì£¼ì œ

        Returns:
            List[str]: ìƒì„±ëœ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
        """
        responses = []

        # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì‘ë‹µ ìƒì„±
        tasks = []
        for i in range(self.num_queries - 1):  # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ ìˆ˜
            # ì•½ê°„ì”© ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¡œ ë‹¤ì–‘ì„± í™•ë³´
            prompt_variation = self._create_prompt_variation(query, i)
            task = self._get_single_response(prompt_variation)
            tasks.append(task)

        # ëª¨ë“  ì‘ë‹µ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ({i+1}): {result}")
            else:
                responses.append(result)

        return responses

    async def _get_single_response(self, prompt: str) -> str:
        """
        ë‹¨ì¼ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            prompt (str): í”„ë¡¬í”„íŠ¸

        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            response = await self.perplexity_client._make_api_call(prompt)
            content = response['choices'][0]['message']['content']
            return content
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            raise

    def _create_prompt_variation(self, query: str, variation_idx: int) -> str:
        """
        ì•½ê°„ì˜ ë³€í˜•ì„ ê°€ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query (str): ì›ë³¸ ì§ˆì˜
            variation_idx (int): ë³€í˜• ì¸ë±ìŠ¤

        Returns:
            str: ë³€í˜•ëœ í”„ë¡¬í”„íŠ¸
        """
        variations = [
            f"{query}ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            f"{query}ì™€ ê´€ë ¨ëœ ìµœì‹  ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
            f"{query}ì— ëŒ€í•œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            f"{query}ì˜ ì¤‘ìš”í•œ ì¸¡ë©´ë“¤ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            f"{query}ì— ëŒ€í•´ ì•Œì•„ì•¼ í•  ì£¼ìš” ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]

        return variations[variation_idx % len(variations)]

    def _analyze_consistency(self, responses: List[str]) -> Tuple[float, List[str]]:
        """
        ì‘ë‹µë“¤ ê°„ì˜ ì¼ê´€ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            responses (List[str]): ë¶„ì„í•  ì‘ë‹µ ë¦¬ìŠ¤íŠ¸

        Returns:
            Tuple[float, List[str]]: ì¼ê´€ì„± ë¹„ìœ¨ê³¼ ì£¼ìš” ë³€í˜•ë“¤
        """
        if len(responses) < 2:
            return 1.0, []

        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì¼ê´€ì„± ê³„ì‚°
        embeddings = self.sentence_model.encode(responses)
        similarity_matrix = cosine_similarity(embeddings)

        # ì‘ë‹µë³„ í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ
        key_elements_per_response = [self._extract_key_elements(resp) for resp in responses]

        # ìš”ì†Œ ê°„ ì¼ì¹˜ë„ì™€ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ëª¨ë‘ ê³ ë ¤
        total_comparisons = 0
        consistent_comparisons = 0
        variations = []

        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                # ì˜ë¯¸ì  ìœ ì‚¬ë„
                semantic_sim = similarity_matrix[i][j]

                # ìš”ì†Œ ê¸°ë°˜ ìœ ì‚¬ë„
                element_sim = self._calculate_element_similarity(
                    key_elements_per_response[i],
                    key_elements_per_response[j]
                )

                # í†µí•© ìœ ì‚¬ë„ (ì˜ë¯¸ì  ìœ ì‚¬ë„ì™€ ìš”ì†Œ ìœ ì‚¬ë„ì˜ í‰ê· )
                combined_similarity = (semantic_sim + element_sim) / 2

                total_comparisons += 1
                if combined_similarity >= self.similarity_threshold:
                    consistent_comparisons += 1
                else:
                    # ì°¨ì´ì  ê¸°ë¡
                    diff = self._find_differences(responses[i], responses[j])
                    if diff:
                        variations.append(diff)

        consistency_rate = consistent_comparisons / total_comparisons if total_comparisons > 0 else 0.0

        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆœ ì •ë ¬
        variation_counter = Counter(variations)
        unique_variations = [var for var, _ in variation_counter.most_common()]

        return consistency_rate, unique_variations

    def _extract_key_elements(self, text: str) -> Set[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ìš”ì†Œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸

        Returns:
            Set[str]: í•µì‹¬ ìš”ì†Œ ì§‘í•©
        """
        # ìˆ«ì, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬, ì£¼ìš” ëª…ì‚¬êµ¬ ì¶”ì¶œ
        elements = set()

        # ìˆ«ì ì¶”ì¶œ (í¼ì„¼íŠ¸ í¬í•¨)
        numbers = re.findall(r'\b\d+(?:\.\d+)?%?\b', text)
        elements.update(numbers)

        # í•œê¸€ ë‚ ì§œ íŒ¨í„´ (2024ë…„, 2024ë…„ 3ì›”, 2024ë…„ 3ì›” 15ì¼)
        korean_dates = re.findall(r'\b\d{4}ë…„(?:\s*\d{1,2}ì›”)?(?:\s*\d{1,2}ì¼)?\b', text)
        elements.update(korean_dates)

        # ì˜ì–´ ë‚ ì§œ íŒ¨í„´
        english_dates = re.findall(r'\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b', text)
        elements.update(english_dates)

        # ë‹¨ìˆœ ì—°ë„
        years = re.findall(r'\b20\d{2}ë…„?\b', text)
        elements.update(years)

        # ì¸ìš©êµ¬
        quotes = re.findall(r'"([^"]+)"', text)
        elements.update(quotes)

        # ì˜ì–´ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤ (ê³ ìœ ëª…ì‚¬)
        english_proper = re.findall(r'\b[A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*\b', text)
        elements.update(english_proper)

        # ì˜ì–´ ì•½ì–´ (ëª¨ë‘ ëŒ€ë¬¸ì)
        acronyms = re.findall(r'\b[A-Z]{2,}(?:-\d+)?\b', text)  # GPT-5 ê°™ì€ íŒ¨í„´ë„ í¬í•¨
        elements.update(acronyms)

        return elements

    def _calculate_element_similarity(self, elements1: Set[str], elements2: Set[str]) -> float:
        """
        ë‘ ìš”ì†Œ ì§‘í•© ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            elements1 (Set[str]): ì²« ë²ˆì§¸ ìš”ì†Œ ì§‘í•©
            elements2 (Set[str]): ë‘ ë²ˆì§¸ ìš”ì†Œ ì§‘í•©

        Returns:
            float: ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        if not elements1 and not elements2:
            return 1.0
        if not elements1 or not elements2:
            return 0.0

        intersection = elements1.intersection(elements2)
        union = elements1.union(elements2)

        return len(intersection) / len(union)

    def _find_differences(self, text1: str, text2: str) -> Optional[str]:
        """
        ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì£¼ìš” ì°¨ì´ì ì„ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            text1 (str): ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2 (str): ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸

        Returns:
            Optional[str]: ì£¼ìš” ì°¨ì´ì  ì„¤ëª…
        """
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences1 = [s.strip() for s in text1.split('.') if s.strip()]
        sentences2 = [s.strip() for s in text2.split('.') if s.strip()]

        # ìœ ì‚¬ë„ê°€ ë‚®ì€ ë¬¸ì¥ ìŒ ì°¾ê¸°
        matcher = SequenceMatcher(None, sentences1, sentences2)
        differences = []

        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'replace':
                if i1 < len(sentences1) and j1 < len(sentences2):
                    diff = f"ë¶ˆì¼ì¹˜: '{sentences1[i1][:50]}...' vs '{sentences2[j1][:50]}...'"
                    differences.append(diff)
            elif tag == 'delete':
                if i1 < len(sentences1):
                    diff = f"ì²« ë²ˆì§¸ì—ë§Œ ì¡´ì¬: '{sentences1[i1][:50]}...'"
                    differences.append(diff)
            elif tag == 'insert':
                if j1 < len(sentences2):
                    diff = f"ë‘ ë²ˆì§¸ì—ë§Œ ì¡´ì¬: '{sentences2[j1][:50]}...'"
                    differences.append(diff)

        return differences[0] if differences else None

    def _extract_common_elements(self, responses: List[str]) -> List[str]:
        """
        ëª¨ë“  ì‘ë‹µì—ì„œ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ìš”ì†Œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            responses (List[str]): ì‘ë‹µ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[str]: ê³µí†µ ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
        """
        if not responses:
            return []

        # ê° ì‘ë‹µì—ì„œ ìš”ì†Œ ì¶”ì¶œ
        all_elements = [self._extract_key_elements(resp) for resp in responses]

        # ëª¨ë“  ì‘ë‹µì— ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ìš”ì†Œ
        common = set.intersection(*all_elements) if all_elements else set()

        return list(common)

    def _extract_divergent_elements(self, responses: List[str]) -> List[str]:
        """
        ì‘ë‹µ ê°„ ì°¨ì´ë¥¼ ë³´ì´ëŠ” ìš”ì†Œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            responses (List[str]): ì‘ë‹µ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[str]: ì°¨ì´ì  ë¦¬ìŠ¤íŠ¸
        """
        if len(responses) < 2:
            return []

        divergent = []
        all_elements = [self._extract_key_elements(resp) for resp in responses]

        # ì¼ë¶€ ì‘ë‹µì—ë§Œ ë‚˜íƒ€ë‚˜ëŠ” ìš”ì†Œ ì°¾ê¸°
        element_counts = Counter()
        for elements in all_elements:
            element_counts.update(elements)

        # ì „ì²´ ì‘ë‹µì˜ ì ˆë°˜ ë¯¸ë§Œì—ì„œë§Œ ë‚˜íƒ€ë‚˜ëŠ” ìš”ì†Œ
        threshold = len(responses) / 2
        for element, count in element_counts.items():
            if count < threshold:
                divergent.append(f"{element} (ë“±ì¥: {count}/{len(responses)})")

        return divergent[:10]  # ìƒìœ„ 10ê°œë§Œ

    def _count_consistent_responses(self, responses: List[str]) -> int:
        """
        ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µì˜ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            responses (List[str]): ì‘ë‹µ ë¦¬ìŠ¤íŠ¸

        Returns:
            int: ì¼ê´€ëœ ì‘ë‹µ ìˆ˜
        """
        if len(responses) < 2:
            return len(responses)

        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        embeddings = self.sentence_model.encode(responses)
        similarity_matrix = cosine_similarity(embeddings)

        # ê° ì‘ë‹µì´ ì–¼ë§ˆë‚˜ ë§ì€ ë‹¤ë¥¸ ì‘ë‹µê³¼ ìœ ì‚¬í•œì§€ ê³„ì‚°
        consistent_count = 0
        for i, row in enumerate(similarity_matrix):
            similar_responses = sum(1 for j, sim in enumerate(row) if i != j and sim >= self.similarity_threshold)
            # ì ˆë°˜ ì´ìƒì˜ ë‹¤ë¥¸ ì‘ë‹µê³¼ ìœ ì‚¬í•˜ë©´ ì¼ê´€ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
            if similar_responses >= (len(responses) - 1) / 2:
                consistent_count += 1

        return consistent_count

    def _calculate_average_similarity(self, responses: List[str]) -> float:
        """
        ì‘ë‹µë“¤ ê°„ì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            responses (List[str]): ì‘ë‹µ ë¦¬ìŠ¤íŠ¸

        Returns:
            float: í‰ê·  ìœ ì‚¬ë„
        """
        if len(responses) < 2:
            return 1.0

        embeddings = self.sentence_model.encode(responses)
        similarity_matrix = cosine_similarity(embeddings)

        # ëŒ€ê°ì„  ì œì™¸ í‰ê· 
        np.fill_diagonal(similarity_matrix, 0)
        n = len(responses)
        avg_similarity = np.sum(similarity_matrix) / (n * (n - 1))

        return float(avg_similarity)

    def _calculate_confidence_from_consistency(
        self,
        consistency_rate: float,
        num_consistent: int,
        total_responses: int
    ) -> float:
        """
        ì¼ê´€ì„± ì§€í‘œë“¤ë¡œë¶€í„° ìµœì¢… ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            consistency_rate (float): ì¼ê´€ì„± ë¹„ìœ¨
            num_consistent (int): ì¼ê´€ëœ ì‘ë‹µ ìˆ˜
            total_responses (int): ì „ì²´ ì‘ë‹µ ìˆ˜

        Returns:
            float: ìµœì¢… ì‹ ë¢°ë„ (0.0 ~ 1.0)
        """
        # ì¼ê´€ì„± ë¹„ìœ¨ (50%)
        consistency_score = consistency_rate

        # ì¼ê´€ëœ ì‘ë‹µ ë¹„ìœ¨ (50%)
        consistent_ratio = num_consistent / total_responses if total_responses > 0 else 0.0

        # ê°€ì¤‘ í‰ê· 
        confidence = 0.5 * consistency_score + 0.5 * consistent_ratio

        return round(confidence, 3)

    def _extract_topic_from_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            text (str): í…ìŠ¤íŠ¸

        Returns:
            str: ì¶”ì¶œëœ ì£¼ì œ
        """
        # ì œëª©ì´ë‚˜ ì²« ë¬¸ì¥ì—ì„œ ì£¼ì œ ì¶”ì¶œ ì‹œë„
        lines = text.split('\n')
        for line in lines:
            if line.strip() and len(line.strip()) > 10:
                # ì œëª© í˜•ì‹ í™•ì¸
                if line.startswith('ì œëª©:') or line.startswith('#'):
                    return line.replace('ì œëª©:', '').replace('#', '').strip()

        # ì²« ë¬¸ì¥ ë°˜í™˜
        first_sentence = text.split('.')[0] if '.' in text else text[:100]
        return first_sentence.strip()