"""
RePPL ê¸°ë°˜ í™˜ê° íƒì§€ ë° ì‘ë‹µ ì‹ ë¢°ë„ ê²€ì¦ ëª¨ë“ˆ.

ì´ ëª¨ë“ˆì€ "Repetition as Pre-Perplexity"(RePPL) ê°œë…ì„ ê¸°ë°˜ìœ¼ë¡œ,
ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì˜ í™˜ê° í˜„ìƒì„ íƒì§€í•˜ê³  ì‹ ë¢°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1.  **ë°˜ë³µì„± ë¶„ì„ (Repetition Analysis):** í…ìŠ¤íŠ¸ ë‚´ êµ¬ë¬¸ ë°˜ë³µì„ ì¸¡ì •í•©ë‹ˆë‹¤.
2.  **Perplexity ê³„ì‚°:** GPT-4o APIì˜ logprobsë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ í†µê³„ì  ìì—°ìŠ¤ëŸ¬ì›€ì„ í‰ê°€í•©ë‹ˆë‹¤.
3.  **ì˜ë¯¸ì  ì—”íŠ¸ë¡œí”¼ (Semantic Entropy):** ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ë‹¤ì–‘ì„±ì„ ì¸¡ì •í•˜ì—¬ ë‚´ìš©ì˜ í’ë¶€í•¨ì„ í‰ê°€í•©ë‹ˆë‹¤.
4.  **ì‹ ë¢°ë„ ì ìˆ˜ ì¢…í•©:** ìœ„ì˜ ì„¸ ê°€ì§€ ì§€í‘œë¥¼ ê°€ì¤‘ í•©ì‚°í•˜ì—¬ ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
5.  **ì´ìŠˆ ê²€ìƒ‰ê¸° ì—°ë™:** ìƒì„±ëœ ì´ìŠˆ/ì½˜í…ì¸ ë¥¼ RePPLë¡œ ê²€ì¦í•˜ì—¬ ì‹ ë¢°ë„ ë‚®ì€ ê²°ê³¼ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
"""

import re
import numpy as np
import math
import asyncio
from typing import List, Dict, Tuple, Optional
from collections import Counter
from dataclasses import dataclass
from loguru import logger
from sentence_transformers import SentenceTransformer
from openai import AsyncOpenAI
from sklearn.metrics.pairwise import cosine_similarity

# --- ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸ ---
# í”„ë¡œì íŠ¸ì˜ ì„¤ì •, ë°ì´í„° ëª¨ë¸, ì´ìŠˆ ê²€ìƒ‰ê¸°, í‚¤ì›Œë“œ ìƒì„±ê¸°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from src.config import config
from src.models import KeywordResult, IssueItem, SearchResult
from src.issue_searcher import create_issue_searcher
from src.keyword_generator import generate_keywords_for_topic


@dataclass
class RePPLScore:
    """RePPL ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤.

    í™˜ê° íƒì§€ ëª¨ë“ˆì´ ê³„ì‚°í•œ ë‹¤ì–‘í•œ ì ìˆ˜ì™€ ë¶„ì„ ì„¸ë¶€ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.

    Attributes:
        repetition_score (float): í…ìŠ¤íŠ¸ì˜ êµ¬ë¬¸ ë°˜ë³µ ì ìˆ˜ (0~1). ë†’ì„ìˆ˜ë¡ ë°˜ë³µì´ ë§ìŒ.
        perplexity (float): GPT-4o ê¸°ë°˜ Perplexity ì ìˆ˜. ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸.
        semantic_entropy (float): ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ë‹¤ì–‘ì„± ì ìˆ˜ (0~1). ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ë‹¤ë£¸.
        confidence (float): ì„¸ ê°€ì§€ ì§€í‘œë¥¼ ì¢…í•©í•œ ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ (0~1). ë†’ì„ìˆ˜ë¡ ì‹ ë¢° ê°€ëŠ¥.
        repeated_phrases (List[str]): í…ìŠ¤íŠ¸ì—ì„œ 2ë²ˆ ì´ìƒ ë°˜ë³µëœ êµ¬ë¬¸ ë¦¬ìŠ¤íŠ¸.
        analysis_details (Dict[str, any]): í† í° ìˆ˜, ë¬¸ì¥ ìˆ˜ ë“± ë¶„ì„ì— ì‚¬ìš©ëœ ì¶”ê°€ ë©”íƒ€ë°ì´í„°.
    """
    repetition_score: float
    perplexity: float
    semantic_entropy: float
    confidence: float
    repeated_phrases: List[str]
    analysis_details: Dict[str, any]


class RePPLHallucinationDetector:
    """RePPL ê¸°ë°˜ í™˜ê° íƒì§€ê¸° (GPT-4o API ì‚¬ìš©).

    í…ìŠ¤íŠ¸ì˜ ë°˜ë³µì„±, Perplexity, ì˜ë¯¸ì  ì—”íŠ¸ë¡œí”¼ë¥¼ ë¶„ì„í•˜ì—¬
    LLM ì‘ë‹µì˜ ì‹ ë¢°ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """

    def __init__(self, model_name: Optional[str] = None):
        """
        RePPLHallucinationDetector ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_name (Optional[str]): Perplexity ê³„ì‚°ì— ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì´ë¦„.
                                        ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ 'gpt-4o'ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        """
        # ì„¤ì •ì—ì„œ OpenAI API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        self.api_key = config.get_openai_api_key()
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ì™€ Perplexity ê³„ì‚° ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        self.client = AsyncOpenAI(api_key=self.api_key)
        self.perplexity_model = model_name or "gpt-4o"

        # ë¬¸ì¥ ì„ë² ë”©ì„ ìœ„í•œ Sentence Transformer ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

        # ì‹ ë¢°ë„ ê³„ì‚°ì— ì‚¬ìš©ë  ì„ê³„ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        self.repetition_threshold = 0.3  # ë°˜ë³µ ì ìˆ˜ê°€ ì´ ê°’ì„ ë„˜ìœ¼ë©´ ì‹ ë¢°ë„ í•˜ë½
        self.perplexity_threshold = 50   # Perplexityê°€ ì´ ê°’ì„ ë„˜ìœ¼ë©´ ì‹ ë¢°ë„ í•˜ë½

        logger.info(f"RePPL í™˜ê° íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ (Perplexity ëª¨ë¸: {self.perplexity_model})")

    async def analyze_response(self, text: str, context: Optional[str] = None) -> RePPLScore:
        """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ RePPL ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸ (LLM ì‘ë‹µ).
            context (Optional[str]): í…ìŠ¤íŠ¸ì˜ ì£¼ì œë‚˜ ë§¥ë½. Perplexity ê³„ì‚°ì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

        Returns:
            RePPLScore: ë°˜ë³µì„±, Perplexity, ì˜ë¯¸ì  ì—”íŠ¸ë¡œí”¼, ìµœì¢… ì‹ ë¢°ë„ ë“±ì´ í¬í•¨ëœ ë¶„ì„ ê²°ê³¼ ê°ì²´.
        """
        logger.debug(f"RePPL ë¶„ì„ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)})")

        # ê° ì§€í‘œë¥¼ ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        repetition_score, repeated_phrases = self._analyze_repetition(text)
        perplexity = await self._calculate_perplexity_with_gpt(text, context)
        semantic_entropy = self._calculate_semantic_entropy(text)

        # ê³„ì‚°ëœ ì§€í‘œë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
        confidence = self._calculate_confidence_score(
            repetition_score, perplexity, semantic_entropy
        )

        logger.debug(f"RePPL ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: Repetition={repetition_score:.2f}, PPL={perplexity:.2f}, Entropy={semantic_entropy:.2f} -> Confidence={confidence:.2f}")

        # ë¶„ì„ì— ì‚¬ìš©ëœ ì¶”ê°€ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
        analysis_details = {
            "token_count": len(text.split()),
            "sentence_count": len(text.split('.')),
            "has_context": context is not None
        }

        # ìµœì¢… ê²°ê³¼ë¥¼ RePPLScore ê°ì²´ë¡œ ë§Œë“¤ì–´ ë°˜í™˜í•©ë‹ˆë‹¤.
        return RePPLScore(
            repetition_score=repetition_score,
            perplexity=perplexity,
            semantic_entropy=semantic_entropy,
            confidence=confidence,
            repeated_phrases=repeated_phrases,
            analysis_details=analysis_details
        )

    def _analyze_repetition(self, text: str) -> Tuple[float, List[str]]:
        """í…ìŠ¤íŠ¸ ë‚´ n-gram(ì—°ì†ëœ ë‹¨ì–´ ë¬¶ìŒ)ì˜ ë°˜ë³µ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸.

        Returns:
            Tuple[float, List[str]]:
                - ë°˜ë³µì„± ì ìˆ˜ (0.0 ~ 1.0).
                - í…ìŠ¤íŠ¸ ë‚´ì—ì„œ 2ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚œ êµ¬ë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        phrases, repeated_phrases_set = [], set()

        # ğŸ’¡ [ìˆ˜ì •] êµ¬ë‘ì ì„ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë‹¨ì–´ ì¼ì¹˜ìœ¨ì„ ë†’ì…ë‹ˆë‹¤.
        cleaned_text = re.sub(r'[^\w\s]', '', text.lower())
        words = cleaned_text.split()

        if not words: return 0.0, []

        # 3-gramë¶€í„° 7-gramê¹Œì§€ ë°˜ë³µë˜ëŠ” êµ¬ë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤.
        for n in range(3, 8):
            if len(words) < n: break
            ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]
            phrase_counts = Counter(ngrams)
            for phrase, count in phrase_counts.items():
                if count > 1:
                    repeated_phrases_set.add(' '.join(phrase))
                    phrases.append((' '.join(phrase), count))

        # ë°˜ë³µëœ ë‹¨ì–´ì˜ ì´ ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì ìˆ˜í™”í•©ë‹ˆë‹¤.
        repeated_words = sum(len(p.split()) * (c - 1) for p, c in phrases)
        repetition_score = min(1.0, repeated_words / len(words)) if len(words) > 0 else 0.0

        logger.debug(f"ë°˜ë³µì„± ì ìˆ˜: {repetition_score:.3f}, ë°˜ë³µëœ êµ¬ë¬¸: {len(repeated_phrases_set)}ê°œ")
        return repetition_score, list(repeated_phrases_set)

    async def _calculate_perplexity_with_gpt(self, text: str, context: Optional[str] = None) -> float:
        """OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ Perplexityë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        PerplexityëŠ” ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì–¼ë§ˆë‚˜ 'ë†€ë¼ì›Œí•˜ëŠ”ì§€'ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ½ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

        Args:
            text (str): Perplexityë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸.
            context (Optional[str]): í…ìŠ¤íŠ¸ì˜ ë§¥ë½ ì •ë³´.

        Returns:
            float: ê³„ì‚°ëœ Perplexity ê°’. ê³„ì‚° ì‹¤íŒ¨ ì‹œ ë†’ì€ ì„ê³„ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not text.strip(): return self.perplexity_threshold * 2  # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ìµœëŒ€ íŒ¨ë„í‹°
        logger.debug(f"Perplexity ê³„ì‚° ìš”ì²­ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)})")
        try:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
            full_text = f"Context: {context}\n\nText to evaluate: {text}" if context else text
            # OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ í† í°ë³„ ë¡œê·¸ í™•ë¥ (logprobs)ì„ ìš”ì²­í•©ë‹ˆë‹¤.
            response = await self.client.chat.completions.create(
                model=self.perplexity_model,
                messages=[{"role": "user", "content": full_text}],
                max_tokens=2048,
                temperature=0,
                logprobs=True  # ğŸ’¡ logprobs í™œì„±í™”ê°€ Perplexity ê³„ì‚°ì˜ í•µì‹¬
            )

            logprobs = [token.logprob for token in response.choices[0].logprobs.content if token.logprob is not None]
            if not logprobs: return self.perplexity_threshold * 2

            # ë¡œê·¸ í™•ë¥ ì˜ í‰ê· ì„ êµ¬í•œ í›„, Perplexity ê³µì‹ì„ ì ìš©í•©ë‹ˆë‹¤. PPL = exp(-mean_log_prob)
            mean_logprob = sum(logprobs) / len(logprobs)
            perplexity = math.exp(-mean_logprob)
            logger.debug(f"GPT Perplexity ê³„ì‚° ì„±ê³µ: {perplexity:.2f}")
            return perplexity
        except Exception as e:
            # API í˜¸ì¶œ ì‹¤íŒ¨ë‚˜ ê³„ì‚° ì˜¤ë¥˜ ë°œìƒ ì‹œ ì—ëŸ¬ë¥¼ ê¸°ë¡í•˜ê³  íŒ¨ë„í‹° ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            logger.error(f"GPT Perplexity ê³„ì‚° ì‹¤íŒ¨: {e}")
            return self.perplexity_threshold * 2

    def _calculate_semantic_entropy(self, text: str) -> float:
        """ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì  ë‹¤ì–‘ì„±(ì—”íŠ¸ë¡œí”¼)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì„ìˆ˜ë¡ í…ìŠ¤íŠ¸ê°€ ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ë‹¤ë£¨ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸.

        Returns:
            float: ì˜ë¯¸ì  ì—”íŠ¸ë¡œí”¼ ì ìˆ˜ (0.0 ~ 1.0). 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¬¸ì¥ë“¤ì´ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•¨.
        """
        # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³ , ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ì œì™¸í•©ë‹ˆë‹¤.
        sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 5]
        if len(sentences) < 2:
            return 0.0  # ë¬¸ì¥ì´ 2ê°œ ë¯¸ë§Œì´ë©´ ë‹¤ì–‘ì„± ê³„ì‚°ì´ ë¬´ì˜ë¯¸

        # ê° ë¬¸ì¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        embeddings = self.sentence_model.encode(sentences)
        # ë¬¸ì¥ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        sim_matrix = cosine_similarity(embeddings)
        num_sentences = len(sentences)

        # ëŒ€ê°ì„ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„)ì€ 0ìœ¼ë¡œ ë§Œë“¤ì–´ í‰ê·  ê³„ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        np.fill_diagonal(sim_matrix, 0)

        # ëª¨ë“  ë¬¸ì¥ ìŒì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        avg_similarity = np.sum(sim_matrix) / (num_sentences * (num_sentences - 1))

        # ì—”íŠ¸ë¡œí”¼ë¥¼ '1 - í‰ê·  ìœ ì‚¬ë„'ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
        entropy = 1 - avg_similarity
        logger.debug(f"ì˜ë¯¸ì  ì—”íŠ¸ë¡œí”¼ ê³„ì‚°ë¨: {entropy:.3f} (ë¬¸ì¥ {len(sentences)}ê°œ ê¸°ë°˜)")
        return entropy

    def _calculate_confidence_score(self, repetition: float, perplexity: float, entropy: float) -> float:
        """ì„¸ ê°€ì§€ ê°œë³„ ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            repetition (float): ë°˜ë³µì„± ì ìˆ˜.
            perplexity (float): Perplexity ì ìˆ˜.
            entropy (float): ì˜ë¯¸ì  ì—”íŠ¸ë¡œí”¼ ì ìˆ˜.

        Returns:
            float: ì¢…í•© ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0).
        """
        # ê° ì ìˆ˜ë¥¼ 0~1 ì‚¬ì´ì˜ ê¸ì •ì ì¸ ì ìˆ˜(ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
        rep_score = 1 - min(1.0, repetition / self.repetition_threshold)
        ppl_score = max(0.0, 1 - (perplexity / self.perplexity_threshold))
        ent_score = min(1.0, entropy)  # ì—”íŠ¸ë¡œí”¼ëŠ” ì´ë¯¸ 0~1 ë²”ìœ„ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©

        # ê° ì§€í‘œì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. (ë°˜ë³µì„±ì— ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬)
        weights = {'repetition': 0.4, 'perplexity': 0.3, 'entropy': 0.3}

        # ê°€ì¤‘ í•©ì‚°ì„ í†µí•´ ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        confidence = (
            weights['repetition'] * rep_score +
            weights['perplexity'] * ppl_score +
            weights['entropy'] * ent_score
        )
        return round(confidence, 3)


class RePPLEnhancedIssueSearcher:
    """RePPL í™˜ê° íƒì§€ ëª¨ë“ˆì´ í†µí•©ëœ ì´ìŠˆ ê²€ìƒ‰ê¸°.

    ê¸°ë³¸ ì´ìŠˆ ê²€ìƒ‰ê¸°ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•œ í›„, ê° ê²°ê³¼ë¬¼ì˜ ì‹ ë¢°ë„ë¥¼ RePPLë¡œ ê²€ì¦í•˜ì—¬
    í’ˆì§ˆì´ ë‚®ì€ ì½˜í…ì¸ ë¥¼ í•„í„°ë§í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    def __init__(self, api_key: Optional[str] = None):
        """RePPLEnhancedIssueSearcher ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            api_key (Optional[str]): ì´ìŠˆ ê²€ìƒ‰ì— ì‚¬ìš©í•  API í‚¤.
        """
        # ê¸°ë³¸ ì´ìŠˆ ê²€ìƒ‰ê¸°ì™€ RePPL íƒì§€ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        self.base_searcher = create_issue_searcher(api_key)
        self.reppl_detector = RePPLHallucinationDetector()

        # ê²€ì¦ì„ í†µê³¼í•˜ê¸° ìœ„í•œ ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        self.min_confidence_threshold = 0.5
        logger.info("RePPL ê°•í™” ì´ìŠˆ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    async def search_with_validation(self, keyword_result: KeywordResult, time_period: str) -> SearchResult:
        """í‚¤ì›Œë“œë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  RePPLë¡œ ê° ê²°ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

        ì‹ ë¢°ë„ ë†’ì€ ê²°ê³¼ë¥¼ ì¶©ë¶„íˆ ì–»ì§€ ëª»í•˜ë©´, í‚¤ì›Œë“œë¥¼ ì¬ìƒì„±í•˜ì—¬ ê²€ìƒ‰ì„ ì¬ì‹œë„í•©ë‹ˆë‹¤.

        Args:
            keyword_result (KeywordResult): ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ ì •ë³´ ê°ì²´.
            time_period (str): ê²€ìƒ‰í•  ê¸°ê°„ (ì˜ˆ: 'past_week').

        Returns:
            SearchResult: RePPL ê²€ì¦ì„ í†µê³¼í•œ ì´ìŠˆ ëª©ë¡ì´ í¬í•¨ëœ ê²€ìƒ‰ ê²°ê³¼ ê°ì²´.
        """
        max_retries = 2
        current_keywords = keyword_result

        for attempt in range(max_retries):
            # 1. ê¸°ë³¸ ê²€ìƒ‰ê¸°ë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            search_result = await self.base_searcher.search_issues_from_keywords(
                current_keywords, time_period, collect_details=True
            )
            validated_issues = []
            if search_result.issues:
                # 2. ê²€ìƒ‰ëœ ê° ì´ìŠˆì— ëŒ€í•´ RePPL ê²€ì¦ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                validation_tasks = [self._validate_issue(issue, current_keywords.topic) for issue in search_result.issues]
                validation_results = await asyncio.gather(*validation_tasks)
                # ê²€ì¦ì„ í†µê³¼í•œ ì´ìŠˆ(Noneì´ ì•„ë‹Œ ê²°ê³¼)ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
                validated_issues = [issue for issue in validation_results if issue is not None]

            # 3. ì¶©ë¶„í•œ ìˆ˜ì˜ ì‹ ë¢°ë„ ë†’ì€ ì´ìŠˆë¥¼ ì°¾ì•˜ê±°ë‚˜ ë§ˆì§€ë§‰ ì‹œë„ì´ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
            if len(validated_issues) >= 3 or attempt == max_retries - 1:
                search_result.issues = validated_issues
                search_result.total_found = len(validated_issues)
                return search_result

            # 4. ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´, ë‹¤ë¥¸ ê´€ì ì˜ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.
            logger.info(f"ì‹ ë¢°ë„ ë†’ì€ ì´ìŠˆ ë¶€ì¡±, í‚¤ì›Œë“œë¥¼ ì¬ìƒì„±í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤. (ì‹œë„ {attempt + 1}/{max_retries})")
            current_keywords = await generate_keywords_for_topic(f"{current_keywords.topic}ì˜ ë‹¤ë¥¸ ê´€ì ")

        return search_result

    async def _validate_issue(self, issue: IssueItem, topic: str) -> Optional[IssueItem]:
        """ê°œë³„ ì´ìŠˆ í•­ëª©ì˜ ì½˜í…ì¸ ë¥¼ RePPLë¡œ ë¶„ì„í•˜ê³  ì‹ ë¢°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            issue (IssueItem): ê²€ì¦í•  ì´ìŠˆ ê°ì²´.
            topic (str): ì´ìŠˆì˜ ìƒìœ„ ì£¼ì œ (ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©).

        Returns:
            Optional[IssueItem]: ì‹ ë¢°ë„ ì„ê³„ê°’ì„ í†µê³¼í•œ ê²½ìš° í•´ë‹¹ ì´ìŠˆ ê°ì²´ë¥¼, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        # ê²€ì¦í•  í…ìŠ¤íŠ¸ë¥¼ ì œëª©, ìš”ì•½, ìƒì„¸ ë‚´ìš© ì¼ë¶€ë¥¼ ì¡°í•©í•˜ì—¬ êµ¬ì„±í•©ë‹ˆë‹¤.
        content_to_validate = f"ì œëª©: {issue.title}\nìš”ì•½: {issue.summary}"
        if issue.detailed_content:
            content_to_validate += f"\nìƒì„¸ë‚´ìš©: {issue.detailed_content[:500]}"

        # RePPL ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        reppl_score = await self.reppl_detector.analyze_response(content_to_validate, context=topic)

        # ê³„ì‚°ëœ ì‹ ë¢°ë„ê°€ ì„¤ì •ëœ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ì—ë§Œ ì´ìŠˆë¥¼ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
        if reppl_score.confidence >= self.min_confidence_threshold:
            # ì´ìŠˆ ê°ì²´ì— RePPL ì‹ ë¢°ë„ ë° ë¶„ì„ ê²°ê³¼ë¥¼ ì†ì„±ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
            setattr(issue, 'reppl_confidence', reppl_score.confidence)
            setattr(issue, 'reppl_analysis', reppl_score)
            logger.debug(f"ì´ìŠˆ '{issue.title[:30]}...' ê²€ì¦ í†µê³¼ (ì‹ ë¢°ë„: {reppl_score.confidence:.2f})")
            return issue
        else:
            # ì„ê³„ê°’ ë¯¸ë§Œì¸ ê²½ìš° ê²½ê³ ë¥¼ ë¡œê·¸í•˜ê³  í•´ë‹¹ ì´ìŠˆë¥¼ ì œì™¸(None ë°˜í™˜)í•©ë‹ˆë‹¤.
            logger.warning(f"ì´ìŠˆ '{issue.title[:30]}...' ì œì™¸ë¨ - RePPL ì‹ ë¢°ë„: {reppl_score.confidence:.2f} < {self.min_confidence_threshold}")
            return None