"""
RePPL ê¸°ë°˜ í™˜ê° íƒì§€ ëª¨ë“ˆ (GPT-4o API ê¸°ë°˜ Perplexity ê³„ì‚°)
Repetition as Pre-Perplexityë¥¼ í™œìš©í•œ LLM ì‘ë‹µ ì‹ ë¢°ë„ ê²€ì¦
"""
import re
import numpy as np
import math
import asyncio
from typing import List, Dict, Tuple, Optional
from collections import Counter
from dataclasses import dataclass
from loguru import logger
from sentence_transformers import SentenceTransformer
from openai import AsyncOpenAI
# ğŸ’¡ [ì¶”ê°€] scikit-learnì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í•¨ìˆ˜ import
from sklearn.metrics.pairwise import cosine_similarity

from src.config import config
from src.models import KeywordResult, IssueItem, SearchResult
from src.issue_searcher import create_issue_searcher


@dataclass
class RePPLScore:
    """RePPL ë¶„ì„ ê²°ê³¼"""
    repetition_score: float
    perplexity: float
    semantic_entropy: float
    confidence: float
    repeated_phrases: List[str]
    analysis_details: Dict[str, any]


class RePPLHallucinationDetector:
    """RePPL ê¸°ë°˜ í™˜ê° íƒì§€ê¸° (GPT-4o API ì‚¬ìš©)"""

    def __init__(self, model_name: Optional[str] = None):
        self.api_key = config.get_openai_api_key()
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = AsyncOpenAI(api_key=self.api_key)
        self.perplexity_model = model_name or "gpt-4o"

        self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

        self.repetition_threshold = 0.3
        self.perplexity_threshold = 50

        logger.info(f"RePPL í™˜ê° íƒì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ (Perplexity ëª¨ë¸: {self.perplexity_model})")

    async def analyze_response(self, text: str, context: Optional[str] = None) -> RePPLScore:
        # ... (ì´ì „ê³¼ ë™ì¼)
        logger.debug(f"RePPL ë¶„ì„ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)})")
        repetition_score, repeated_phrases = self._analyze_repetition(text)
        perplexity = await self._calculate_perplexity_with_gpt(text, context)
        semantic_entropy = self._calculate_semantic_entropy(text)
        confidence = self._calculate_confidence_score(repetition_score, perplexity, semantic_entropy)
        logger.debug(f"RePPL ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: Repetition={repetition_score:.2f}, PPL={perplexity:.2f}, Entropy={semantic_entropy:.2f} -> Confidence={confidence:.2f}")
        analysis_details = {"token_count": len(text.split()),"sentence_count": len(text.split('.')),"has_context": context is not None}
        return RePPLScore(repetition_score=repetition_score, perplexity=perplexity, semantic_entropy=semantic_entropy, confidence=confidence, repeated_phrases=repeated_phrases, analysis_details=analysis_details)


    def _analyze_repetition(self, text: str) -> Tuple[float, List[str]]:
        # ... (ì´ì „ê³¼ ë™ì¼)
        phrases, repeated_phrases_set = [], set()
        words = text.split()
        if not words: return 0.0, []
        for n in range(3, 8):
            if len(words) < n: break
            ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]
            phrase_counts = Counter(ngrams)
            for phrase, count in phrase_counts.items():
                if count > 1:
                    repeated_phrases_set.add(' '.join(phrase))
                    phrases.append((' '.join(phrase), count))
        repeated_words = sum(len(p.split()) * (c - 1) for p, c in phrases)
        repetition_score = min(1.0, repeated_words / len(words))
        logger.debug(f"ë°˜ë³µì„± ì ìˆ˜: {repetition_score:.3f}, ë°˜ë³µëœ êµ¬ë¬¸: {len(repeated_phrases_set)}ê°œ")
        return repetition_score, list(repeated_phrases_set)


    async def _calculate_perplexity_with_gpt(self, text: str, context: Optional[str] = None) -> float:
        # ... (ì´ì „ê³¼ ë™ì¼)
        if not text.strip(): return self.perplexity_threshold * 2
        logger.debug(f"Perplexity ê³„ì‚° ìš”ì²­ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)})")
        try:
            full_text = f"Context: {context}\n\nText to evaluate: {text}" if context else text
            response = await self.client.chat.completions.create(model=self.perplexity_model, messages=[{"role": "user", "content": full_text}], max_tokens=2048, temperature=0, logprobs=True)
            logprobs = [token.logprob for token in response.choices[0].logprobs.content if token.logprob is not None]
            if not logprobs: return self.perplexity_threshold * 2
            mean_logprob = sum(logprobs) / len(logprobs)
            perplexity = math.exp(-mean_logprob)
            logger.debug(f"GPT Perplexity ê³„ì‚° ì„±ê³µ: {perplexity:.2f}")
            return perplexity
        except Exception as e:
            logger.error(f"GPT Perplexity ê³„ì‚° ì‹¤íŒ¨: {e}")
            return self.perplexity_threshold * 2

    # ğŸ’¡ [í•µì‹¬ ìˆ˜ì •] np.inner ëŒ€ì‹  sklearn.metrics.pairwise.cosine_similarity ì‚¬ìš©
    def _calculate_semantic_entropy(self, text: str) -> float:
        """ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ë‹¤ì–‘ì„±(ì—”íŠ¸ë¡œí”¼) ê³„ì‚° (ì•ˆì •ì„± ê°•í™”)"""
        sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 5]
        if len(sentences) < 2:
            return 0.0

        embeddings = self.sentence_model.encode(sentences)

        # scikit-learnì„ ì‚¬ìš©í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        sim_matrix = cosine_similarity(embeddings)

        # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„(ëŒ€ê°ì„ )ë¥¼ ì œì™¸í•œ ëª¨ë“  ìœ ì‚¬ë„ì˜ í‰ê·  ê³„ì‚°
        num_sentences = len(sentences)
        # ëŒ€ê°ì„ ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ í•©ì‚°ì—ì„œ ì œì™¸
        np.fill_diagonal(sim_matrix, 0)
        # off-diagonal ìš”ì†Œë“¤ì˜ í•©ì„ ê°œìˆ˜(N*(N-1))ë¡œ ë‚˜ëˆ”
        avg_similarity = np.sum(sim_matrix) / (num_sentences * (num_sentences - 1))

        entropy = 1 - avg_similarity
        logger.debug(f"ì˜ë¯¸ì  ì—”íŠ¸ë¡œí”¼ ê³„ì‚°ë¨: {entropy:.3f} (ë¬¸ì¥ {len(sentences)}ê°œ ê¸°ë°˜)")
        return entropy

    def _calculate_confidence_score(self, repetition: float, perplexity: float, entropy: float) -> float:
        """í†µí•© ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        # (ì´ì „ê³¼ ë™ì¼)
        rep_score = 1 - min(1.0, repetition / self.repetition_threshold)
        ppl_score = max(0.0, 1 - (perplexity / self.perplexity_threshold))
        ent_score = min(1.0, entropy)
        weights = {'repetition': 0.4, 'perplexity': 0.3, 'entropy': 0.3}
        confidence = (weights['repetition'] * rep_score + weights['perplexity'] * ppl_score + weights['entropy'] * ent_score)
        return round(confidence, 3)


class RePPLEnhancedIssueSearcher:
    # ... (ì´í•˜ í´ë˜ìŠ¤ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼)
    """RePPLì´ í†µí•©ëœ ì´ìŠˆ ê²€ìƒ‰ê¸°"""
    def __init__(self, api_key: Optional[str] = None):
        self.base_searcher = create_issue_searcher(api_key)
        self.reppl_detector = RePPLHallucinationDetector()
        self.min_confidence_threshold = 0.5 # ì‹ ë¢°ë„ ì„ê³„ê°’
        logger.info("RePPL ê°•í™” ì´ìŠˆ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    async def search_with_validation(self, keyword_result: KeywordResult, time_period: str) -> SearchResult:
        """RePPL ê²€ì¦ì´ í¬í•¨ëœ ì´ìŠˆ ê²€ìƒ‰"""
        max_retries = 2
        current_keywords = keyword_result

        for attempt in range(max_retries):
            search_result = await self.base_searcher.search_issues_from_keywords(current_keywords, time_period, collect_details=True)
            validated_issues = []
            if search_result.issues:
                validation_tasks = [self._validate_issue(issue, current_keywords.topic) for issue in search_result.issues]
                validation_results = await asyncio.gather(*validation_tasks)
                validated_issues = [issue for issue in validation_results if issue is not None]

            if len(validated_issues) >= 3 or attempt == max_retries - 1:
                search_result.issues = validated_issues
                search_result.total_found = len(validated_issues)
                return search_result

            logger.info(f"ì‹ ë¢°ë„ ë†’ì€ ì´ìŠˆ ë¶€ì¡±, í‚¤ì›Œë“œë¥¼ ì¬ìƒì„±í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            from src.keyword_generator import generate_keywords_for_topic
            current_keywords = await generate_keywords_for_topic(f"{current_keywords.topic}ì˜ ë‹¤ë¥¸ ê´€ì ")
        return search_result

    async def _validate_issue(self, issue: IssueItem, topic: str) -> Optional[IssueItem]:
        """ê°œë³„ ì´ìŠˆë¥¼ RePPLë¡œ ê²€ì¦í•˜ê³  ì‹ ë¢°ë„ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        content_to_validate = f"ì œëª©: {issue.title}\nìš”ì•½: {issue.summary}"
        if issue.detailed_content:
            content_to_validate += f"\nìƒì„¸ë‚´ìš©: {issue.detailed_content[:500]}"
        reppl_score = await self.reppl_detector.analyze_response(content_to_validate, context=topic)
        if reppl_score.confidence >= self.min_confidence_threshold:
            setattr(issue, 'reppl_confidence', reppl_score.confidence)
            setattr(issue, 'reppl_analysis', reppl_score)
            logger.debug(f"ì´ìŠˆ '{issue.title[:30]}...' ê²€ì¦ í†µê³¼ (ì‹ ë¢°ë„: {reppl_score.confidence:.2f})")
            return issue
        else:
            logger.warning(f"ì´ìŠˆ '{issue.title[:30]}...' ì œì™¸ë¨ - RePPL ì‹ ë¢°ë„: {reppl_score.confidence:.2f}")
            return None