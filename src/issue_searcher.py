"""
ì´ìŠˆ ê²€ìƒ‰ ëª¨ë“ˆ - Perplexity API ì—°ë™
ìƒì„±ëœ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ëŠ” ëª¨ë“ˆ
"""

import asyncio
import json
import time
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import httpx
from loguru import logger

from src.config import config
from src.keyword_generator import KeywordResult


@dataclass
class IssueItem:
    """ê°œë³„ ì´ìŠˆ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    title: str                      # ì´ìŠˆ ì œëª©
    summary: str                    # ì´ìŠˆ ìš”ì•½
    source: str                     # ì¶œì²˜ (URL ë˜ëŠ” ë§¤ì²´ëª…)
    published_date: Optional[str]   # ë°œí–‰ì¼
    relevance_score: float          # ê´€ë ¨ì„± ì ìˆ˜ (0.0-1.0)
    category: str                   # ì¹´í…Œê³ ë¦¬ (news, blog, social, academic)
    content_snippet: str            # ë‚´ìš© ì¼ë¶€


@dataclass
class SearchResult:
    """ì´ìŠˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    query_keywords: List[str]       # ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ
    total_found: int                # ì´ ë°œê²¬ëœ ì´ìŠˆ ìˆ˜
    issues: List[IssueItem]         # ì´ìŠˆ ëª©ë¡
    search_time: float              # ê²€ìƒ‰ ì†Œìš” ì‹œê°„ (ì´ˆ)
    api_calls_used: int             # ì‚¬ìš©ëœ API í˜¸ì¶œ ìˆ˜
    confidence_score: float         # ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„
    time_period: str                # ê²€ìƒ‰ ê¸°ê°„
    raw_responses: List[str]        # ì›ë³¸ API ì‘ë‹µë“¤


class PerplexityClient:
    """Perplexity API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or config.get_perplexity_api_key()
        if not self.api_key:
            raise ValueError("Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        self.base_url = "https://api.perplexity.ai/chat/completions"
        self.model = "llama-3.1-sonar-small-128k-online"
        self.timeout = 60
        self.max_retries = 3

        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        logger.info(f"PerplexityClient ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")

    async def search_issues(
        self,
        keywords: List[str],
        time_period: str = "ìµœê·¼ 1ì£¼ì¼",
        max_results: int = 10
    ) -> Dict[str, Any]:
        """
        í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìŠˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤

        Args:
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            time_period: ê²€ìƒ‰ ê¸°ê°„
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            Dict: Perplexity API ì‘ë‹µ
        """
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        keyword_str = ", ".join(keywords[:5])  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©

        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œë“¤ê³¼ ê´€ë ¨ëœ {time_period} ë™ì•ˆì˜ ìµœì‹  ì´ìŠˆì™€ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”: {keyword_str}

ê²€ìƒ‰ ìš”êµ¬ì‚¬í•­:
1. ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì†Œì…œë¯¸ë””ì–´, í•™ìˆ ë…¼ë¬¸ì—ì„œ ê´€ë ¨ ì´ìŠˆ ê²€ìƒ‰
2. ê° ì´ìŠˆë§ˆë‹¤ ì œëª©, ê°„ëµí•œ ìš”ì•½, ì¶œì²˜ë¥¼ í¬í•¨
3. ìµœëŒ€ {max_results}ê°œì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì´ìŠˆ ì„ ë³„
4. ë°œí–‰ì¼ìê°€ ìµœê·¼ì¸ ìˆœì„œë¡œ ì •ë ¬

ì‘ë‹µ í˜•ì‹:
ê° ì´ìŠˆë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
**ì œëª©**: [ì´ìŠˆ ì œëª©]
**ìš”ì•½**: [2-3ë¬¸ì¥ ìš”ì•½]
**ì¶œì²˜**: [ë§¤ì²´ëª… ë˜ëŠ” URL]
**ì¼ì**: [ë°œí–‰ì¼ì]
**ì¹´í…Œê³ ë¦¬**: [news/blog/social/academic]

ê´€ë ¨ì„±ì´ ë†’ê³  ì‹ ë¢°í•  ë§Œí•œ ìµœì‹  ì •ë³´ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”."""

        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ì´ìŠˆ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ì™€ ì´ìŠˆë¥¼ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ì—ì„œ ì°¾ì•„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.3,
            "search_recency_filter": "week",  # ìµœê·¼ 1ì£¼ì¼ ê²°ê³¼ë§Œ
            "return_related_questions": False,  # ê´€ë ¨ ì§ˆë¬¸ ë¶ˆí•„ìš”
            "return_images": False  # ì´ë¯¸ì§€ ë¶ˆí•„ìš” (í…ìŠ¤íŠ¸ë§Œ)
        }

        logger.info(f"Perplexity API í˜¸ì¶œ ì‹œì‘: í‚¤ì›Œë“œ={keyword_str}, ê¸°ê°„={time_period}")

        # API í˜¸ì¶œ with ì¬ì‹œë„
        for attempt in range(self.max_retries):
            try:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.post(
                        self.base_url,
                        headers=self.headers,
                        json=payload
                    )

                    if response.status_code == 200:
                        result = response.json()
                        logger.success(f"Perplexity API í˜¸ì¶œ ì„±ê³µ (ì‹œë„ {attempt + 1})")
                        return result
                    elif response.status_code == 401:
                        raise ValueError("Perplexity API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    elif response.status_code == 429:
                        wait_time = 2 ** attempt
                        logger.warning(f"API ìš”ì²­ í•œë„ ì´ˆê³¼, {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        error_text = response.text
                        logger.error(f"Perplexity API ì˜¤ë¥˜ (ìƒíƒœ: {response.status_code}): {error_text}")
                        raise ValueError(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")

            except httpx.TimeoutException:
                logger.warning(f"API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{self.max_retries})")
                if attempt == self.max_retries - 1:
                    raise ValueError("API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ")
                await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)}")
                if attempt == self.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)

        raise ValueError("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")


class IssueSearcher:
    """
    ì´ìŠˆ ê²€ìƒ‰ê¸° - Perplexity APIë¥¼ ì‚¬ìš©í•œ ì´ìŠˆ íƒìƒ‰

    ì£¼ìš” ê¸°ëŠ¥:
    - í‚¤ì›Œë“œ ê¸°ë°˜ ì´ìŠˆ ê²€ìƒ‰
    - ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ë° êµ¬ì¡°í™”
    - ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    - ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„ í‰ê°€
    """

    def __init__(self, api_key: Optional[str] = None):
        self.client = PerplexityClient(api_key)
        self.max_keywords_per_search = 5
        self.max_results_per_search = 10

        logger.info("IssueSearcher ì´ˆê¸°í™” ì™„ë£Œ")

    async def search_issues_from_keywords(
        self,
        keyword_result: KeywordResult,
        time_period: str = "ìµœê·¼ 1ì£¼ì¼",
        max_total_results: int = 20
    ) -> SearchResult:
        """
        í‚¤ì›Œë“œ ìƒì„± ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤

        Args:
            keyword_result: í‚¤ì›Œë“œ ìƒì„± ê²°ê³¼
            time_period: ê²€ìƒ‰ ê¸°ê°„
            max_total_results: ìµœëŒ€ ì´ ê²°ê³¼ ìˆ˜

        Returns:
            SearchResult: ê²€ìƒ‰ ê²°ê³¼
        """
        start_time = time.time()
        logger.info(f"ì´ìŠˆ ê²€ìƒ‰ ì‹œì‘: ì£¼ì œ='{keyword_result.topic}', ê¸°ê°„='{time_period}'")

        try:
            # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ë³„ ê²€ìƒ‰ ì „ëµ
            search_keywords = self._prepare_search_keywords(keyword_result)

            # Perplexity API í˜¸ì¶œ
            api_response = await self.client.search_issues(
                keywords=search_keywords,
                time_period=time_period,
                max_results=max_total_results
            )

            # ì‘ë‹µ íŒŒì‹±
            issues = self._parse_api_response(api_response, search_keywords)

            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            scored_issues = self._calculate_relevance_scores(issues, keyword_result)

            # ìƒìœ„ ê²°ê³¼ë§Œ ì„ ë³„
            top_issues = sorted(scored_issues, key=lambda x: x.relevance_score, reverse=True)[:max_total_results]

            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_score = self._calculate_confidence_score(top_issues, keyword_result)

            search_time = time.time() - start_time

            # ì›ë³¸ ì‘ë‹µ ì €ì¥ (ì•ˆì „í•œ ë°©ì‹)
            try:
                raw_response_str = json.dumps(api_response, ensure_ascii=False, indent=2)
            except (TypeError, ValueError) as e:
                logger.warning(f"API ì‘ë‹µ JSON ì§ë ¬í™” ì‹¤íŒ¨: {e}")
                raw_response_str = str(api_response)

            result = SearchResult(
                query_keywords=search_keywords,
                total_found=len(top_issues),
                issues=top_issues,
                search_time=search_time,
                api_calls_used=1,
                confidence_score=confidence_score,
                time_period=time_period,
                raw_responses=[raw_response_str]
            )

            logger.success(
                f"ì´ìŠˆ ê²€ìƒ‰ ì™„ë£Œ: {len(top_issues)}ê°œ ì´ìŠˆ ë°œê²¬, "
                f"ì‹ ë¢°ë„ {confidence_score:.2f}, ì†Œìš”ì‹œê°„ {search_time:.1f}ì´ˆ"
            )

            return result

        except Exception as e:
            logger.error(f"ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            # í´ë°± ê²°ê³¼ ë°˜í™˜
            return self._create_fallback_result(keyword_result, time_period, time.time() - start_time)

    def _prepare_search_keywords(self, keyword_result: KeywordResult) -> List[str]:
        """ê²€ìƒ‰ì„ ìœ„í•œ ìµœì ì˜ í‚¤ì›Œë“œ ì¡°í•©ì„ ì¤€ë¹„í•©ë‹ˆë‹¤"""
        # ìš°ì„ ìˆœìœ„: í•µì‹¬ í‚¤ì›Œë“œ â†’ ê´€ë ¨ ìš©ì–´ â†’ ë™ì˜ì–´
        keywords = []
        keywords.extend(keyword_result.primary_keywords[:3])  # ìµœëŒ€ 3ê°œ í•µì‹¬ í‚¤ì›Œë“œ
        keywords.extend(keyword_result.related_terms[:2])     # ìµœëŒ€ 2ê°œ ê´€ë ¨ ìš©ì–´

        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
        unique_keywords = list(dict.fromkeys(keywords))[:self.max_keywords_per_search]

        logger.debug(f"ê²€ìƒ‰ í‚¤ì›Œë“œ ì¤€ë¹„ ì™„ë£Œ: {unique_keywords}")
        return unique_keywords

    def _parse_api_response(self, api_response: Dict[str, Any], search_keywords: List[str]) -> List[IssueItem]:
        """Perplexity API ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ IssueItem ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤"""
        issues = []

        try:
            # API ì‘ë‹µì—ì„œ ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ
            content = api_response['choices'][0]['message']['content']

            # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            # ì´ëŠ” Perplexity APIì˜ ì‹¤ì œ ì‘ë‹µ í˜•ì‹ì— ë§ì¶° ì¡°ì •í•´ì•¼ í•¨
            sections = content.split('**ì œëª©**:')

            for i, section in enumerate(sections[1:], 1):  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
                try:
                    issue = self._parse_issue_section(section, i)
                    if issue:
                        issues.append(issue)
                except Exception as e:
                    logger.warning(f"ì´ìŠˆ ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨ ({i}ë²ˆì§¸): {e}")
                    continue

            logger.info(f"API ì‘ë‹µ íŒŒì‹± ì™„ë£Œ: {len(issues)}ê°œ ì´ìŠˆ íŒŒì‹±ë¨")

        except Exception as e:
            logger.error(f"API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.debug(f"ì›ë³¸ ì‘ë‹µ: {api_response}")

        return issues

    def _parse_issue_section(self, section: str, index: int) -> Optional[IssueItem]:
        """ê°œë³„ ì´ìŠˆ ì„¹ì…˜ì„ íŒŒì‹±í•©ë‹ˆë‹¤"""
        try:
            lines = section.strip().split('\n')
            title = lines[0].strip()

            summary = ""
            source = "Unknown"
            published_date = None
            category = "news"

            # ê° ë¼ì¸ì„ íŒŒì‹±
            for line in lines[1:]:
                line = line.strip()
                if line.startswith('**ìš”ì•½**:'):
                    summary = line.replace('**ìš”ì•½**:', '').strip()
                elif line.startswith('**ì¶œì²˜**:'):
                    source = line.replace('**ì¶œì²˜**:', '').strip()
                elif line.startswith('**ì¼ì**:'):
                    published_date = line.replace('**ì¼ì**:', '').strip()
                elif line.startswith('**ì¹´í…Œê³ ë¦¬**:'):
                    category = line.replace('**ì¹´í…Œê³ ë¦¬**:', '').strip()

            if not title or not summary:
                return None

            return IssueItem(
                title=title,
                summary=summary,
                source=source,
                published_date=published_date,
                relevance_score=0.5,  # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— ê³„ì‚°ë¨
                category=category,
                content_snippet=summary[:200]  # ìš”ì•½ì˜ ì¼ë¶€
            )

        except Exception as e:
            logger.warning(f"ì´ìŠˆ ì„¹ì…˜ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _calculate_relevance_scores(self, issues: List[IssueItem], keyword_result: KeywordResult) -> List[IssueItem]:
        """ê° ì´ìŠˆì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤"""
        all_keywords = []
        all_keywords.extend(keyword_result.primary_keywords)
        all_keywords.extend(keyword_result.related_terms)
        all_keywords.extend(keyword_result.synonyms)

        for issue in issues:
            score = 0.0
            total_text = f"{issue.title} {issue.summary}".lower()

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            for keyword in all_keywords:
                if keyword.lower() in total_text:
                    if keyword in keyword_result.primary_keywords:
                        score += 0.3  # í•µì‹¬ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
                    elif keyword in keyword_result.related_terms:
                        score += 0.2  # ê´€ë ¨ ìš©ì–´ ê°€ì¤‘ì¹˜
                    else:
                        score += 0.1  # ë™ì˜ì–´ ê°€ì¤‘ì¹˜

            # ì‹ ì„ ë„ ì ìˆ˜ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            if issue.published_date:
                # ì‹¤ì œë¡œëŠ” ë‚ ì§œ íŒŒì‹±ì´ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
                score += 0.1

            # ì¶œì²˜ ì‹ ë¢°ë„ ì ìˆ˜
            if any(trusted in issue.source.lower() for trusted in ['reuters', 'bbc', 'cnn', 'nyt']):
                score += 0.2

            issue.relevance_score = min(1.0, score)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ

        return issues

    def _calculate_confidence_score(self, issues: List[IssueItem], keyword_result: KeywordResult) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ì „ì²´ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤"""
        if not issues:
            return 0.0

        # ê¸°ë³¸ ì‹ ë¢°ë„ëŠ” í‚¤ì›Œë“œ ìƒì„± ì‹ ë¢°ë„ì—ì„œ ì‹œì‘
        base_confidence = keyword_result.confidence_score * 0.7

        # ì´ìŠˆ ê°œìˆ˜ ë³´ë„ˆìŠ¤
        count_bonus = min(0.2, len(issues) * 0.02)

        # í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ ë³´ë„ˆìŠ¤
        avg_relevance = sum(issue.relevance_score for issue in issues) / len(issues)
        relevance_bonus = avg_relevance * 0.1

        total_confidence = base_confidence + count_bonus + relevance_bonus
        return min(1.0, total_confidence)

    def _create_fallback_result(self, keyword_result: KeywordResult, time_period: str, search_time: float) -> SearchResult:
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°± ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        logger.warning("ê²€ìƒ‰ ì‹¤íŒ¨ë¡œ ì¸í•œ í´ë°± ê²°ê³¼ ìƒì„±")

        return SearchResult(
            query_keywords=keyword_result.primary_keywords[:3],
            total_found=0,
            issues=[],
            search_time=search_time,
            api_calls_used=0,
            confidence_score=0.1,  # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„
            time_period=time_period,
            raw_responses=["ê²€ìƒ‰ ì‹¤íŒ¨ë¡œ ì¸í•œ ì‘ë‹µ ì—†ìŒ"]
        )

    def format_search_summary(self, result: SearchResult) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤"""
        if result.total_found == 0:
            return f"**ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨** (í‚¤ì›Œë“œ: {', '.join(result.query_keywords[:3])})\nâŒ ê´€ë ¨ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        confidence_percent = int(result.confidence_score * 100)

        summary = f"**ì´ìŠˆ ê²€ìƒ‰ ì™„ë£Œ** (í‚¤ì›Œë“œ: {', '.join(result.query_keywords[:3])})\n"
        summary += f"ğŸ“Š ì´ {result.total_found}ê°œ ì´ìŠˆ ë°œê²¬ | ì‹ ë¢°ë„: {confidence_percent}% | ì†Œìš”ì‹œê°„: {result.search_time:.1f}ì´ˆ\n\n"

        # ìƒìœ„ 3ê°œ ì´ìŠˆ ë¯¸ë¦¬ë³´ê¸°
        for i, issue in enumerate(result.issues[:3], 1):
            summary += f"**{i}. {issue.title}**\n"
            summary += f"   ğŸ“° {issue.source} | ê´€ë ¨ë„: {int(issue.relevance_score * 100)}%\n"
            summary += f"   ğŸ“ {issue.summary[:100]}{'...' if len(issue.summary) > 100 else ''}\n\n"

        if result.total_found > 3:
            summary += f"ğŸ“‹ ì¶”ê°€ {result.total_found - 3}ê°œ ì´ìŠˆê°€ ë” ìˆìŠµë‹ˆë‹¤.\n"

        return summary


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_issue_searcher(api_key: Optional[str] = None) -> IssueSearcher:
    """ì´ìŠˆ ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    return IssueSearcher(api_key=api_key)


async def search_issues_for_keywords(keyword_result: KeywordResult, time_period: str = "ìµœê·¼ 1ì£¼ì¼") -> SearchResult:
    """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    searcher = create_issue_searcher()
    return await searcher.search_issues_from_keywords(keyword_result, time_period)


if __name__ == "__main__":
    # pytest ì‹¤í–‰ ì•ˆë‚´
    print("ğŸ” ì´ìŠˆ ê²€ìƒ‰ê¸° í…ŒìŠ¤íŠ¸")
    print("pytestë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
    print("pytest tests/test_issue_searcher.py -v")