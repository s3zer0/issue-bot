"""
ì´ìŠˆ ê²€ìƒ‰ ëª¨ë“ˆ - Perplexity API ì—°ë™
ìƒì„±ëœ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  ì„¸ë¶€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
"""

import asyncio
import json
import time
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import httpx
from loguru import logger

from src.config import config
from src.keyword_generator import KeywordResult


@dataclass
class EntityInfo:
    """ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€ ì •ë³´"""
    name: str  # ì¸ë¬¼/ê¸°ê´€ëª…
    role: str  # ì—­í• /ì§ì±…
    relevance: float  # ê´€ë ¨ë„ (0.0-1.0)
    entity_type: str  # 'person', 'organization', 'company', 'government'
    description: str  # ê°„ë‹¨í•œ ì„¤ëª…


@dataclass
class ImpactAnalysis:
    """ì˜í–¥ë„ ë¶„ì„ ì •ë³´"""
    impact_level: str  # 'low', 'medium', 'high', 'critical'
    impact_score: float  # ìˆ˜ì¹˜ì  ì˜í–¥ë„ (0.0-1.0)
    affected_sectors: List[str]  # ì˜í–¥ë°›ëŠ” ë¶„ì•¼
    geographic_scope: str  # 'local', 'national', 'regional', 'global'
    time_sensitivity: str  # 'immediate', 'short-term', 'long-term'
    reasoning: str  # ì˜í–¥ë„ íŒë‹¨ ê·¼ê±°


@dataclass
class TimelineEvent:
    """ì‹œê°„ìˆœ ì´ë²¤íŠ¸ ì •ë³´"""
    date: str  # ì´ë²¤íŠ¸ ë°œìƒì¼
    event_type: str  # 'announcement', 'development', 'reaction', 'consequence'
    description: str  # ì´ë²¤íŠ¸ ì„¤ëª…
    importance: float  # ì¤‘ìš”ë„ (0.0-1.0)
    source: str  # ì •ë³´ ì¶œì²˜


@dataclass
class IssueItem:
    """ê°œë³„ ì´ìŠˆ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤ - í™•ì¥ë¨"""
    title: str  # ì´ìŠˆ ì œëª©
    summary: str  # ì´ìŠˆ ìš”ì•½
    source: str  # ì¶œì²˜ (URL ë˜ëŠ” ë§¤ì²´ëª…)
    published_date: Optional[str]  # ë°œí–‰ì¼
    relevance_score: float  # ê´€ë ¨ì„± ì ìˆ˜ (0.0-1.0)
    category: str  # ì¹´í…Œê³ ë¦¬ (news, blog, social, academic)
    content_snippet: str  # ë‚´ìš© ì¼ë¶€

    # 4ë‹¨ê³„ ì¶”ê°€ ì •ë³´
    detailed_content: Optional[str] = None  # ìƒì„¸ ë‚´ìš©
    related_entities: List[EntityInfo] = None  # ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€
    impact_analysis: Optional[ImpactAnalysis] = None  # ì˜í–¥ë„ ë¶„ì„
    timeline_events: List[TimelineEvent] = None  # ì‹œê°„ìˆœ ì´ë²¤íŠ¸
    background_context: Optional[str] = None  # ë°°ê²½ ì •ë³´
    detail_collection_time: Optional[float] = None  # ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì‹œê°„
    detail_confidence: Optional[float] = None  # ì„¸ë¶€ ì •ë³´ ì‹ ë¢°ë„


@dataclass
class SearchResult:
    """ì´ìŠˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤ - í™•ì¥ë¨"""
    query_keywords: List[str]  # ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ
    total_found: int  # ì´ ë°œê²¬ëœ ì´ìŠˆ ìˆ˜
    issues: List[IssueItem]  # ì´ìŠˆ ëª©ë¡
    search_time: float  # ê²€ìƒ‰ ì†Œìš” ì‹œê°„ (ì´ˆ)
    api_calls_used: int  # ì‚¬ìš©ëœ API í˜¸ì¶œ ìˆ˜
    confidence_score: float  # ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„
    time_period: str  # ê²€ìƒ‰ ê¸°ê°„
    raw_responses: List[str]  # ì›ë³¸ API ì‘ë‹µë“¤

    # 4ë‹¨ê³„ ì¶”ê°€ ì •ë³´
    detailed_issues_count: int = 0  # ì„¸ë¶€ ì •ë³´ê°€ ìˆ˜ì§‘ëœ ì´ìŠˆ ìˆ˜
    total_detail_collection_time: float = 0.0  # ì´ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì‹œê°„
    average_detail_confidence: float = 0.0  # í‰ê·  ì„¸ë¶€ ì •ë³´ ì‹ ë¢°ë„


class PerplexityClient:
    """Perplexity API í´ë¼ì´ì–¸íŠ¸ - 4ë‹¨ê³„ ê¸°ëŠ¥ ì¶”ê°€"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or config.get_perplexity_api_key()
        if not self.api_key:
            raise ValueError("Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        self.base_url = "https://api.perplexity.ai/chat/completions"
        self.model = "llama-3.1-sonar-large-128k-online"
        self.timeout = 60
        self.max_retries = 3

        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        logger.info(f"PerplexityClient ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")

    async def search_issues(
            self,
            keywords: List[str],
            time_period: str = "ìµœê·¼ 1ì£¼ì¼",
            max_results: int = 10
    ) -> Dict[str, Any]:
        """
        í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìŠˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤ (ê¸°ì¡´ ë©”ì„œë“œ)
        """
        # ê¸°ì¡´ êµ¬í˜„ ìœ ì§€
        keyword_str = ", ".join(keywords[:5])

        prompt = f"""ë‹¤ìŒ í‚¤ì›Œë“œë“¤ê³¼ ê´€ë ¨ëœ {time_period} ë™ì•ˆì˜ ìµœì‹  ì´ìŠˆì™€ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”: {keyword_str}

ê²€ìƒ‰ ìš”êµ¬ì‚¬í•­:
1. ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì†Œì…œë¯¸ë””ì–´, í•™ìˆ ë…¼ë¬¸ì—ì„œ ê´€ë ¨ ì´ìŠˆ ê²€ìƒ‰
2. ê° ì´ìŠˆë§ˆë‹¤ ì œëª©, ê°„ëµí•œ ìš”ì•½, ì¶œì²˜ë¥¼ í¬í•¨
3. ìµœëŒ€ {max_results}ê°œì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì´ìŠˆ ì„ ë³„
4. ë°œí–‰ì¼ìê°€ ìµœê·¼ì¸ ìˆœì„œë¡œ ì •ë ¬

ì‘ë‹µ í˜•ì‹:
ê° ì´ìŠˆë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
**ì œëª©**: [ì´ìŠˆ ì œëª©]
**ìš”ì•½**: [2-3ë¬¸ì¥ ìš”ì•½]
**ì¶œì²˜**: [ë§¤ì²´ëª… ë˜ëŠ” URL]
**ì¼ì**: [ë°œí–‰ì¼ì]
**ì¹´í…Œê³ ë¦¬**: [news/blog/social/academic]

ê´€ë ¨ì„±ì´ ë†’ê³  ì‹ ë¢°í•  ë§Œí•œ ìµœì‹  ì •ë³´ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”."""

        return await self._make_api_call(prompt)

    async def collect_detailed_information(
            self,
            issue_title: str,
            issue_summary: str,
            original_keywords: List[str]
    ) -> Dict[str, Any]:
        """
        íŠ¹ì • ì´ìŠˆì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (4ë‹¨ê³„ ì‹ ê·œ)
        """
        keywords_str = ", ".join(original_keywords[:3])

        prompt = f"""ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•œ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:

**ì´ìŠˆ ì œëª©**: {issue_title}
**ê¸°ë³¸ ìš”ì•½**: {issue_summary}
**ê´€ë ¨ í‚¤ì›Œë“œ**: {keywords_str}

ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **ìƒì„¸ ë‚´ìš©**: ì´ìŠˆì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ê³¼ ë°°ê²½
2. **ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€**: 
   - ì´ë¦„, ì—­í• , ê´€ë ¨ë„ (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ)
   - ìœ í˜•: ê°œì¸/ê¸°ì—…/ì •ë¶€ê¸°ê´€/êµ­ì œê¸°êµ¬
3. **ì˜í–¥ë„ ë¶„ì„**:
   - ì˜í–¥ ìˆ˜ì¤€: ë‚®ìŒ/ì¤‘ê°„/ë†’ìŒ/ë§¤ìš°ë†’ìŒ
   - ì˜í–¥ë°›ëŠ” ë¶„ì•¼ë“¤
   - ì§€ë¦¬ì  ë²”ìœ„: ì§€ì—­/êµ­ê°€/ì§€ì—­ê¶Œ/ê¸€ë¡œë²Œ
   - ì‹œê°„ ë¯¼ê°ë„: ì¦‰ì‹œ/ë‹¨ê¸°/ì¥ê¸°
4. **ì‹œê°„ìˆœ ì „ê°œ**:
   - ì£¼ìš” ì´ë²¤íŠ¸ë“¤ì˜ ì‹œê°„ìˆœ ë‚˜ì—´
   - ê° ì´ë²¤íŠ¸ì˜ ì¤‘ìš”ë„
5. **ë°°ê²½ ì •ë³´**: ì´ìŠˆë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ë§¥ë½ ì •ë³´

ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ê°ê´€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""

        logger.info(f"ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ API í˜¸ì¶œ: {issue_title[:50]}...")
        return await self._make_api_call(prompt)

    async def extract_entities_and_impact(
            self,
            issue_title: str,
            detailed_content: str
    ) -> Dict[str, Any]:
        """
        ì´ìŠˆì—ì„œ ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€ ë° ì˜í–¥ë„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (4ë‹¨ê³„ ì‹ ê·œ)
        """
        prompt = f"""ë‹¤ìŒ ì´ìŠˆ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

**ì´ìŠˆ**: {issue_title}
**ë‚´ìš©**: {detailed_content[:1000]}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "entities": [
        {{
            "name": "ì¸ë¬¼/ê¸°ê´€ëª…",
            "role": "ì—­í• /ì§ì±…",
            "relevance": 0.8,
            "entity_type": "person|organization|company|government",
            "description": "ê°„ë‹¨í•œ ì„¤ëª…"
        }}
    ],
    "impact": {{
        "impact_level": "low|medium|high|critical",
        "impact_score": 0.7,
        "affected_sectors": ["ê¸°ìˆ ", "ê²½ì œ", "ì •ì¹˜"],
        "geographic_scope": "local|national|regional|global",
        "time_sensitivity": "immediate|short-term|long-term",
        "reasoning": "ì˜í–¥ë„ íŒë‹¨ ê·¼ê±°"
    }},
    "confidence": 0.85
}}

ê°ê´€ì ì´ê³  êµ¬ì²´ì ì¸ ì •ë³´ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”."""

        logger.info(f"ì—”í‹°í‹° ë° ì˜í–¥ë„ ì¶”ì¶œ: {issue_title[:30]}...")
        return await self._make_api_call(prompt)

    async def extract_timeline(
            self,
            issue_title: str,
            detailed_content: str
    ) -> Dict[str, Any]:
        """
        ì´ìŠˆì˜ ì‹œê°„ìˆœ ì „ê°œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (4ë‹¨ê³„ ì‹ ê·œ)
        """
        prompt = f"""ë‹¤ìŒ ì´ìŠˆì˜ ì‹œê°„ìˆœ ì „ê°œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

**ì´ìŠˆ**: {issue_title}
**ë‚´ìš©**: {detailed_content[:1000]}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "timeline": [
        {{
            "date": "2024-01-15",
            "event_type": "announcement|development|reaction|consequence",
            "description": "ì´ë²¤íŠ¸ ì„¤ëª…",
            "importance": 0.8,
            "source": "ì •ë³´ ì¶œì²˜"
        }}
    ],
    "background_context": "ì´ìŠˆë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ë°°ê²½ ì •ë³´",
    "confidence": 0.9
}}

ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¤‘ìš”í•œ ì´ë²¤íŠ¸ë“¤ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”."""

        logger.info(f"íƒ€ì„ë¼ì¸ ì¶”ì¶œ: {issue_title[:30]}...")
        return await self._make_api_call(prompt)

    async def _make_api_call(self, prompt: str) -> Dict[str, Any]:
        """
        ê³µí†µ API í˜¸ì¶œ ë©”ì„œë“œ
        """
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³ , ìš”ì²­ëœ í˜•ì‹ì— ë§ì¶° ì‘ë‹µí•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 2500,
            "temperature": 0.3,
            "search_recency_filter": "week",
            "return_related_questions": False,
            "return_images": False
        }

        # API í˜¸ì¶œ with ì¬ì‹œë„
        for attempt in range(self.max_retries):
            try:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.post(
                        self.base_url,
                        headers=self.headers,
                        json=payload
                    )

                    if response.status_code == 200:
                        result = response.json()
                        logger.debug(f"API í˜¸ì¶œ ì„±ê³µ (ì‹œë„ {attempt + 1})")
                        return result
                    elif response.status_code == 401:
                        raise ValueError("Perplexity API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    elif response.status_code == 429:
                        wait_time = 2 ** attempt
                        logger.warning(f"API ìš”ì²­ í•œë„ ì´ˆê³¼, {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        error_text = response.text
                        logger.error(f"API ì˜¤ë¥˜ (ìƒíƒœ: {response.status_code}): {error_text}")
                        raise ValueError(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")

            except httpx.TimeoutException:
                logger.warning(f"API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{self.max_retries})")
                if attempt == self.max_retries - 1:
                    raise ValueError("API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ")
                await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)}")
                if attempt == self.max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)

        raise ValueError("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")


class IssueSearcher:
    """
    ì´ìŠˆ ê²€ìƒ‰ê¸° - 4ë‹¨ê³„ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ê¸°ëŠ¥ ì¶”ê°€

    ì£¼ìš” ê¸°ëŠ¥:
    - í‚¤ì›Œë“œ ê¸°ë°˜ ì´ìŠˆ ê²€ìƒ‰ (ê¸°ì¡´)
    - ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ë° êµ¬ì¡°í™” (ê¸°ì¡´)
    - ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê¸°ì¡´)
    - ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ (ì‹ ê·œ)
    - ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€ ì¶”ì¶œ (ì‹ ê·œ)
    - ì˜í–¥ë„ ë¶„ì„ (ì‹ ê·œ)
    - ì‹œê°„ìˆœ ì „ê°œ ì¶”ì  (ì‹ ê·œ)
    """

    def __init__(self, api_key: Optional[str] = None):
        self.client = PerplexityClient(api_key)
        self.max_keywords_per_search = 5
        self.max_results_per_search = 10

        # 4ë‹¨ê³„ ì„¤ì •
        self.enable_detailed_collection = True
        self.max_detailed_issues = 10  # ì„¸ë¶€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•  ìµœëŒ€ ì´ìŠˆ ìˆ˜
        self.detail_collection_timeout = 60  # ê° ì´ìŠˆë³„ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ íƒ€ì„ì•„ì›ƒ

        logger.info("IssueSearcher ì´ˆê¸°í™” ì™„ë£Œ (4ë‹¨ê³„ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì§€ì›)")

    async def search_issues_from_keywords(
            self,
            keyword_result: KeywordResult,
            time_period: str = "ìµœê·¼ 1ì£¼ì¼",
            max_total_results: int = 20,
            collect_details: bool = True
    ) -> SearchResult:
        """
        í‚¤ì›Œë“œ ìƒì„± ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  ì„¸ë¶€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (í™•ì¥ë¨)

        Args:
            keyword_result: í‚¤ì›Œë“œ ìƒì„± ê²°ê³¼
            time_period: ê²€ìƒ‰ ê¸°ê°„
            max_total_results: ìµœëŒ€ ì´ ê²°ê³¼ ìˆ˜
            collect_details: ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì—¬ë¶€ (4ë‹¨ê³„)

        Returns:
            SearchResult: ê²€ìƒ‰ ê²°ê³¼ (ì„¸ë¶€ ì •ë³´ í¬í•¨)
        """
        start_time = time.time()
        logger.info(f"ì´ìŠˆ ê²€ìƒ‰ ì‹œì‘: ì£¼ì œ='{keyword_result.topic}', ê¸°ê°„='{time_period}', ì„¸ë¶€ìˆ˜ì§‘={collect_details}")

        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì´ìŠˆ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
            search_keywords = self._prepare_search_keywords(keyword_result)
            api_response = await self.client.search_issues(
                keywords=search_keywords,
                time_period=time_period,
                max_results=max_total_results
            )

            # 2ë‹¨ê³„: ì‘ë‹µ íŒŒì‹± (ê¸°ì¡´ ë¡œì§)
            issues = self._parse_api_response(api_response, search_keywords)
            scored_issues = self._calculate_relevance_scores(issues, keyword_result)
            top_issues = sorted(scored_issues, key=lambda x: x.relevance_score, reverse=True)[:max_total_results]

            # 3ë‹¨ê³„: ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ (ì‹ ê·œ ë¡œì§)
            detailed_issues_count = 0
            total_detail_time = 0.0
            detail_confidences = []

            if collect_details and self.enable_detailed_collection and top_issues:
                logger.info(f"4ë‹¨ê³„ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘: {min(len(top_issues), self.max_detailed_issues)}ê°œ ì´ìŠˆ")

                # ìƒìœ„ ì´ìŠˆë“¤ì— ëŒ€í•´ì„œë§Œ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘
                issues_to_detail = top_issues[:self.max_detailed_issues]

                for i, issue in enumerate(issues_to_detail):
                    try:
                        detail_start = time.time()
                        logger.info(f"ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ({i + 1}/{len(issues_to_detail)}): {issue.title[:50]}...")

                        # ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì‹¤í–‰
                        enhanced_issue = await self._collect_issue_details(issue, search_keywords)

                        # ì›ë³¸ ì´ìŠˆë¥¼ ì—…ë°ì´íŠ¸ëœ ì´ìŠˆë¡œ êµì²´
                        top_issues[top_issues.index(issue)] = enhanced_issue

                        detail_time = time.time() - detail_start
                        total_detail_time += detail_time
                        detailed_issues_count += 1

                        if enhanced_issue.detail_confidence:
                            detail_confidences.append(enhanced_issue.detail_confidence)

                        logger.success(f"ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ({i + 1}/{len(issues_to_detail)}): {detail_time:.1f}ì´ˆ")

                        # API ì œí•œì„ ê³ ë ¤í•œ ì§€ì—°
                        if i < len(issues_to_detail) - 1:
                            await asyncio.sleep(1)

                    except asyncio.TimeoutError:
                        logger.warning(f"ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ íƒ€ì„ì•„ì›ƒ: {issue.title[:50]}")
                        continue
                    except Exception as e:
                        logger.error(f"ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {issue.title[:50]} - {str(e)}")
                        continue

            # 4ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ìƒì„±
            confidence_score = self._calculate_confidence_score(top_issues, keyword_result)
            search_time = time.time() - start_time

            # ì›ë³¸ ì‘ë‹µ ì €ì¥
            try:
                raw_response_str = json.dumps(api_response, ensure_ascii=False, indent=2)
            except (TypeError, ValueError) as e:
                logger.warning(f"API ì‘ë‹µ JSON ì§ë ¬í™” ì‹¤íŒ¨: {e}")
                raw_response_str = str(api_response)

            # í‰ê·  ì„¸ë¶€ ì •ë³´ ì‹ ë¢°ë„ ê³„ì‚°
            avg_detail_confidence = sum(detail_confidences) / len(detail_confidences) if detail_confidences else 0.0

            result = SearchResult(
                query_keywords=search_keywords,
                total_found=len(top_issues),
                issues=top_issues,
                search_time=search_time,
                api_calls_used=1 + detailed_issues_count * 2,  # ê¸°ë³¸ ê²€ìƒ‰ + ê° ì´ìŠˆë³„ 2íšŒ í˜¸ì¶œ
                confidence_score=confidence_score,
                time_period=time_period,
                raw_responses=[raw_response_str],
                detailed_issues_count=detailed_issues_count,
                total_detail_collection_time=total_detail_time,
                average_detail_confidence=avg_detail_confidence
            )

            logger.success(
                f"ì´ìŠˆ ê²€ìƒ‰ ì™„ë£Œ: {len(top_issues)}ê°œ ì´ìŠˆ (ì„¸ë¶€ì •ë³´ {detailed_issues_count}ê°œ), "
                f"ì‹ ë¢°ë„ {confidence_score:.2f}, ì´ ì†Œìš”ì‹œê°„ {search_time:.1f}ì´ˆ"
            )

            return result

        except Exception as e:
            logger.error(f"ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return self._create_fallback_result(keyword_result, time_period, time.time() - start_time)

    async def _collect_issue_details(
            self,
            issue: IssueItem,
            original_keywords: List[str]
    ) -> IssueItem:
        """
        ê°œë³„ ì´ìŠˆì˜ ì„¸ë¶€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (4ë‹¨ê³„ ì‹ ê·œ)
        """
        detail_start_time = time.time()

        try:
            # 1. ê¸°ë³¸ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘
            detailed_response = await asyncio.wait_for(
                self.client.collect_detailed_information(
                    issue.title,
                    issue.summary,
                    original_keywords
                ),
                timeout=self.detail_collection_timeout
            )

            detailed_content = self._extract_detailed_content(detailed_response)

            # 2. ë³‘ë ¬ë¡œ ì—”í‹°í‹°/ì˜í–¥ë„ ë° íƒ€ì„ë¼ì¸ ìˆ˜ì§‘
            entity_task = asyncio.create_task(
                self.client.extract_entities_and_impact(issue.title, detailed_content)
            )
            timeline_task = asyncio.create_task(
                self.client.extract_timeline(issue.title, detailed_content)
            )

            # ê²°ê³¼ ëŒ€ê¸°
            entity_response, timeline_response = await asyncio.gather(
                entity_task, timeline_task, return_exceptions=True
            )

            # 3. ì‘ë‹µ íŒŒì‹±
            entities, impact = self._parse_entity_and_impact_response(entity_response)
            timeline_events, background_context = self._parse_timeline_response(timeline_response)

            # 4. ì‹ ë¢°ë„ ê³„ì‚°
            detail_confidence = self._calculate_detail_confidence(
                detailed_content, entities, impact, timeline_events
            )

            # 5. ê¸°ì¡´ IssueItem ì—…ë°ì´íŠ¸
            issue.detailed_content = detailed_content
            issue.related_entities = entities
            issue.impact_analysis = impact
            issue.timeline_events = timeline_events
            issue.background_context = background_context
            issue.detail_collection_time = time.time() - detail_start_time
            issue.detail_confidence = detail_confidence

            return issue

        except asyncio.TimeoutError:
            logger.warning(f"ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ íƒ€ì„ì•„ì›ƒ: {issue.title[:50]}")
            raise
        except Exception as e:
            logger.error(f"ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise

    def _extract_detailed_content(self, api_response: Dict[str, Any]) -> str:
        """API ì‘ë‹µì—ì„œ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        try:
            content = api_response['choices'][0]['message']['content']
            # ìƒì„¸ ë‚´ìš© ì„¹ì…˜ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
            if "**ìƒì„¸ ë‚´ìš©**" in content:
                sections = content.split("**ìƒì„¸ ë‚´ìš©**:")
                if len(sections) > 1:
                    detailed_section = sections[1].split("**")[0].strip()
                    return detailed_section
            return content[:1000]  # í´ë°±: ì²˜ìŒ 1000ì
        except Exception as e:
            logger.warning(f"ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"

    def _parse_entity_and_impact_response(
            self,
            response: Any
    ) -> Tuple[List[EntityInfo], Optional[ImpactAnalysis]]:
        """ì—”í‹°í‹° ë° ì˜í–¥ë„ ì‘ë‹µì„ íŒŒì‹±í•©ë‹ˆë‹¤"""
        entities = []
        impact = None

        try:
            if isinstance(response, Exception):
                logger.warning(f"ì—”í‹°í‹°/ì˜í–¥ë„ ìˆ˜ì§‘ ì‹¤íŒ¨: {response}")
                return entities, impact

            content = response['choices'][0]['message']['content']

            # JSON ì¶”ì¶œ ì‹œë„
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())

                # ì—”í‹°í‹° íŒŒì‹±
                for entity_data in data.get('entities', []):
                    entity = EntityInfo(
                        name=entity_data.get('name', ''),
                        role=entity_data.get('role', ''),
                        relevance=float(entity_data.get('relevance', 0.5)),
                        entity_type=entity_data.get('entity_type', 'unknown'),
                        description=entity_data.get('description', '')
                    )
                    entities.append(entity)

                # ì˜í–¥ë„ íŒŒì‹±
                impact_data = data.get('impact', {})
                if impact_data:
                    impact = ImpactAnalysis(
                        impact_level=impact_data.get('impact_level', 'medium'),
                        impact_score=float(impact_data.get('impact_score', 0.5)),
                        affected_sectors=impact_data.get('affected_sectors', []),
                        geographic_scope=impact_data.get('geographic_scope', 'national'),
                        time_sensitivity=impact_data.get('time_sensitivity', 'short-term'),
                        reasoning=impact_data.get('reasoning', '')
                    )

        except Exception as e:
            logger.warning(f"ì—”í‹°í‹°/ì˜í–¥ë„ íŒŒì‹± ì‹¤íŒ¨: {e}")

        return entities, impact

    def _parse_timeline_response(
            self,
            response: Any
    ) -> Tuple[List[TimelineEvent], str]:
        """íƒ€ì„ë¼ì¸ ì‘ë‹µì„ íŒŒì‹±í•©ë‹ˆë‹¤"""
        timeline_events = []
        background_context = ""

        try:
            if isinstance(response, Exception):
                logger.warning(f"íƒ€ì„ë¼ì¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {response}")
                return timeline_events, background_context

            content = response['choices'][0]['message']['content']

            # JSON ì¶”ì¶œ ì‹œë„
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())

                # íƒ€ì„ë¼ì¸ íŒŒì‹±
                for event_data in data.get('timeline', []):
                    event = TimelineEvent(
                        date=event_data.get('date', ''),
                        event_type=event_data.get('event_type', 'development'),
                        description=event_data.get('description', ''),
                        importance=float(event_data.get('importance', 0.5)),
                        source=event_data.get('source', '')
                    )
                    timeline_events.append(event)

                # ë°°ê²½ ì •ë³´
                background_context = data.get('background_context', '')

        except Exception as e:
            logger.warning(f"íƒ€ì„ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: {e}")

        return timeline_events, background_context

    def _calculate_detail_confidence(
            self,
            detailed_content: str,
            entities: List[EntityInfo],
            impact: Optional[ImpactAnalysis],
            timeline_events: List[TimelineEvent]
    ) -> float:
        confidence = 0.0

        # ë‚´ìš© ê¸¸ì´ ì ìˆ˜ (ìµœëŒ€ 0.2) - ê¸°ì¡´ 0.3ì—ì„œ ì¶•ì†Œ
        if len(detailed_content) > 100:
            confidence += 0.2
        elif len(detailed_content) > 50:
            confidence += 0.15
        else:
            confidence += 0.1

        # ì—”í‹°í‹° ì •ë³´ ì ìˆ˜ (ìµœëŒ€ 0.4) - ê¸°ì¡´ 0.3ì—ì„œ í™•ëŒ€
        if entities:
            entity_score = min(0.3, len(entities) * 0.15)
            # ê³ í’ˆì§ˆ ì—”í‹°í‹° ë³´ë„ˆìŠ¤ ê°•í™”
            high_relevance_entities = [e for e in entities if e.relevance > 0.8]
            if high_relevance_entities:
                entity_score += 0.1
            confidence += entity_score

        # ì˜í–¥ë„ ë¶„ì„ ì ìˆ˜ (ìµœëŒ€ 0.3) - ê¸°ì¡´ 0.2ì—ì„œ í™•ëŒ€
        if impact:
            confidence += 0.2
            # êµ¬ì²´ì ì¸ ì˜í–¥ë„ ë¶„ì„ ë³´ë„ˆìŠ¤ ê°•í™”
            if impact.affected_sectors and len(impact.affected_sectors) > 0:
                confidence += 0.1

        # íƒ€ì„ë¼ì¸ ì ìˆ˜ (ìµœëŒ€ 0.2) - ìœ ì§€
        if timeline_events:
            timeline_score = min(0.2, len(timeline_events) * 0.1)
            confidence += timeline_score

        return min(1.0, confidence)

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
    def _prepare_search_keywords(self, keyword_result: KeywordResult) -> List[str]:
        """ê²€ìƒ‰ì„ ìœ„í•œ ìµœì ì˜ í‚¤ì›Œë“œ ì¡°í•©ì„ ì¤€ë¹„í•©ë‹ˆë‹¤"""
        keywords = []
        keywords.extend(keyword_result.primary_keywords[:3])
        keywords.extend(keyword_result.related_terms[:2])
        unique_keywords = list(dict.fromkeys(keywords))[:self.max_keywords_per_search]
        logger.debug(f"ê²€ìƒ‰ í‚¤ì›Œë“œ ì¤€ë¹„ ì™„ë£Œ: {unique_keywords}")
        return unique_keywords

    def _parse_api_response(self, api_response: Dict[str, Any], search_keywords: List[str]) -> List[IssueItem]:
        """Perplexity API ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ IssueItem ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤"""
        issues = []

        try:
            content = api_response['choices'][0]['message']['content']
            sections = content.split('**ì œëª©**:')

            for i, section in enumerate(sections[1:], 1):
                try:
                    issue = self._parse_issue_section(section, i)
                    if issue:
                        issues.append(issue)
                except Exception as e:
                    logger.warning(f"ì´ìŠˆ ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨ ({i}ë²ˆì§¸): {e}")
                    continue

            logger.info(f"API ì‘ë‹µ íŒŒì‹± ì™„ë£Œ: {len(issues)}ê°œ ì´ìŠˆ íŒŒì‹±ë¨")

        except Exception as e:
            logger.error(f"API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.debug(f"ì›ë³¸ ì‘ë‹µ: {api_response}")

        return issues

    def _parse_issue_section(self, section: str, index: int) -> Optional[IssueItem]:
        """ê°œë³„ ì´ìŠˆ ì„¹ì…˜ì„ íŒŒì‹±í•©ë‹ˆë‹¤"""
        try:
            lines = section.strip().split('\n')
            title = lines[0].strip()

            summary = ""
            source = "Unknown"
            published_date = None
            category = "news"

            for line in lines[1:]:
                line = line.strip()
                if line.startswith('**ìš”ì•½**:'):
                    summary = line.replace('**ìš”ì•½**:', '').strip()
                elif line.startswith('**ì¶œì²˜**:'):
                    source = line.replace('**ì¶œì²˜**:', '').strip()
                elif line.startswith('**ì¼ì**:'):
                    published_date = line.replace('**ì¼ì**:', '').strip()
                elif line.startswith('**ì¹´í…Œê³ ë¦¬**:'):
                    category = line.replace('**ì¹´í…Œê³ ë¦¬**:', '').strip()

            if not title or not summary:
                return None

            return IssueItem(
                title=title,
                summary=summary,
                source=source,
                published_date=published_date,
                relevance_score=0.5,
                category=category,
                content_snippet=summary[:200],
                # 4ë‹¨ê³„ í•„ë“œë“¤ì€ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„¤ì •
                related_entities=[],
                timeline_events=[]
            )

        except Exception as e:
            logger.warning(f"ì´ìŠˆ ì„¹ì…˜ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _calculate_relevance_scores(self, issues: List[IssueItem], keyword_result: KeywordResult) -> List[IssueItem]:
        """ê° ì´ìŠˆì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤"""
        all_keywords = []
        all_keywords.extend(keyword_result.primary_keywords)
        all_keywords.extend(keyword_result.related_terms)
        all_keywords.extend(keyword_result.synonyms)

        for issue in issues:
            score = 0.0
            total_text = f"{issue.title} {issue.summary}".lower()

            for keyword in all_keywords:
                if keyword.lower() in total_text:
                    if keyword in keyword_result.primary_keywords:
                        score += 0.3
                    elif keyword in keyword_result.related_terms:
                        score += 0.2
                    else:
                        score += 0.1

            if issue.published_date:
                score += 0.1

            if any(trusted in issue.source.lower() for trusted in ['reuters', 'bbc', 'cnn', 'nyt']):
                score += 0.2

            issue.relevance_score = min(1.0, score)

        return issues

    def _calculate_confidence_score(self, issues: List[IssueItem], keyword_result: KeywordResult) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ì „ì²´ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤"""
        if not issues:
            return 0.0

        base_confidence = keyword_result.confidence_score * 0.7
        count_bonus = min(0.2, len(issues) * 0.02)
        avg_relevance = sum(issue.relevance_score for issue in issues) / len(issues)
        relevance_bonus = avg_relevance * 0.1

        total_confidence = base_confidence + count_bonus + relevance_bonus
        return min(1.0, total_confidence)

    def _create_fallback_result(self, keyword_result: KeywordResult, time_period: str,
                                search_time: float) -> SearchResult:
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°± ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        logger.warning("ê²€ìƒ‰ ì‹¤íŒ¨ë¡œ ì¸í•œ í´ë°± ê²°ê³¼ ìƒì„±")

        return SearchResult(
            query_keywords=keyword_result.primary_keywords[:3],
            total_found=0,
            issues=[],
            search_time=search_time,
            api_calls_used=0,
            confidence_score=0.1,
            time_period=time_period,
            raw_responses=["ê²€ìƒ‰ ì‹¤íŒ¨ë¡œ ì¸í•œ ì‘ë‹µ ì—†ìŒ"]
        )

    def format_search_summary(self, result: SearchResult) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤ (4ë‹¨ê³„ ì •ë³´ í¬í•¨)"""
        if result.total_found == 0:
            return f"**ì´ìŠˆ ê²€ìƒ‰ ì‹¤íŒ¨** (í‚¤ì›Œë“œ: {', '.join(result.query_keywords[:3])})\nâŒ ê´€ë ¨ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        confidence_percent = int(result.confidence_score * 100)
        detail_confidence_percent = int(
            result.average_detail_confidence * 100) if result.average_detail_confidence > 0 else 0

        summary = f"**ì´ìŠˆ ê²€ìƒ‰ ì™„ë£Œ** (í‚¤ì›Œë“œ: {', '.join(result.query_keywords[:3])})\n"
        summary += f"ğŸ“Š ì´ {result.total_found}ê°œ ì´ìŠˆ ë°œê²¬ | ì‹ ë¢°ë„: {confidence_percent}%"

        if result.detailed_issues_count > 0:
            summary += f" | ì„¸ë¶€ì •ë³´: {result.detailed_issues_count}ê°œ ({detail_confidence_percent}%)"

        summary += f" | ì†Œìš”ì‹œê°„: {result.search_time:.1f}ì´ˆ\n\n"

        # ìƒìœ„ ì´ìŠˆë“¤ ë¯¸ë¦¬ë³´ê¸° (ì„¸ë¶€ ì •ë³´ í¬í•¨)
        for i, issue in enumerate(result.issues[:3], 1):
            summary += f"**{i}. {issue.title}**\n"
            summary += f"   ğŸ“° {issue.source} | ê´€ë ¨ë„: {int(issue.relevance_score * 100)}%"

            # ì„¸ë¶€ ì •ë³´ ì¶”ê°€ í‘œì‹œ
            if issue.detail_confidence and issue.detail_confidence > 0:
                summary += f" | ì„¸ë¶€ì‹ ë¢°ë„: {int(issue.detail_confidence * 100)}%"

            summary += "\n"
            summary += f"   ğŸ“ {issue.summary[:100]}{'...' if len(issue.summary) > 100 else ''}\n"

            # ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
            if issue.related_entities and len(issue.related_entities) > 0:
                top_entities = [e.name for e in
                                sorted(issue.related_entities, key=lambda x: x.relevance, reverse=True)[:2]]
                summary += f"   ğŸ‘¥ ê´€ë ¨: {', '.join(top_entities)}\n"

            # ì˜í–¥ë„ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
            if issue.impact_analysis:
                impact_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸŸ ", "critical": "ğŸ”´"}.get(
                    issue.impact_analysis.impact_level, "âšª")
                summary += f"   {impact_emoji} ì˜í–¥ë„: {issue.impact_analysis.impact_level} ({issue.impact_analysis.geographic_scope})\n"

            summary += "\n"

        if result.total_found > 3:
            summary += f"ğŸ“‹ ì¶”ê°€ {result.total_found - 3}ê°œ ì´ìŠˆê°€ ë” ìˆìŠµë‹ˆë‹¤.\n"

        # ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ìš”ì•½
        if result.detailed_issues_count > 0:
            summary += f"\nğŸ” **ì„¸ë¶€ ë¶„ì„**: {result.detailed_issues_count}ê°œ ì´ìŠˆì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ "
            summary += f"(ì†Œìš”ì‹œê°„: {result.total_detail_collection_time:.1f}ì´ˆ)\n"

        return summary

    def format_detailed_issue_report(self, issue: IssueItem) -> str:
        """ê°œë³„ ì´ìŠˆì˜ ìƒì„¸ ë³´ê³ ì„œë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤ (4ë‹¨ê³„ ì‹ ê·œ)"""
        if not issue.detailed_content:
            return f"**{issue.title}**\nğŸ“ {issue.summary}\nğŸ“° ì¶œì²˜: {issue.source}"

        report = f"# ğŸ“‹ ìƒì„¸ ì´ìŠˆ ë¶„ì„: {issue.title}\n\n"

        # ê¸°ë³¸ ì •ë³´
        report += f"**ğŸ“° ì¶œì²˜**: {issue.source}\n"
        if issue.published_date:
            report += f"**ğŸ“… ë°œí–‰ì¼**: {issue.published_date}\n"
        report += f"**ğŸ” ê´€ë ¨ë„**: {int(issue.relevance_score * 100)}%\n"
        if issue.detail_confidence:
            report += f"**ğŸ¯ ì„¸ë¶€ì‹ ë¢°ë„**: {int(issue.detail_confidence * 100)}%\n"
        report += "\n"

        # ìš”ì•½
        report += f"## ğŸ“ ìš”ì•½\n{issue.summary}\n\n"

        # ìƒì„¸ ë‚´ìš©
        if issue.detailed_content:
            report += f"## ğŸ“– ìƒì„¸ ë‚´ìš©\n{issue.detailed_content}\n\n"

        # ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€
        if issue.related_entities and len(issue.related_entities) > 0:
            report += "## ğŸ‘¥ ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€\n"
            for entity in sorted(issue.related_entities, key=lambda x: x.relevance, reverse=True):
                entity_emoji = {"person": "ğŸ‘¤", "organization": "ğŸ¢", "company": "ğŸ­", "government": "ğŸ›ï¸"}.get(
                    entity.entity_type, "ğŸ“‹")
                report += f"- {entity_emoji} **{entity.name}** ({entity.role})\n"
                report += f"  - ê´€ë ¨ë„: {int(entity.relevance * 100)}%\n"
                if entity.description:
                    report += f"  - {entity.description}\n"
                report += "\n"

        # ì˜í–¥ë„ ë¶„ì„
        if issue.impact_analysis:
            impact = issue.impact_analysis
            impact_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸŸ ", "critical": "ğŸ”´"}.get(impact.impact_level, "âšª")

            report += f"## {impact_emoji} ì˜í–¥ë„ ë¶„ì„\n"
            report += f"- **ì˜í–¥ ìˆ˜ì¤€**: {impact.impact_level} ({int(impact.impact_score * 100)}%)\n"
            report += f"- **ì§€ë¦¬ì  ë²”ìœ„**: {impact.geographic_scope}\n"
            report += f"- **ì‹œê°„ ë¯¼ê°ë„**: {impact.time_sensitivity}\n"

            if impact.affected_sectors:
                report += f"- **ì˜í–¥ë°›ëŠ” ë¶„ì•¼**: {', '.join(impact.affected_sectors)}\n"

            if impact.reasoning:
                report += f"- **íŒë‹¨ ê·¼ê±°**: {impact.reasoning}\n"
            report += "\n"

        # ì‹œê°„ìˆœ ì „ê°œ
        if issue.timeline_events and len(issue.timeline_events) > 0:
            report += "## â° ì‹œê°„ìˆœ ì „ê°œ\n"
            sorted_events = sorted(issue.timeline_events, key=lambda x: x.date)

            for event in sorted_events:
                event_emoji = {
                    "announcement": "ğŸ“¢",
                    "development": "ğŸ”„",
                    "reaction": "ğŸ’¬",
                    "consequence": "âš¡"
                }.get(event.event_type, "ğŸ“Œ")

                importance_stars = "â­" * int(event.importance * 5)
                report += f"- {event_emoji} **{event.date}** ({event.event_type}) {importance_stars}\n"
                report += f"  {event.description}\n"
                if event.source:
                    report += f"  ğŸ“° ì¶œì²˜: {event.source}\n"
                report += "\n"

        # ë°°ê²½ ì •ë³´
        if issue.background_context:
            report += f"## ğŸ”— ë°°ê²½ ì •ë³´\n{issue.background_context}\n\n"

        # ë©”íƒ€ ì •ë³´
        if issue.detail_collection_time:
            report += f"---\n*ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì‹œê°„: {issue.detail_collection_time:.1f}ì´ˆ*\n"

        return report


# í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ + í™•ì¥)
def create_issue_searcher(api_key: Optional[str] = None) -> IssueSearcher:
    """ì´ìŠˆ ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    return IssueSearcher(api_key=api_key)


async def search_issues_for_keywords(
        keyword_result: KeywordResult,
        time_period: str = "ìµœê·¼ 1ì£¼ì¼",
        collect_details: bool = True
) -> SearchResult:
    """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ëŠ” í¸ì˜ í•¨ìˆ˜ (4ë‹¨ê³„ ì§€ì›)"""
    searcher = create_issue_searcher()
    return await searcher.search_issues_from_keywords(keyword_result, time_period, collect_details=collect_details)


# 4ë‹¨ê³„ ì „ìš© í¸ì˜ í•¨ìˆ˜ë“¤
async def get_detailed_issue_analysis(issue_title: str, issue_summary: str, keywords: List[str]) -> Dict[str, Any]:
    """íŠ¹ì • ì´ìŠˆì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ì„ ê°€ì ¸ì˜¤ëŠ” í¸ì˜ í•¨ìˆ˜"""
    client = PerplexityClient()
    return await client.collect_detailed_information(issue_title, issue_summary, keywords)


def create_detailed_report_from_search_result(search_result: SearchResult) -> str:
    """SearchResultì—ì„œ ëª¨ë“  ìƒì„¸ ì´ìŠˆ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    if not search_result.issues:
        return "ìƒì„¸ ë¶„ì„í•  ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤."

    searcher = create_issue_searcher()
    full_report = f"# ğŸ” ì¢…í•© ì´ìŠˆ ë¶„ì„ ë³´ê³ ì„œ\n\n"
    full_report += f"**ê²€ìƒ‰ í‚¤ì›Œë“œ**: {', '.join(search_result.query_keywords)}\n"
    full_report += f"**ê²€ìƒ‰ ê¸°ê°„**: {search_result.time_period}\n"
    full_report += f"**ì´ ì´ìŠˆ ìˆ˜**: {search_result.total_found}ê°œ\n"
    full_report += f"**ì„¸ë¶€ ë¶„ì„ ì´ìŠˆ**: {search_result.detailed_issues_count}ê°œ\n"
    full_report += f"**ì „ì²´ ì‹ ë¢°ë„**: {int(search_result.confidence_score * 100)}%\n\n"

    full_report += "---\n\n"

    # ì„¸ë¶€ ì •ë³´ê°€ ìˆëŠ” ì´ìŠˆë“¤ë§Œ ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
    detailed_issues = [issue for issue in search_result.issues if issue.detailed_content]

    if detailed_issues:
        for i, issue in enumerate(detailed_issues, 1):
            full_report += searcher.format_detailed_issue_report(issue)
            if i < len(detailed_issues):
                full_report += "\n---\n\n"
    else:
        full_report += "ì„¸ë¶€ ë¶„ì„ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.\n"

    return full_report


if __name__ == "__main__":
    print("ğŸ” ì´ìŠˆ ê²€ìƒ‰ê¸° í…ŒìŠ¤íŠ¸ (4ë‹¨ê³„ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì§€ì›)")
    print("pytestë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
    print("pytest tests/test_issue_searcher.py -v")