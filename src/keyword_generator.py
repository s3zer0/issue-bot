"""
í‚¤ì›Œë“œ ìƒì„± ëª¨ë“ˆ - ìˆ˜ì •ëœ ë²„ì „
LLMì„ í™œìš©í•˜ì—¬ ì£¼ì œ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ìë™ ìƒì„±í•˜ëŠ” ëª¨ë“ˆ
"""

import asyncio
import json
import re
import time
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from loguru import logger

try:
    from openai import AsyncOpenAI
except ImportError:
    logger.error("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install openai' ì‹¤í–‰")
    raise

from src.config import config


@dataclass
class KeywordResult:
    """í‚¤ì›Œë“œ ìƒì„± ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    topic: str                          # ì›ë³¸ ì£¼ì œ
    primary_keywords: List[str]         # í•µì‹¬ í‚¤ì›Œë“œ
    related_terms: List[str]            # ê´€ë ¨ ìš©ì–´
    synonyms: List[str]                 # ë™ì˜ì–´
    context_keywords: List[str]         # ë§¥ë½ í‚¤ì›Œë“œ
    confidence_score: float             # ìƒì„± ì‹ ë¢°ë„ (0.0-1.0)
    generation_time: float              # ìƒì„± ì†Œìš” ì‹œê°„ (ì´ˆ)
    raw_response: str                   # ì›ë³¸ LLM ì‘ë‹µ


class KeywordGenerator:
    """
    LLM ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„±ê¸°

    ì£¼ìš” ê¸°ëŠ¥:
    - ì£¼ì œ ê¸°ë°˜ ë‹¤ì–‘í•œ í‚¤ì›Œë“œ ìƒì„±
    - í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
    - ìƒì„± í’ˆì§ˆ ê²€ì¦
    - ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
    """

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
        """
        í‚¤ì›Œë“œ ìƒì„±ê¸° ì´ˆê¸°í™”

        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸: gpt-4o-mini)
        """
        self.api_key = api_key or config.get_openai_api_key()
        self.model = model

        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        self.client = AsyncOpenAI(api_key=self.api_key)

        # ìƒì„± ì„¤ì • - configì—ì„œ ê°€ì ¸ì˜¤ê¸°
        self.max_retries = config.get_max_retry_count()
        self.timeout = config.get_keyword_generation_timeout()
        self.temperature = config.get_openai_temperature()
        self.max_tokens = config.get_openai_max_tokens()

        logger.info(f"KeywordGenerator ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")

    async def generate_keywords(
        self,
        topic: str,
        context: Optional[str] = None,
        num_keywords: int = 20
    ) -> KeywordResult:
        """
        ì£¼ì œì— ëŒ€í•œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤

        Args:
            topic: í‚¤ì›Œë“œë¥¼ ìƒì„±í•  ì£¼ì œ
            context: ì¶”ê°€ ë§¥ë½ ì •ë³´ (ì„ íƒì‚¬í•­)
            num_keywords: ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒì„±í•  í‚¤ì›Œë“œ ìˆ˜

        Returns:
            KeywordResult: ìƒì„±ëœ í‚¤ì›Œë“œ ê²°ê³¼
        """
        start_time = time.time()

        logger.info(f"í‚¤ì›Œë“œ ìƒì„± ì‹œì‘: '{topic}' (ëª¨ë¸: {self.model})")

        # ì…ë ¥ ê²€ì¦
        if not topic or not topic.strip():
            raise ValueError("ì£¼ì œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        topic = topic.strip()

        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_prompt(topic, context, num_keywords)

            # LLM í˜¸ì¶œ
            raw_response = await self._call_llm(prompt)

            # ì‘ë‹µ íŒŒì‹±
            keyword_result = self._parse_response(
                topic, raw_response, time.time() - start_time
            )

            logger.success(
                f"í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {len(keyword_result.primary_keywords)}ê°œ í•µì‹¬ í‚¤ì›Œë“œ, "
                f"ì‹ ë¢°ë„ {keyword_result.confidence_score:.2f}, "
                f"ì†Œìš”ì‹œê°„ {keyword_result.generation_time:.1f}ì´ˆ"
            )

            return keyword_result

        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise

    def _build_prompt(self, topic: str, context: Optional[str], num_keywords: int) -> str:
        """í‚¤ì›Œë“œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤"""

        base_prompt = f"""ì£¼ì œ "{topic}"ì— ëŒ€í•œ ì´ìŠˆ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. í•µì‹¬ í‚¤ì›Œë“œ (Primary Keywords): ì£¼ì œì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ í•µì‹¬ ìš©ì–´ë“¤
2. ê´€ë ¨ ìš©ì–´ (Related Terms): ì£¼ì œì™€ ì—°ê´€ëœ ê¸°ìˆ , ê°œë…, íŠ¸ë Œë“œ
3. ë™ì˜ì–´ (Synonyms): ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ë“¤
4. ë§¥ë½ í‚¤ì›Œë“œ (Context Keywords): í•´ë‹¹ ë¶„ì•¼ì˜ ë°°ê²½ ì§€ì‹ì´ë‚˜ ê´€ë ¨ ì˜ì—­

ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ {max(3, num_keywords//4)}~{min(8, num_keywords//2)}ê°œì”© ìƒì„±í•´ì£¼ì„¸ìš”."""

        if context:
            base_prompt += f"\n\nì¶”ê°€ ë§¥ë½: {context}"

        base_prompt += """

ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ìœ íš¨í•œ JSONìœ¼ë¡œ ì‘ë‹µ):
{
    "primary_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
    "related_terms": ["ìš©ì–´1", "ìš©ì–´2", "ìš©ì–´3", "ìš©ì–´4"],
    "synonyms": ["ë™ì˜ì–´1", "ë™ì˜ì–´2", "ë™ì˜ì–´3"],
    "context_keywords": ["ë§¥ë½1", "ë§¥ë½2", "ë§¥ë½3", "ë§¥ë½4"],
    "confidence": 0.9
}

ì£¼ì˜ì‚¬í•­:
- í‚¤ì›Œë“œëŠ” í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•´ì£¼ì„¸ìš”
- ê²€ìƒ‰ì— íš¨ê³¼ì ì¸ êµ¬ì²´ì ì¸ ìš©ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”  
- ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ëª¨í˜¸í•œ ìš©ì–´ëŠ” í”¼í•´ì£¼ì„¸ìš”
- confidenceëŠ” ìƒì„± í’ˆì§ˆì— ëŒ€í•œ ìì‹ ê°ì„ 0.0-1.0 ì‚¬ì´ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”
- ë°˜ë“œì‹œ ìœ„ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”"""

        return base_prompt

    async def _call_llm(self, prompt: str) -> str:
        """LLM APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤"""

        for attempt in range(self.max_retries):
            try:
                logger.debug(f"LLM API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{self.max_retries}")

                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ ì´ìŠˆ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í‚¤ì›Œë“œ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                                     "ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ í¬ê´„ì ì´ê³  íš¨ê³¼ì ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. "
                                     "ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    timeout=self.timeout
                )

                content = response.choices[0].message.content
                if not content:
                    raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

                logger.debug(f"LLM ì‘ë‹µ ê¸¸ì´: {len(content)}ì")
                return content.strip()

            except Exception as e:
                error_msg = str(e)
                logger.warning(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {error_msg}")

                if attempt == self.max_retries - 1:
                    # ë§ˆì§€ë§‰ ì‹œë„ ì‹¤íŒ¨ì‹œ ìƒì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì œê³µ
                    if "401" in error_msg:
                        raise ValueError("OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    elif "429" in error_msg:
                        raise ValueError("API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    elif "quota" in error_msg.lower():
                        raise ValueError("OpenAI í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê²°ì œ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    else:
                        raise ValueError(f"LLM API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {error_msg}")

                # ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸°
                wait_time = 2 ** attempt
                logger.debug(f"{wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                await asyncio.sleep(wait_time)

    def _parse_response(self, topic: str, raw_response: str, generation_time: float) -> KeywordResult:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ KeywordResultë¡œ ë³€í™˜í•©ë‹ˆë‹¤"""

        try:
            # JSON ì¶”ì¶œ - ë” ê²¬ê³ í•œ íŒŒì‹±
            # ë¨¼ì € ì½”ë“œ ë¸”ë¡ ì œê±°
            cleaned_response = re.sub(r'```json\s*\n', '', raw_response)
            cleaned_response = re.sub(r'\n\s*```', '', cleaned_response)

            # JSON íŒ¨í„´ ì°¾ê¸°
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
            if not json_match:
                raise ValueError("ì‘ë‹µì—ì„œ ìœ íš¨í•œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            json_str = json_match.group()

            try:
                data = json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.debug(f"íŒŒì‹± ì‹œë„í•œ JSON: {json_str}")
                raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['primary_keywords', 'related_terms', 'synonyms', 'context_keywords']
            missing_fields = [field for field in required_fields if field not in data]
            if missing_fields:
                raise ValueError(f"í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_fields}")

            # ë°ì´í„° ì •ì œ
            primary_keywords = self._clean_keywords(data['primary_keywords'])
            related_terms = self._clean_keywords(data['related_terms'])
            synonyms = self._clean_keywords(data['synonyms'])
            context_keywords = self._clean_keywords(data['context_keywords'])

            # ì‹ ë¢°ë„ ì ìˆ˜ ì²˜ë¦¬ - JSONì—ì„œ ë°›ì€ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ ë²”ìœ„ë§Œ ì œí•œ
            raw_confidence = float(data.get('confidence', 0.8))
            confidence_score = max(0.0, min(1.0, raw_confidence))  # 0.0-1.0 ë²”ìœ„ë¡œ ì œí•œ

            # í‚¤ì›Œë“œ ìˆ˜ ì²´í¬ëŠ” ê²½ê³ ë§Œ ë¡œê·¸í•˜ê³  ì‹ ë¢°ë„ ì¡°ì •ì€ í•˜ì§€ ì•ŠìŒ
            total_keywords = len(primary_keywords) + len(related_terms) + len(synonyms) + len(context_keywords)
            if total_keywords < 5:
                logger.warning(f"ìƒì„±ëœ í‚¤ì›Œë“œ ìˆ˜ê°€ ì ìŠµë‹ˆë‹¤: {total_keywords}ê°œ")

            # ìµœì†Œ í‚¤ì›Œë“œ ë³´ì¥
            if len(primary_keywords) == 0:
                primary_keywords = [topic]
                confidence_score = 0.5  # í´ë°±ì˜ ê²½ìš°ì—ë§Œ ë‚®ì€ ì‹ ë¢°ë„ ì ìš©

            return KeywordResult(
                topic=topic,
                primary_keywords=primary_keywords,
                related_terms=related_terms,
                synonyms=synonyms,
                context_keywords=context_keywords,
                confidence_score=confidence_score,
                generation_time=generation_time,
                raw_response=raw_response
            )

        except Exception as e:
            logger.error(f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            logger.debug(f"ì›ë³¸ ì‘ë‹µ: {raw_response}")

            # í´ë°±: ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±
            return self._create_fallback_result(topic, raw_response, generation_time)

    def _clean_keywords(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤"""
        if not isinstance(keywords, list):
            logger.warning(f"í‚¤ì›Œë“œê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(keywords)}")
            return []

        cleaned = []
        for keyword in keywords:
            if isinstance(keyword, str):
                # ë”°ì˜´í‘œ, ê³µë°± ì œê±°
                keyword = keyword.strip().strip('"\'').strip()
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œ ì œì™¸
                if keyword and len(keyword) > 1:
                    cleaned.append(keyword)
            elif keyword is not None:
                # ìˆ«ìë‚˜ ë‹¤ë¥¸ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                keyword_str = str(keyword).strip()
                if keyword_str and len(keyword_str) > 1:
                    cleaned.append(keyword_str)

        # ì¤‘ë³µ ì œê±° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
        seen = set()
        unique_keywords = []
        for keyword in cleaned:
            lower_keyword = keyword.lower()
            if lower_keyword not in seen:
                seen.add(lower_keyword)
                unique_keywords.append(keyword)

        return unique_keywords[:12]  # ìµœëŒ€ 12ê°œë¡œ ì œí•œ

    def _create_fallback_result(self, topic: str, raw_response: str, generation_time: float) -> KeywordResult:
        """íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""

        logger.warning("íŒŒì‹± ì‹¤íŒ¨ë¡œ ì¸í•œ í´ë°± í‚¤ì›Œë“œ ìƒì„±")

        # ì£¼ì œì—ì„œ ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        basic_keywords = [topic.strip()]
        words = topic.split()
        if len(words) > 1:
            basic_keywords.extend([word.strip() for word in words if len(word.strip()) > 1])

        # ì¤‘ë³µ ì œê±°
        basic_keywords = list(dict.fromkeys(basic_keywords))

        return KeywordResult(
            topic=topic,
            primary_keywords=basic_keywords,
            related_terms=[],
            synonyms=[],
            context_keywords=[],
            confidence_score=0.2,  # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„
            generation_time=generation_time,
            raw_response=raw_response
        )

    def get_all_keywords(self, result: KeywordResult) -> List[str]:
        """ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤"""
        all_keywords = []
        all_keywords.extend(result.primary_keywords)
        all_keywords.extend(result.related_terms)
        all_keywords.extend(result.synonyms)
        all_keywords.extend(result.context_keywords)
        return list(dict.fromkeys(all_keywords))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°

    def format_keywords_summary(self, result: KeywordResult) -> str:
        """í‚¤ì›Œë“œ ê²°ê³¼ë¥¼ ìš”ì•½ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤"""
        total_count = len(self.get_all_keywords(result))

        # í¼ì„¼íŠ¸ í‘œì‹œë¥¼ ì •ìˆ˜ë¡œ ë³€ê²½ (92.0% -> 92%)
        confidence_percent = int(result.confidence_score * 100)

        summary = f"**í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ** (ì£¼ì œ: {result.topic})\n"
        summary += f"ğŸ“Š ì´ {total_count}ê°œ í‚¤ì›Œë“œ | ì‹ ë¢°ë„: {confidence_percent}% | ì†Œìš”ì‹œê°„: {result.generation_time:.1f}ì´ˆ\n\n"

        if result.primary_keywords:
            summary += f"ğŸ¯ **í•µì‹¬**: {', '.join(result.primary_keywords[:5])}"
            if len(result.primary_keywords) > 5:
                summary += f" ì™¸ {len(result.primary_keywords) - 5}ê°œ"
            summary += "\n"

        if result.related_terms:
            summary += f"ğŸ”— **ê´€ë ¨**: {', '.join(result.related_terms[:3])}"
            if len(result.related_terms) > 3:
                summary += f" ì™¸ {len(result.related_terms) - 3}ê°œ"
            summary += "\n"

        return summary


# ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
def create_keyword_generator(api_key: Optional[str] = None, model: str = "gpt-4o-mini") -> KeywordGenerator:
    """í‚¤ì›Œë“œ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    return KeywordGenerator(api_key=api_key, model=model)


# í¸ì˜ í•¨ìˆ˜
async def generate_keywords_for_topic(topic: str, context: Optional[str] = None) -> KeywordResult:
    """ì£¼ì œì— ëŒ€í•œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    generator = create_keyword_generator()
    return await generator.generate_keywords(topic, context)


if __name__ == "__main__":
    # pytest ì‹¤í–‰ ì•ˆë‚´
    print("ğŸ§ª í‚¤ì›Œë“œ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸")
    print("pytestë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
    print("pytest tests/test_keyword_generator.py -v")